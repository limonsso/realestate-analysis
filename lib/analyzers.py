"""
Orchestrateur principal pour l'analyse immobili√®re
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, Tuple
import logging

from .interfaces import IDataProcessor, IPropertyClassifier, IFeatureSelector
from .validators import DataValidator
from .data_processors import PropertyDataProcessor
from .classifiers import PropertyClassifier
from .feature_selectors import FeatureSelector
from .property_type_normalizer import PropertyTypeNormalizer

logger = logging.getLogger(__name__)


class PropertyAnalyzer:
    """Orchestrateur principal pour l'analyse des propri√©t√©s"""
    
    def __init__(self, 
                 data_processor: IDataProcessor = None,
                 property_classifier: IPropertyClassifier = None,
                 feature_selector: IFeatureSelector = None,
                 property_types_data: list = None):
        
        # Initialiser les composants avec des valeurs par d√©faut si non fournis
        self.data_processor = data_processor or PropertyDataProcessor()
        self.property_classifier = property_classifier or PropertyClassifier()
        self.feature_selector = feature_selector or FeatureSelector()
        
        # Initialiser le normalisateur de types
        self.property_type_normalizer = PropertyTypeNormalizer(property_types_data=property_types_data)
        
        # Variables pour stocker les r√©sultats de chaque √©tape
        self.original_data = None
        self.processed_data = None
        self.classified_data = None
        self.classification_stats = None
        self.selected_features = None
        self.classification_features = None
        self.feature_importance = None
    
    def validate_and_explore(self, df: pd.DataFrame, target_column: str = 'price') -> pd.DataFrame:
        """
        √âTAPE 1: Validation et exploration des donn√©es
        """
        print(f"\n" + "="*60)
        print(f"üìä √âTAPE 1: VALIDATION ET EXPLORATION")
        print(f"="*60)
        print(f"üìä Donn√©es initiales: {df.shape}")
        print(f"üéØ Variable cible: '{target_column}'")
        
        # Validation des donn√©es
        if not DataValidator.validate_target_column(df, target_column):
            raise ValueError(f"Colonne cible '{target_column}' invalide")
        
        # Statistiques initiales de la variable cible
        if target_column in df.columns:
            target_stats = df[target_column].describe()
            print(f"\nüìà Statistiques de la variable cible '{target_column}':")
            print(f"   üìä Nombre de valeurs: {target_stats['count']:,.0f}")
            print(f"   üí∞ Moyenne: {target_stats['mean']:,.2f}")
            print(f"   üìä M√©diane: {target_stats['50%']:,.2f}")
            print(f"   üìâ Min: {target_stats['min']:,.2f}")
            print(f"   üìà Max: {target_stats['max']:,.2f}")
        
        # Exploration des donn√©es
        print(f"\nüìã Informations sur les colonnes:")
        print(f"   üìä Nombre total de colonnes: {len(df.columns)}")
        print(f"   üìà Colonnes num√©riques: {len(df.select_dtypes(include=[np.number]).columns)}")
        print(f"   üìù Colonnes cat√©gorielles: {len(df.select_dtypes(include=['object']).columns)}")
        print(f"   ‚ùì Valeurs manquantes: {df.isnull().sum().sum()}")
        
        # Stocker les donn√©es originales
        self.original_data = df.copy()
        
        print(f"‚úÖ Validation termin√©e avec succ√®s!")
        return df
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        √âTAPE 2: Nettoyage des donn√©es
        """
        print(f"\n" + "="*60)
        print(f"üßπ √âTAPE 2: NETTOYAGE DES DONN√âES")
        print(f"="*60)
        
        df_cleaned = self.data_processor.clean_data(df)
        
        print(f"üìä R√©sultats du nettoyage:")
        print(f"   üìà Lignes: {df.shape[0]:,} ‚Üí {df_cleaned.shape[0]:,}")
        print(f"   üìä Colonnes: {df.shape[1]} ‚Üí {df_cleaned.shape[1]}")
        if df.shape[1] != df_cleaned.shape[1]:
            print(f"   üìâ Colonnes supprim√©es: {df.shape[1] - df_cleaned.shape[1]}")
        
        print(f"‚úÖ Nettoyage termin√© avec succ√®s!")
        return df_cleaned
    
    def normalize_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        √âTAPE 3: Normalisation des variables cat√©gorielles
        """
        print(f"\n" + "="*60)
        print(f"üîÑ √âTAPE 3: NORMALISATION DES VARIABLES")
        print(f"="*60)
        
        df_normalized = df.copy()
        
        # 1. Normaliser les types de propri√©t√©s avec PropertyTypeNormalizer
        if 'type' in df_normalized.columns:
            print(f"üè† Normalisation des types de propri√©t√©s avec PropertyTypeNormalizer:")
            df_normalized = self.property_type_normalizer.normalize_property_types(df_normalized, 'type')
        
        # 2. Normaliser les autres variables cat√©gorielles
        df_normalized = self._normalize_other_variables(df_normalized)
        
        print(f"\nüìä R√©sultats de la normalisation:")
        print(f"   üìà Lignes: {df.shape[0]:,} ‚Üí {df_normalized.shape[0]:,}")
        print(f"   üìä Colonnes: {df.shape[1]} ‚Üí {df_normalized.shape[1]}")
        
        # Afficher les changements pour les variables importantes
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if col in df_normalized.columns:
                unique_before = df[col].nunique()
                unique_after = df_normalized[col].nunique()
                if unique_before != unique_after:
                    print(f"   üîÑ {col}: {unique_before} ‚Üí {unique_after} valeurs uniques")
        
        print(f"‚úÖ Normalisation termin√©e avec succ√®s!")
        return df_normalized
    
    def _normalize_other_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Normalisation des autres variables cat√©gorielles (sauf type)
        """
        df_normalized = df.copy()
        
        # Normaliser la colonne 'city' si elle existe
        if 'city' in df_normalized.columns:
            print(f"\nüèôÔ∏è Normalisation de 'city':")
            original_cities = df_normalized['city'].nunique()
            
            # Normaliser les noms de villes
            df_normalized['city'] = df_normalized['city'].str.title().str.strip()
            
            # Corrections sp√©cifiques pour le Qu√©bec
            city_corrections = {
                'Montreal': 'Montr√©al',
                'Quebec': 'Qu√©bec',
                'Quebec City': 'Qu√©bec',
                'Ville De Quebec': 'Qu√©bec',
                'Ville De Montreal': 'Montr√©al',
                'St-Jean-Sur-Richelieu': 'Saint-Jean-sur-Richelieu',
                'St-Jerome': 'Saint-J√©r√¥me',
                'Trois-Rivieres': 'Trois-Rivi√®res'
            }
            
            df_normalized['city'] = df_normalized['city'].replace(city_corrections)
            
            normalized_cities = df_normalized['city'].nunique()
            print(f"      Villes: {original_cities} ‚Üí {normalized_cities}")
        
        # Normaliser la colonne 'region' si elle existe
        if 'region' in df_normalized.columns:
            print(f"\nüó∫Ô∏è Normalisation de 'region':")
            original_regions = df_normalized['region'].nunique()
            
            # Normaliser les noms de r√©gions
            df_normalized['region'] = df_normalized['region'].str.title().str.strip()
            
            # Corrections sp√©cifiques pour le Qu√©bec
            region_corrections = {
                'Montreal': 'Montr√©al',
                'Quebec': 'Qu√©bec',
                'Outaouais': 'Outaouais',
                'Monteregie': 'Mont√©r√©gie',
                'Laurentides': 'Laurentides',
                'Lanaudiere': 'Lanaudi√®re'
            }
            
            df_normalized['region'] = df_normalized['region'].replace(region_corrections)
            
            normalized_regions = df_normalized['region'].nunique()
            print(f"      R√©gions: {original_regions} ‚Üí {normalized_regions}")
        
        # Normaliser la colonne 'building_style' si elle existe
        if 'building_style' in df_normalized.columns:
            print(f"\nüèóÔ∏è Normalisation de 'building_style':")
            original_styles = df_normalized['building_style'].nunique()
            
            # Normaliser les styles de construction
            df_normalized['building_style'] = df_normalized['building_style'].str.lower().str.strip()
            
            style_mapping = {
                'contemporain': 'contemporain',
                'contemporary': 'contemporain',
                'moderne': 'moderne',
                'modern': 'moderne',
                'traditionnel': 'traditionnel',
                'traditional': 'traditionnel',
                'colonial': 'colonial',
                'victorien': 'victorien',
                'victorian': 'victorien',
                'craftsman': 'artisan',
                'ranch': 'ranch',
                'bungalow': 'bungalow'
            }
            
            df_normalized['building_style'] = df_normalized['building_style'].map(style_mapping).fillna(df_normalized['building_style'])
            
            normalized_styles = df_normalized['building_style'].nunique()
            print(f"      Styles: {original_styles} ‚Üí {normalized_styles}")
        
        return df_normalized
    
    def _basic_normalize_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Normalisation basique des variables cat√©gorielles (m√©thode de fallback)
        """
        df_normalized = df.copy()
        
        # Normaliser la colonne 'type' si elle existe (fallback)
        if 'type' in df_normalized.columns:
            original_values = df_normalized['type'].value_counts()
            print(f"   üè∑Ô∏è Normalisation basique de 'type':")
            print(f"      Valeurs avant: {list(original_values.index)}")
            
            # Normaliser les types de propri√©t√©s
            type_mapping = {
                # Variations de "maison"
                'maison': 'maison',
                'maison √† vendre': 'maison',
                'maison unifamiliale': 'maison',
                'maison de ville': 'maison',
                'house': 'maison',
                'single family': 'maison',
                'detached': 'maison',
                
                # Variations de "condo"
                'condo': 'condo',
                'condominium': 'condo',
                'appartement': 'condo',
                'apartment': 'condo',
                'copropri√©t√©': 'condo',
                
                # Variations de "duplex"
                'duplex': 'duplex',
                'triplex': 'triplex',
                'quadruplex': 'quadruplex',
                'multiplex': 'multiplex',
                'plex': 'multiplex',
                
                # Autres types
                'terrain': 'terrain',
                'lot': 'terrain',
                'land': 'terrain',
                'commercial': 'commercial',
                'industriel': 'industriel',
                'industrial': 'industriel'
            }
            
            # Appliquer la normalisation
            df_normalized['type'] = df_normalized['type'].str.lower().str.strip()
            df_normalized['type'] = df_normalized['type'].map(type_mapping).fillna(df_normalized['type'])
            
            normalized_values = df_normalized['type'].value_counts()
            print(f"      Valeurs apr√®s: {list(normalized_values.index)}")
        
        # Appliquer la normalisation des autres variables
        df_normalized = self._normalize_other_variables(df_normalized)
        
        return df_normalized
    
    def encode_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        √âTAPE 4: Encodage des variables cat√©gorielles
        """
        print(f"\n" + "="*60)
        print(f"üî¢ √âTAPE 4: ENCODAGE DES VARIABLES")
        print(f"="*60)
        
        df_encoded = self.data_processor.encode_features(df)
        
        print(f"üìä R√©sultats de l'encodage:")
        print(f"   üìà Lignes: {df.shape[0]:,} ‚Üí {df_encoded.shape[0]:,}")
        print(f"   üìä Colonnes: {df.shape[1]} ‚Üí {df_encoded.shape[1]}")
        if df.shape[1] != df_encoded.shape[1]:
            print(f"   üìà Nouvelles colonnes cr√©√©es: {df_encoded.shape[1] - df.shape[1]}")
        
        print(f"‚úÖ Encodage termin√© avec succ√®s!")
        return df_encoded
    
    def impute_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        √âTAPE 5: Imputation des valeurs manquantes
        """
        print(f"\n" + "="*60)
        print(f"üîß √âTAPE 5: IMPUTATION DES VALEURS MANQUANTES")
        print(f"="*60)
        
        missing_before = df.isnull().sum().sum()
        df_imputed = self.data_processor.impute_missing_values(df)
        missing_after = df_imputed.isnull().sum().sum()
        
        print(f"üìä R√©sultats de l'imputation:")
        print(f"   ‚ùì Valeurs manquantes avant: {missing_before:,}")
        print(f"   ‚ùì Valeurs manquantes apr√®s: {missing_after:,}")
        print(f"   ‚úÖ Valeurs imput√©es: {missing_before - missing_after:,}")
        
        # Stocker les donn√©es trait√©es
        self.processed_data = df_imputed.copy()
        
        print(f"‚úÖ Imputation termin√©e avec succ√®s!")
        return df_imputed
    
    def classify_properties(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        √âTAPE 6: Classification des propri√©t√©s
        """
        print(f"\n" + "="*60)
        print(f"üè† √âTAPE 6: CLASSIFICATION DES PROPRI√âT√âS")
        print(f"="*60)
        
        df_classified = self.property_classifier.classify_properties(df)
        self.classification_stats = self.property_classifier.get_classification_stats(df_classified)
        
        print(f"üìä R√©sultats de la classification:")
        for category, count in self.classification_stats.get('counts', {}).items():
            pct = self.classification_stats.get('percentages', {}).get(category, 0)
            print(f"   üè∑Ô∏è {category}: {count:,} propri√©t√©s ({pct:.1f}%)")
        
        # Stocker les donn√©es classifi√©es
        self.classified_data = df_classified.copy()
        
        print(f"‚úÖ Classification termin√©e avec succ√®s!")
        return df_classified
    
    def prepare_for_modeling(self, df: pd.DataFrame, target_column: str = 'price') -> Tuple[pd.DataFrame, pd.Series]:
        """
        √âTAPE 7: Pr√©paration des donn√©es pour la mod√©lisation
        """
        print(f"\n" + "="*60)
        print(f"üéØ √âTAPE 7: PR√âPARATION POUR LA MOD√âLISATION")
        print(f"="*60)
        
        if target_column not in df.columns:
            raise ValueError(f"Colonne cible '{target_column}' manquante apr√®s traitement")
        
        X = df.drop(columns=[target_column])
        y = df[target_column]
        
        print(f"üìä Donn√©es pr√©par√©es:")
        print(f"   üìà Features (X): {X.shape}")
        print(f"   üéØ Cible (y): {y.shape}")
        print(f"   üìù Colonnes features: {list(X.columns)}")
        
        print(f"‚úÖ Pr√©paration termin√©e avec succ√®s!")
        return X, y
    
    def select_features(self, X: pd.DataFrame, y: pd.Series) -> list:
        """
        √âTAPE 8: S√©lection de variables
        """
        print(f"\n" + "="*60)
        print(f"üéØ √âTAPE 8: S√âLECTION DE VARIABLES")
        print(f"="*60)
        
        self.selected_features = self.feature_selector.select_features(X, y)
        
        print(f"üìä R√©sultats de la s√©lection:")
        print(f"   üìà Variables disponibles: {X.shape[1]}")
        print(f"   ‚úÖ Variables s√©lectionn√©es: {len(self.selected_features)}")
        print(f"   üìâ Variables √©limin√©es: {X.shape[1] - len(self.selected_features)}")
        print(f"   üìä Taux de conservation: {(len(self.selected_features)/X.shape[1]*100):.1f}%")
        
        print(f"\nüèÜ Variables s√©lectionn√©es:")
        for i, feature in enumerate(self.selected_features, 1):
            print(f"   {i}. {feature}")
        
        print(f"‚úÖ S√©lection termin√©e avec succ√®s!")
        return self.selected_features
    
    def select_features_by_classification(self, X: pd.DataFrame, y: pd.Series, classification_column: pd.Series) -> Dict[str, list]:
        """
        √âTAPE 9: S√©lection par type de propri√©t√©
        """
        print(f"\n" + "="*60)
        print(f"üè† √âTAPE 9: S√âLECTION PAR TYPE DE PROPRI√âT√â")
        print(f"="*60)
        
        self.classification_features = self.feature_selector.select_features_by_classification(
            X, y, classification_column
        )
        
        if self.classification_features:
            print(f"üìä Variables sp√©cifiques par type:")
            for prop_type, features in self.classification_features.items():
                print(f"   üè∑Ô∏è {prop_type}: {len(features)} variables")
                for i, feature in enumerate(features[:5], 1):  # Afficher seulement les 5 premi√®res
                    print(f"      {i}. {feature}")
                if len(features) > 5:
                    print(f"      ... et {len(features) - 5} autres")
        else:
            print(f"‚ö†Ô∏è Pas assez de donn√©es pour une s√©lection par type")
        
        print(f"‚úÖ S√©lection par type termin√©e avec succ√®s!")
        return self.classification_features
    
    def calculate_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """
        √âTAPE 10: Calcul de l'importance des variables
        """
        print(f"\n" + "="*60)
        print(f"üìä √âTAPE 10: IMPORTANCE DES VARIABLES")
        print(f"="*60)
        
        self.feature_importance = self.feature_selector.get_feature_importance(X, y)
        
        print(f"üìà Top 10 des variables les plus importantes:")
        sorted_features = sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_features[:10], 1):
            print(f"   {i}. {feature}: {importance:.4f}")
        
        print(f"‚úÖ Calcul d'importance termin√© avec succ√®s!")
        return self.feature_importance
    
    def final_summary(self) -> Dict[str, Any]:
        """
        √âTAPE 11: R√©sum√© final
        """
        print(f"\n" + "="*60)
        print(f"üèÜ √âTAPE 11: R√âSUM√â FINAL DE L'ANALYSE")
        print(f"="*60)
        
        if self.original_data is None or self.classified_data is None:
            print("‚ö†Ô∏è Donn√©es manquantes pour le r√©sum√©")
            return {}
        
        print(f"üìä Transformation des donn√©es:")
        print(f"   üìà Lignes: {self.original_data.shape[0]:,} ‚Üí {self.classified_data.shape[0]:,}")
        print(f"   üìä Colonnes: {self.original_data.shape[1]} ‚Üí {self.classified_data.shape[1]}")
        print(f"   üìâ R√©duction colonnes: {self.original_data.shape[1] - self.classified_data.shape[1]} (-{((self.original_data.shape[1] - self.classified_data.shape[1])/self.original_data.shape[1]*100):.1f}%)")
        
        if self.selected_features:
            print(f"\nüéØ S√©lection de variables:")
            print(f"   üìà Variables disponibles: {self.classified_data.shape[1] - 1}")  # -1 pour la cible
            print(f"   ‚úÖ Variables s√©lectionn√©es: {len(self.selected_features)}")
            print(f"   üìâ Variables √©limin√©es: {(self.classified_data.shape[1] - 1) - len(self.selected_features)}")
            print(f"   üìä Taux de conservation: {(len(self.selected_features)/(self.classified_data.shape[1] - 1)*100):.1f}%")
        
        if self.classification_stats:
            print(f"\nüè† Classification des propri√©t√©s:")
            for category, count in self.classification_stats.get('counts', {}).items():
                pct = self.classification_stats.get('percentages', {}).get(category, 0)
                print(f"   üè∑Ô∏è {category}: {count:,} propri√©t√©s ({pct:.1f}%)")
        
        results = {
            'shape_original': self.original_data.shape,
            'shape_processed': self.classified_data.shape,
            'classification_stats': self.classification_stats,
            'selected_features': self.selected_features,
            'classification_features': self.classification_features,
            'feature_importance': self.feature_importance
        }
        
        print(f"\nüéâ === ANALYSE TERMIN√âE AVEC SUCC√àS! ===")
        return results

    def analyze_properties(self, 
                          df: pd.DataFrame, 
                          target_column: str = 'price') -> Dict[str, Any]:
        """
        Ex√©cute l'analyse compl√®te des propri√©t√©s selon les 5 √©tapes d√©finies:
        
        1. üßπ Nettoyage des donn√©es
        2. üè∑Ô∏è Classification des propri√©t√©s  
        3. üîß Pr√©paration des donn√©es
        4. üéØ S√©lection des variables
        5. üìä Analyse des r√©sultats
        """
        logger.info("üöÄ D√©marrage de l'analyse des propri√©t√©s...")
        
        print(f"\n" + "="*60)
        print(f"üè† ANALYSE IMMOBILI√àRE COMPL√àTE")
        print(f"="*60)
        print(f"üìä Donn√©es initiales: {df.shape}")
        print(f"üéØ Variable cible: '{target_column}'")
        
        # Validation des donn√©es
        if not DataValidator.validate_target_column(df, target_column):
            raise ValueError(f"Colonne cible '{target_column}' invalide")
        
        # Statistiques initiales de la variable cible
        if target_column in df.columns:
            target_stats = df[target_column].describe()
            print(f"\nüìà Statistiques de la variable cible '{target_column}':")
            print(f"   üìä Nombre de valeurs: {target_stats['count']:,.0f}")
            print(f"   üí∞ Moyenne: {target_stats['mean']:,.2f}")
            print(f"   üìä M√©diane: {target_stats['50%']:,.2f}")
            print(f"   üìâ Min: {target_stats['min']:,.2f}")
            print(f"   üìà Max: {target_stats['max']:,.2f}")
        
        # √âTAPE 1: üßπ NETTOYAGE DES DONN√âES
        print(f"\n" + "="*60)
        print(f"üßπ √âTAPE 1: NETTOYAGE DES DONN√âES")
        print(f"="*60)
        print(f"   - Suppression des colonnes avec trop de valeurs manquantes")
        print(f"   - Suppression des colonnes non pertinentes (IDs, liens, etc.)")
        
        df_cleaned = self.data_processor.clean_data(df)
        
        print(f"\nüìä R√©sultats du nettoyage:")
        print(f"   üìà Lignes: {df.shape[0]:,} ‚Üí {df_cleaned.shape[0]:,}")
        print(f"   üìä Colonnes: {df.shape[1]} ‚Üí {df_cleaned.shape[1]}")
        if df.shape[1] != df_cleaned.shape[1]:
            print(f"   üìâ Colonnes supprim√©es: {df.shape[1] - df_cleaned.shape[1]}")
        
        # √âTAPE 2: üè∑Ô∏è CLASSIFICATION DES PROPRI√âT√âS
        print(f"\n" + "="*60)
        print(f"üè∑Ô∏è √âTAPE 2: CLASSIFICATION DES PROPRI√âT√âS")
        print(f"="*60)
        print(f"   - Analyse du type de propri√©t√© (`type` column)")
        print(f"   - Classification automatique : R√©sidentiel, Revenu, Autres")
        
        df_classified = self.property_classifier.classify_properties(df_cleaned)
        self.classification_stats = self.property_classifier.get_classification_stats(df_classified)
        
        print(f"\nüìä Classification termin√©e:")
        for category, count in self.classification_stats.get('counts', {}).items():
            pct = self.classification_stats.get('percentages', {}).get(category, 0)
            print(f"   üè∑Ô∏è {category}: {count:,} propri√©t√©s ({pct:.1f}%)")
        
        # √âTAPE 3: üîß PR√âPARATION DES DONN√âES
        print(f"\n" + "="*60)
        print(f"üîß √âTAPE 3: PR√âPARATION DES DONN√âES")
        print(f"="*60)
        print(f"   - Imputation des valeurs manquantes")
        print(f"   - Normalisation des variables si n√©cessaire")
        print(f"   - Encodage des variables cat√©gorielles")
        print(f"   - Pr√©paration finale pour la mod√©lisation")
        
        # 3.1 Imputation des valeurs manquantes (avant encodage)
        print(f"\nüîß 3.1 Imputation des valeurs manquantes...")
        df_imputed = self.data_processor.impute_missing_values(df_classified)
        missing_before = df_classified.isnull().sum().sum()
        missing_after = df_imputed.isnull().sum().sum()
        print(f"   ‚ùì Valeurs manquantes: {missing_before:,} ‚Üí {missing_after:,}")
        
        # 3.2 Normalisation des variables (si applicable)
        print(f"\nüîÑ 3.2 Normalisation des variables...")
        # Ici on pourrait ajouter la normalisation des types de propri√©t√©s
        # ou d'autres normalisations sp√©cifiques
        df_normalized = df_imputed  # Pour l'instant, pas de normalisation sp√©ciale
        print(f"   ‚úÖ Variables normalis√©es (le cas √©ch√©ant)")
        
        # 3.3 Encodage des variables cat√©gorielles
        print(f"\nüî§ 3.3 Encodage des variables cat√©gorielles...")
        df_encoded = self.data_processor.encode_features(df_normalized)
        categorical_cols_before = df_normalized.select_dtypes(include=['object']).shape[1]
        categorical_cols_after = df_encoded.select_dtypes(include=['object']).shape[1]
        print(f"   üìù Variables cat√©gorielles: {categorical_cols_before} ‚Üí {categorical_cols_after}")
        
        # 3.4 Pr√©paration finale pour la mod√©lisation
        print(f"\nüéØ 3.4 Pr√©paration finale pour la mod√©lisation...")
        
        if target_column not in df_encoded.columns:
            raise ValueError(f"Colonne cible '{target_column}' manquante apr√®s traitement")
        
        X = df_encoded.drop(columns=[target_column])
        y = df_encoded[target_column]
        
        # V√©rifier que toutes les colonnes sont num√©riques
        non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()
        if non_numeric_cols:
            print(f"‚ö†Ô∏è Colonnes non-num√©riques d√©tect√©es: {non_numeric_cols}")
            # Conversion forc√©e en num√©rique si possible
            for col in non_numeric_cols:
                try:
                    X[col] = pd.to_numeric(X[col], errors='coerce')
                    print(f"   ‚úÖ {col} converti en num√©rique")
                except:
                    print(f"   ‚ùå Impossible de convertir {col}, suppression")
                    X = X.drop(columns=[col])
            
            # Imputation finale des NaN cr√©√©s par la conversion
            if X.isnull().sum().sum() > 0:
                X = X.fillna(X.median())
                print(f"   üîß Valeurs NaN imput√©es apr√®s conversion")
        
        # Stocker le dataframe pr√©par√©
        df_prepared = df_encoded.copy()
        
        print(f"\nüìä R√©sultats de la pr√©paration:")
        print(f"   üìà Lignes: {df_classified.shape[0]:,} ‚Üí {df_prepared.shape[0]:,}")
        print(f"   üìä Colonnes: {df_classified.shape[1]} ‚Üí {df_prepared.shape[1]}")
        print(f"   ‚úÖ √âtape 3.1: Valeurs manquantes imput√©es")
        print(f"   ‚úÖ √âtape 3.2: Variables normalis√©es")
        print(f"   ‚úÖ √âtape 3.3: Variables cat√©gorielles encod√©es")
        print(f"   ‚úÖ √âtape 3.4: Donn√©es pr√©par√©es pour la mod√©lisation")
        
        print(f"\nüìä Donn√©es pr√™tes pour la mod√©lisation:")
        print(f"   üìà Features (X): {X.shape}")
        print(f"   üéØ Cible (y): {y.shape}")
        print(f"   üî¢ Toutes les variables sont num√©riques: {X.dtypes.apply(lambda x: np.issubdtype(x, np.number)).all()}")
        
        # √âTAPE 4: üéØ S√âLECTION DES VARIABLES
        print(f"\n" + "="*60)
        print(f"üéØ √âTAPE 4: S√âLECTION DES VARIABLES")
        print(f"="*60)
        print(f"   - M√©thode Lasso : R√©gularisation L1 pour √©liminer les variables non significatives")
        print(f"   - M√©thode Random Forest : Importance des variables bas√©e sur les arbres")
        print(f"   - Combinaison des deux approches pour un r√©sultat optimal")
        
        # S√©lection des variables
        try:
            self.selected_features = self.feature_selector.select_features(X, y)
            
            print(f"\nüìä R√©sultats de la s√©lection:")
            print(f"   üìà Variables disponibles: {X.shape[1]}")
            print(f"   ‚úÖ Variables s√©lectionn√©es: {len(self.selected_features)}")
            print(f"   üìâ Variables √©limin√©es: {X.shape[1] - len(self.selected_features)}")
            print(f"   üìä Taux de conservation: {(len(self.selected_features)/X.shape[1]*100):.1f}%")
            
            # Afficher les variables s√©lectionn√©es
            print(f"\nüèÜ Variables s√©lectionn√©es:")
            for i, feature in enumerate(self.selected_features[:10], 1):  # Afficher les 10 premi√®res
                print(f"   {i}. {feature}")
            if len(self.selected_features) > 10:
                print(f"   ... et {len(self.selected_features) - 10} autres")
                
        except Exception as e:
            print(f"   ‚ùå Erreur lors de la s√©lection de variables: {e}")
            print(f"   üîÑ Utilisation de toutes les variables disponibles")
            self.selected_features = X.columns.tolist()
        
        # S√©lection par type de propri√©t√© (si applicable)
        classification_features = {}
        if 'classification_immobiliere' in df_classified.columns:
            try:
                classification_features = self.feature_selector.select_features_by_classification(
                    X, y, df_classified['classification_immobiliere']
                )
                
                if classification_features:
                    print(f"\nüè† Variables sp√©cifiques par type:")
                    for prop_type, features in classification_features.items():
                        print(f"   üè∑Ô∏è {prop_type}: {len(features)} variables")
                        
            except Exception as e:
                print(f"\n‚ö†Ô∏è Erreur lors de la s√©lection par type: {e}")
        
        # √âTAPE 5: üìä ANALYSE DES R√âSULTATS
        print(f"\n" + "="*60)
        print(f"üìä √âTAPE 5: ANALYSE DES R√âSULTATS")
        print(f"="*60)
        print(f"   - Statistiques par type de propri√©t√©")
        print(f"   - Variables s√©lectionn√©es et leur importance")
        print(f"   - M√©triques de performance des mod√®les")
        
        # Stocker les donn√©es trait√©es
        self.processed_data = {
            'X': X,
            'y': y,
            'df_classified': df_classified,
            'df_prepared': df_prepared
        }
        
        # Calcul de l'importance des variables
        feature_importance = {}
        try:
            feature_importance = self.feature_selector.get_feature_importance(X, y)
            print(f"\nüéØ Importance des variables calcul√©e:")
            
            # Afficher le top 5 des variables les plus importantes
            if feature_importance:
                sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
                print(f"   üìà Top 5 des variables les plus importantes:")
                for i, (feature, importance) in enumerate(sorted_importance[:5], 1):
                    print(f"      {i}. {feature}: {importance:.4f}")
                    
        except Exception as e:
            print(f"   ‚ö†Ô∏è Impossible de calculer l'importance: {e}")
        
        # R√©sum√© final des r√©sultats
        print(f"\nüèÜ === R√âSUM√â FINAL DE L'ANALYSE ===")
        print(f"üìä Transformation des donn√©es:")
        print(f"   üìà Lignes: {df.shape[0]:,} ‚Üí {df_prepared.shape[0]:,}")
        print(f"   üìä Colonnes: {df.shape[1]} ‚Üí {df_prepared.shape[1]}")
        print(f"   üìâ R√©duction colonnes: {df.shape[1] - df_prepared.shape[1]} (-{((df.shape[1] - df_prepared.shape[1])/df.shape[1]*100):.1f}%)")
        
        print(f"\nüéØ S√©lection de variables:")
        print(f"   üìà Variables disponibles: {X.shape[1]}")
        print(f"   ‚úÖ Variables s√©lectionn√©es: {len(self.selected_features)}")
        print(f"   üìâ Variables √©limin√©es: {X.shape[1] - len(self.selected_features)}")
        print(f"   üìä Taux de conservation: {(len(self.selected_features)/X.shape[1]*100):.1f}%")
        
        print(f"\nüè† Classification des propri√©t√©s:")
        for category, count in self.classification_stats.get('counts', {}).items():
            pct = self.classification_stats.get('percentages', {}).get(category, 0)
            print(f"   üè∑Ô∏è {category}: {count:,} propri√©t√©s ({pct:.1f}%)")
        
        # Variables disponibles pour la suite
        print(f"\nüîß Variables disponibles pour la mod√©lisation:")
        print(f"   üìä X: Matrice des features ({X.shape})")
        print(f"   üéØ y: Vecteur cible ({y.shape})")
        print(f"   üè† df_classified: DataFrame avec classification")
        print(f"   üìù selected_features: Liste des variables importantes")
        
        # R√©sultats finaux
        results = {
            'shape_original': df.shape,
            'shape_processed': df_prepared.shape,
            'classification_stats': self.classification_stats,
            'selected_features': self.selected_features,
            'classification_features': classification_features,
            'feature_importance': feature_importance
        }
        
        print(f"\nüéâ === ANALYSE TERMIN√âE AVEC SUCC√àS! ===")
        logger.info("üèÜ Analyse termin√©e avec succ√®s!")
        return results
    
    def get_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© de l'analyse"""
        if self.processed_data is None:
            return {"error": "Aucune analyse effectu√©e"}
        
        X = self.processed_data['X']
        y = self.processed_data['y']
        
        return {
            'total_properties': len(y),
            'total_features': X.shape[1],
            'selected_features_count': len(self.selected_features),
            'reduction_percentage': ((X.shape[1] - len(self.selected_features)) / X.shape[1] * 100),
            'price_stats': {
                'mean': y.mean(),
                'median': y.median(),
                'min': y.min(),
                'max': y.max()
            },
            'classification_distribution': self.classification_stats.get('counts', {})
        } 