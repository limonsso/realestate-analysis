"""
Classes de s√©lection de variables pour l'analyse immobili√®re
Optimis√©es pour tous les volumes de donn√©es
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import logging
import time

from .interfaces import IFeatureSelector

logger = logging.getLogger(__name__)


class FastFeatureSelector(IFeatureSelector):
    """S√©lecteur de variables optimis√© pour tr√®s gros volumes (>100K)"""
    
    def __init__(self, 
                 sample_size: int = 8000,
                 random_state: int = 42,
                 correlation_threshold: float = 0.1,
                 univariate_k: int = 20,
                 rf_estimators: int = 20,
                 rf_max_depth: int = 10):
        self.sample_size = sample_size
        self.random_state = random_state
        self.correlation_threshold = correlation_threshold
        self.univariate_k = univariate_k
        self.rf_estimators = rf_estimators
        self.rf_max_depth = rf_max_depth
        self.selected_features_ = []
        self.feature_scores_ = {}
        
    def select_features(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """S√©lection rapide optimis√©e pour gros volumes"""
        start_time = time.time()
        
        print(f"\n‚ö° === S√âLECTION RAPIDE POUR GROS VOLUMES ===")
        print(f"üìä Donn√©es: {X.shape[0]:,} √ó {X.shape[1]}")
        
        # √âchantillonnage stratifi√© si n√©cessaire
        X_work, y_work = self._create_sample(X, y)
        
        # 1. S√©lection par corr√©lation (tr√®s rapide)
        corr_features = self._select_by_correlation(X_work, y_work)
        
        # 2. S√©lection univari√©e (rapide)
        univariate_features = self._select_univariate(X_work, y_work)
        
        # 3. Random Forest l√©ger (mod√©r√©)
        rf_features = self._select_rf_light(X_work, y_work)
        
        # 4. Combinaison intelligente
        selected = self._combine_methods(corr_features, univariate_features, rf_features)
        
        elapsed = time.time() - start_time
        print(f"\n‚úÖ S√©lection termin√©e en {elapsed:.1f}s")
        print(f"üéØ Variables s√©lectionn√©es: {len(selected)}")
        
        self.selected_features_ = selected
        return selected
    
    def _create_sample(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """Cr√©e un √©chantillon stratifi√©"""
        if len(X) <= self.sample_size:
            print(f"‚úÖ Donn√©es compl√®tes utilis√©es: {len(X):,}")
            return X, y
            
        print(f"üìä √âchantillonnage stratifi√©: {len(X):,} ‚Üí {self.sample_size:,}")
        
        # √âchantillonnage stratifi√© par quantiles de y
        try:
            stratify = pd.qcut(y, q=5, duplicates='drop')
            X_sample, _, y_sample, _ = train_test_split(
                X, y, train_size=self.sample_size, 
                random_state=self.random_state, stratify=stratify
            )
        except:
            # Fallback: √©chantillonnage simple
            X_sample, _, y_sample, _ = train_test_split(
                X, y, train_size=self.sample_size, 
                random_state=self.random_state
            )
        
        print(f"   ‚úÖ √âchantillon cr√©√©: {X_sample.shape}")
        return X_sample, y_sample
    
    def _select_by_correlation(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """S√©lection par corr√©lation (instantan√©)"""
        print(f"\nüìä 1. S√©lection par corr√©lation...")
        start = time.time()
        
        correlations = X.corrwith(y).abs().sort_values(ascending=False)
        selected = correlations[correlations >= self.correlation_threshold].index.tolist()
        
        # Garder au minimum les 10 meilleures
        if len(selected) < 10:
            selected = correlations.head(10).index.tolist()
        
        elapsed = time.time() - start
        print(f"   ‚ö° {len(selected)} variables en {elapsed:.2f}s")
        print(f"   üèÜ Top 3: {selected[:3]}")
        
        # Stocker les scores
        for feature in selected:
            self.feature_scores_[feature] = correlations[feature]
        
        return selected
    
    def _select_univariate(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """S√©lection univari√©e rapide"""
        print(f"\nüî¨ 2. S√©lection univari√©e (f_regression)...")
        start = time.time()
        
        k = min(self.univariate_k, X.shape[1])
        selector = SelectKBest(score_func=f_regression, k=k)
        
        try:
            selector.fit(X, y)
            selected = X.columns[selector.get_support()].tolist()
            scores = selector.scores_
            
            elapsed = time.time() - start
            print(f"   ‚ö° {len(selected)} variables en {elapsed:.2f}s")
            print(f"   üèÜ Top 3: {selected[:3]}")
            
            # Stocker les scores normalis√©s
            max_score = max(scores) if len(scores) > 0 else 1
            for i, feature in enumerate(X.columns):
                if feature in selected:
                    self.feature_scores_[feature] = scores[i] / max_score
            
            return selected
            
        except Exception as e:
            print(f"   ‚ùå Erreur: {e}")
            return []
    
    def _select_rf_light(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """Random Forest l√©ger et rapide"""
        print(f"\nüå≤ 3. Random Forest l√©ger...")
        start = time.time()
        
        try:
            rf = RandomForestRegressor(
                n_estimators=self.rf_estimators,
                max_depth=self.rf_max_depth,
                random_state=self.random_state,
                n_jobs=-1,
                min_samples_split=5,
                min_samples_leaf=2
            )
            
            rf.fit(X, y)
            
            # S√©lection bas√©e sur l'importance m√©diane
            importances = rf.feature_importances_
            threshold = np.median(importances)
            selected = [X.columns[i] for i, imp in enumerate(importances) if imp >= threshold]
            
            elapsed = time.time() - start
            print(f"   ‚ö° {len(selected)} variables en {elapsed:.2f}s")
            print(f"   üìä Seuil: {threshold:.4f}")
            print(f"   üèÜ Top 3: {selected[:3]}")
            
            # Stocker les scores
            for i, feature in enumerate(X.columns):
                if feature in selected:
                    self.feature_scores_[feature] = importances[i]
            
            return selected
            
        except Exception as e:
            print(f"   ‚ùå Erreur: {e}")
            return []
    
    def _combine_methods(self, corr_features: List[str], 
                        univariate_features: List[str], 
                        rf_features: List[str]) -> List[str]:
        """Combine intelligemment les r√©sultats"""
        print(f"\nüîó 4. Combinaison des m√©thodes...")
        
        # Compter les votes pour chaque variable
        feature_votes = {}
        all_features = set(corr_features + univariate_features + rf_features)
        
        for feature in all_features:
            votes = 0
            if feature in corr_features:
                votes += 1
            if feature in univariate_features:
                votes += 1
            if feature in rf_features:
                votes += 1
            feature_votes[feature] = votes
        
        # Variables avec au moins 2 votes
        consensus_features = [f for f, v in feature_votes.items() if v >= 2]
        
        # Ajouter les meilleures variables de chaque m√©thode
        top_from_each = set()
        top_from_each.update(corr_features[:5])
        top_from_each.update(univariate_features[:5])
        top_from_each.update(rf_features[:5])
        
        # Combinaison finale
        final_features = list(set(consensus_features + list(top_from_each)))
        
        print(f"   üìä Corr√©lation: {len(corr_features)}")
        print(f"   üìä Univari√©e: {len(univariate_features)}")
        print(f"   üìä Random Forest: {len(rf_features)}")
        print(f"   ü§ù Consensus (‚â•2 votes): {len(consensus_features)}")
        print(f"   üèÜ Top de chaque: {len(top_from_each)}")
        print(f"   ‚úÖ Final: {len(final_features)}")
        
        return final_features

    def get_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """Retourne l'importance des variables s√©lectionn√©es"""
        return self.feature_scores_


class HybridFeatureSelector(IFeatureSelector):
    """S√©lecteur hybride pour volumes moyens (20K-100K)"""
    
    def __init__(self, 
                 sample_size: int = 15000,
                 random_state: int = 42,
                 use_lasso: bool = True,
                 rf_estimators: int = 50):
        self.sample_size = sample_size
        self.random_state = random_state
        self.use_lasso = use_lasso
        self.rf_estimators = rf_estimators
        self.selected_features_ = []
        
    def select_features(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """S√©lection hybride √©quilibr√©e"""
        start_time = time.time()
        
        print(f"\nüéØ === S√âLECTION HYBRIDE √âQUILIBR√âE ===")
        print(f"üìä Donn√©es: {X.shape[0]:,} √ó {X.shape[1]}")
        
        # √âchantillonnage si n√©cessaire
        X_work, y_work = self._create_sample(X, y)
        
        selected_features = []
        
        # 1. M√©thodes rapides
        fast_selector = FastFeatureSelector(
            sample_size=min(8000, len(X_work)),
            rf_estimators=30
        )
        fast_features = fast_selector.select_features(X_work, y_work)
        selected_features.extend(fast_features)
        
        # 2. Lasso si demand√© et faisable
        if self.use_lasso and len(X_work) <= 15000:
            lasso_features = self._select_lasso(X_work, y_work)
            selected_features.extend(lasso_features)
        
        # 3. Random Forest plus sophistiqu√©
        rf_features = self._select_rf_advanced(X_work, y_work)
        selected_features.extend(rf_features)
        
        # D√©duplication et finalisation
        final_features = list(set(selected_features))
        
        elapsed = time.time() - start_time
        print(f"\n‚úÖ S√©lection hybride termin√©e en {elapsed:.1f}s")
        print(f"üéØ Variables finales: {len(final_features)}")
        
        self.selected_features_ = final_features
        return final_features
    
    def _create_sample(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
        """Cr√©e un √©chantillon si n√©cessaire"""
        if len(X) <= self.sample_size:
            return X, y
            
        print(f"üìä √âchantillonnage: {len(X):,} ‚Üí {self.sample_size:,}")
        X_sample, _, y_sample, _ = train_test_split(
            X, y, train_size=self.sample_size, 
            random_state=self.random_state
        )
        return X_sample, y_sample
    
    def _select_lasso(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """S√©lection Lasso optimis√©e"""
        print(f"\nüîç S√©lection Lasso...")
        start = time.time()
        
        try:
            # Standardisation pour Lasso
            scaler = StandardScaler()
            X_scaled = pd.DataFrame(
                scaler.fit_transform(X), 
                columns=X.columns, 
                index=X.index
            )
            
            lasso = LassoCV(cv=3, random_state=self.random_state, max_iter=2000)
            lasso.fit(X_scaled, y)
            
            selected = [X.columns[i] for i, coef in enumerate(lasso.coef_) if abs(coef) > 1e-6]
            
            elapsed = time.time() - start
            print(f"   ‚ö° {len(selected)} variables en {elapsed:.2f}s")
            
            return selected
            
        except Exception as e:
            print(f"   ‚ùå Erreur Lasso: {e}")
            return []
    
    def _select_rf_advanced(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """Random Forest plus sophistiqu√©"""
        print(f"\nüå≤ Random Forest avanc√©...")
        start = time.time()
        
        try:
            rf = RandomForestRegressor(
                n_estimators=self.rf_estimators,
                random_state=self.random_state,
                n_jobs=-1,
                oob_score=True
            )
            
            rf.fit(X, y)
            
            # S√©lection bas√©e on importance > percentile 75
            importances = rf.feature_importances_
            threshold = np.percentile(importances, 75)
            selected = [X.columns[i] for i, imp in enumerate(importances) if imp >= threshold]
            
            elapsed = time.time() - start
            print(f"   ‚ö° {len(selected)} variables en {elapsed:.2f}s")
            print(f"   üìä OOB Score: {rf.oob_score_:.3f}")
            
            return selected
            
        except Exception as e:
            print(f"   ‚ùå Erreur RF avanc√©: {e}")
            return []

    def get_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """Placeholder pour compatibilit√©"""
        return {}


class AdaptiveFeatureSelector(IFeatureSelector):
    """S√©lecteur adaptatif qui choisit automatiquement la strat√©gie optimale"""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
        self.strategy_used_ = ""
        self.selector_ = None
        
    def select_features(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """S√©lection adaptative selon la taille des donn√©es"""
        data_size = len(X)
        
        print(f"\nü§ñ === S√âLECTION ADAPTATIVE ===")
        print(f"üìä Analyse du volume: {data_size:,} propri√©t√©s")
        
        # Choisir la strat√©gie optimale
        if data_size > 100000:
            print(f"üöÄ Strat√©gie: ULTRA-RAPIDE (>100K)")
            self.selector_ = FastFeatureSelector(
                sample_size=8000,
                rf_estimators=15,
                rf_max_depth=8
            )
            self.strategy_used_ = "ultra_fast"
            
        elif data_size > 50000:
            print(f"‚ö° Strat√©gie: RAPIDE (50K-100K)")
            self.selector_ = FastFeatureSelector(
                sample_size=12000,
                rf_estimators=25,
                rf_max_depth=12
            )
            self.strategy_used_ = "fast"
            
        elif data_size > 20000:
            print(f"üéØ Strat√©gie: HYBRIDE (20K-50K)")
            self.selector_ = HybridFeatureSelector(
                sample_size=15000,
                rf_estimators=40
            )
            self.strategy_used_ = "hybrid"
            
        else:
            print(f"üî¨ Strat√©gie: STANDARD (<20K)")
            self.selector_ = FeatureSelector(rf_n_estimators=75)
            self.strategy_used_ = "standard"
        
        # Ex√©cuter la s√©lection
        selected = self.selector_.select_features(X, y)
        
        print(f"\nüìà Strat√©gie '{self.strategy_used_}' utilis√©e")
        print(f"‚úÖ {len(selected)} variables s√©lectionn√©es")
        
        return selected
    
    def get_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """D√©l√®gue au s√©lecteur utilis√©"""
        if self.selector_:
            return self.selector_.get_feature_importance(X, y)
        return {}

    def select_features_by_classification(self, 
                                        X: pd.DataFrame, 
                                        y: pd.Series,
                                        classification: pd.Series) -> Dict[str, List[str]]:
        """S√©lection adaptative par classification"""
        print(f"\nüè† === S√âLECTION ADAPTATIVE PAR TYPE ===")
        
        results = {}
        unique_categories = classification.unique()
        
        for category in unique_categories:
            if category == 'non_classifie':
                continue
                
            mask = classification == category
            X_cat = X[mask]
            y_cat = y[mask]
            
            if len(X_cat) < 50:
                print(f"   ‚ö†Ô∏è {category}: donn√©es insuffisantes ({len(X_cat)})")
                continue
            
            print(f"\n   üè∑Ô∏è {category}: {len(X_cat)} propri√©t√©s")
            
            # Adapter selon la taille de chaque cat√©gorie
            if len(X_cat) > 5000:
                selector = FastFeatureSelector(sample_size=3000, rf_estimators=15)
            elif len(X_cat) > 1000:
                selector = FastFeatureSelector(sample_size=len(X_cat), rf_estimators=25)
            else:
                # S√©lection simple pour petites cat√©gories
                try:
                    correlations = X_cat.corrwith(y_cat).abs().sort_values(ascending=False)
                    results[category] = correlations.head(min(10, len(correlations))).index.tolist()
                    print(f"      üìä {len(results[category])} variables (corr√©lation)")
                    continue
                except:
                    results[category] = []
                    continue
            
            try:
                selected = selector.select_features(X_cat, y_cat)
                results[category] = selected[:15]  # Limiter √† 15 max
                print(f"      üìä {len(results[category])} variables s√©lectionn√©es")
            except Exception as e:
                print(f"      ‚ùå Erreur: {e}")
                results[category] = []
        
        return results


class FeatureSelector(IFeatureSelector):
    """Classe pour la s√©lection de variables importantes (version am√©lior√©e)"""
    
    def __init__(self, 
                 cv_folds: int = 5, 
                 random_state: int = 42,
                 max_iter: int = 10000,
                 tolerance: float = 1e-3,
                 rf_n_estimators: int = 100,
                 rf_threshold: float = 0.01,
                 enable_optimizations: bool = True):
        self.cv_folds = cv_folds
        self.random_state = random_state
        self.max_iter = max_iter
        self.tolerance = tolerance
        self.rf_n_estimators = rf_n_estimators
        self.rf_threshold = rf_threshold
        self.enable_optimizations = enable_optimizations
        self.lasso_model = None
        self.rf_model = None
    
    def select_features(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """S√©lectionne les variables importantes en combinant Lasso et Random Forest"""
        logger.info("üéØ S√©lection de variables...")
        
        print(f"\nüéØ === S√âLECTION DE VARIABLES ===")
        print(f"üìä Variables disponibles: {X.shape[1]}")
        
        # Optimisation automatique pour gros volumes
        if self.enable_optimizations and len(X) > 50000:
            print(f"‚ö° Optimisation activ√©e pour {len(X):,} propri√©t√©s")
            adaptive_selector = AdaptiveFeatureSelector(self.random_state)
            return adaptive_selector.select_features(X, y)
        
        print(f"üìù Variables: {list(X.columns)}")
        
        # S√©lection par Lasso
        print(f"\nüîç S√©lection par Lasso (r√©gularisation L1):")
        lasso_features = self._select_lasso_features(X, y)
        
        # S√©lection par Random Forest
        print(f"\nüå≤ S√©lection par Random Forest:")
        rf_features = self._select_rf_features(X, y)
        
        # Combiner les r√©sultats
        all_selected = list(set(lasso_features + rf_features))
        
        print(f"\nüîó === COMBINAISON DES M√âTHODES ===")
        print(f"üìä Variables s√©lectionn√©es par Lasso: {len(lasso_features)}")
        print(f"üìä Variables s√©lectionn√©es par Random Forest: {len(rf_features)}")
        print(f"üìä Variables uniques s√©lectionn√©es: {len(all_selected)}")
        
        if all_selected:
            print(f"\n‚úÖ Variables finales s√©lectionn√©es:")
            for i, feature in enumerate(all_selected, 1):
                print(f"   {i:2d}. {feature}")
        else:
            print(f"‚ö†Ô∏è Aucune variable s√©lectionn√©e")
        
        return all_selected
    
    def _select_lasso_features(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """S√©lection de variables par Lasso"""
        try:
            # Configuration du mod√®le Lasso
            lasso = LassoCV(
                cv=self.cv_folds,
                random_state=self.random_state,
                max_iter=self.max_iter,
                tol=self.tolerance
            )
            
            # Entra√Ænement
            lasso.fit(X, y)
            self.lasso_model = lasso
            
            # Identifier les variables avec coefficient non-nul
            selected_features = []
            for i, coef in enumerate(lasso.coef_):
                if abs(coef) > 0:
                    selected_features.append(X.columns[i])
            
            print(f"   üìà Alpha optimal: {lasso.alpha_:.6f}")
            print(f"   üìä Variables s√©lectionn√©es: {len(selected_features)}")
            print(f"   üìù Variables: {selected_features}")
            
            return selected_features
            
        except Exception as e:
            print(f"   ‚ùå Erreur Lasso: {e}")
            return []
    
    def _select_rf_features(self, X: pd.DataFrame, y: pd.Series) -> List[str]:
        """S√©lection de variables par Random Forest"""
        try:
            # Configuration du mod√®le Random Forest
            rf = RandomForestRegressor(
                n_estimators=self.rf_n_estimators,
                random_state=self.random_state,
                n_jobs=-1
            )
            
            # Entra√Ænement
            rf.fit(X, y)
            self.rf_model = rf
            
            # Calculer l'importance des variables
            feature_importance = pd.DataFrame({
                'feature': X.columns,
                'importance': rf.feature_importances_
            }).sort_values('importance', ascending=False)
            
            # S√©lectionner les variables importantes
            threshold = self.rf_threshold
            important_features = feature_importance[feature_importance['importance'] >= threshold]['feature'].tolist()
            
            print(f"   üìä Seuil d'importance: {threshold}")
            print(f"   üìà Variables importantes: {len(important_features)}")
            print(f"   üìù Variables: {important_features}")
            
            # Afficher les 5 variables les plus importantes
            print(f"   üèÜ Top 5 variables importantes:")
            for i, (_, row) in enumerate(feature_importance.head().iterrows(), 1):
                print(f"      {i}. {row['feature']}: {row['importance']:.4f}")
            
            return important_features
            
        except Exception as e:
            print(f"   ‚ùå Erreur Random Forest: {e}")
            return []
    
    def get_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """Retourne l'importance des variables"""
        if self.rf_model is None:
            # Entra√Æner un mod√®le si n√©cessaire
            rf = RandomForestRegressor(n_estimators=100, random_state=42)
            rf.fit(X, y)
        else:
            rf = self.rf_model
        
        return dict(zip(X.columns, rf.feature_importances_))
    
    def select_features_by_classification(self, 
                                        X: pd.DataFrame, 
                                        y: pd.Series,
                                        classification: pd.Series) -> Dict[str, List[str]]:
        """S√©lectionne les variables importantes par type de propri√©t√©"""
        logger.info("üè† S√©lection de variables par classification...")
        
        # Utiliser le s√©lecteur adaptatif pour gros volumes
        if self.enable_optimizations and len(X) > 20000:
            adaptive_selector = AdaptiveFeatureSelector(self.random_state)
            return adaptive_selector.select_features_by_classification(X, y, classification)
        
        print(f"\nüè† === S√âLECTION PAR TYPE DE PROPRI√âT√â ===")
        
        classification_features = {}
        unique_categories = classification.unique()
        
        for category in unique_categories:
            if category == 'non_classifie':
                continue
                
            # Filtrer les donn√©es pour cette cat√©gorie
            mask = classification == category
            X_cat = X[mask]
            y_cat = y[mask]
            
            if len(X_cat) < 10:  # Trop peu de donn√©es
                print(f"   ‚ö†Ô∏è {category}: donn√©es insuffisantes ({len(X_cat)} √©chantillons)")
                continue
            
            print(f"\n   üè∑Ô∏è {category}: {len(X_cat)} propri√©t√©s")
            
            # S√©lection de variables pour cette cat√©gorie
            try:
                # Utiliser Random Forest pour cette cat√©gorie
                rf_cat = RandomForestRegressor(
                    n_estimators=50,
                    random_state=42,
                    n_jobs=-1
                )
                rf_cat.fit(X_cat, y_cat)
                
                # S√©lectionner les variables importantes
                feature_importance = pd.DataFrame({
                    'feature': X_cat.columns,
                    'importance': rf_cat.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Prendre les 10 variables les plus importantes
                top_features = feature_importance.head(10)['feature'].tolist()
                classification_features[category] = top_features
                
                print(f"      üìä Variables s√©lectionn√©es: {len(top_features)}")
                print(f"      üìù Top 3: {top_features[:3]}")
                
            except Exception as e:
                print(f"      ‚ùå Erreur pour {category}: {e}")
                classification_features[category] = []
        
        return classification_features 