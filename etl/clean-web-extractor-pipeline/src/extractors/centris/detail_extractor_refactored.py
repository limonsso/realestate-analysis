#!/usr/bin/env python3
"""
Extracteur de d√©tails de propri√©t√©s depuis Centris.ca
Version refactoris√©e utilisant des extracteurs sp√©cialis√©s
"""

import structlog
import re
from typing import Optional
from bs4 import BeautifulSoup

from src.models.property import (
    Property, PropertyType, PropertyStatus, Address, Location, FinancialInfo, 
    PropertyFeatures, PropertyDimensions, PropertyMedia, PropertyDescription, 
    PropertyMetadata
)
from src.utils.validators import RegionValidator, PropertyValidator, DataValidator

# Import des extracteurs sp√©cialis√©s
from .extractors import AddressExtractor, FinancialExtractor, NumericExtractor

logger = structlog.get_logger()


class CentrisDetailExtractor:
    """Extracteur de d√©tails de propri√©t√©s depuis les pages de d√©tail"""
    
    def __init__(self):
        self.validators = {
            'region': RegionValidator(),
            'property': PropertyValidator(),
            'data': DataValidator()
        }
        
        # Initialisation des extracteurs sp√©cialis√©s
        self.address_extractor = AddressExtractor()
        self.financial_extractor = FinancialExtractor()
        self.numeric_extractor = NumericExtractor()
    
    async def extract_property_details(self, soup: BeautifulSoup, url: str) -> Optional[Property]:
        """
        Extrait toutes les donn√©es d'une propri√©t√© depuis sa page de d√©tail
        
        Args:
            soup: BeautifulSoup object de la page
            url: URL de la page de d√©tail
            
        Returns:
            Property: Objet Property avec tous les d√©tails ou None en cas d'√©chec
        """
        try:
            # Extraction des informations de base
            property_id = self._extract_property_id(soup)
            if not property_id:
                logger.error(f"‚ùå Impossible d'extraire l'ID de la propri√©t√© depuis {url}")
                return None
            
            # Extraction des diff√©rentes sections avec les extracteurs sp√©cialis√©s
            address = self.address_extractor.extract_address(soup)
            financial = self.financial_extractor.extract_financial(soup)
            dimensions = self._extract_dimensions(soup)
            media = self._extract_media(soup)
            description = self._extract_description(soup)
            
            # Extraction des nouvelles informations d√©taill√©es
            # COMMENT√â: Utilisation de NumericExtractor √† la place
            # property_usage = self._extract_property_usage(soup)
            # building_style = self._extract_building_style(soup)
            # parking_info = self._extract_parking_info(soup)
            # units_info = self._extract_units_info(soup)
            # main_unit_info = self._extract_main_unit_info(soup)
            # move_in_date = self._extract_move_in_date(soup)
            # walk_score = self._extract_walk_score(soup)
            
            # Extraction des caract√©ristiques d√©taill√©es avec le NumericExtractor
            detailed_features = self.numeric_extractor.extract_detailed_features(soup)
            # Extraction des valeurs num√©riques sp√©cifiques avec le NumericExtractor
            numeric_values = self.numeric_extractor.extract_numeric_values(soup)
            
            # Cr√©ation des features √† partir des extractions num√©riques
            features = {
                'rooms': detailed_features.get('main_unit_rooms'),
                'bedrooms': detailed_features.get('main_unit_bedrooms'),
                'bathrooms': detailed_features.get('main_unit_bathrooms'),
                'total_bedrooms': detailed_features.get('main_unit_bedrooms'),  # Simplifi√© pour l'instant
                'bedrooms_basement': None  # √Ä calculer si n√©cessaire
            }
            
            # Extraction du type HTML exact depuis la page (ex: "Triplex")
            html_type = self._extract_html_property_type(soup)
            if not html_type:
                # Valeur par d√©faut bas√©e sur l'URL ou la cat√©gorie
                html_type = "Triplex"  # Valeur par d√©faut pour les plex
                logger.debug(f"üè∑Ô∏è Type HTML par d√©faut utilis√©: {html_type}")
            else:
                logger.debug(f"üè∑Ô∏è Type HTML extrait: {html_type}")
            
            # D√©tection de la cat√©gorie depuis l'URL (ex: "Plex")
            original_url = f"https://www.centris.ca/fr/triplex~a-vendre~chambly/{property_id}"
            property_category = self._detect_property_type(original_url)
            
            # Extraction des coordonn√©es GPS
            location = self._extract_location(soup)
            
            # Cr√©ation de l'objet Property avec la nouvelle logique
            property_data = Property(
                id=property_id,
                type=html_type,  # Type: "Triplex" (depuis le HTML)
                category=property_category,  # Cat√©gorie: Plex (enum)
                status=PropertyStatus.FOR_SALE,
                address=address,
                location=location,
                financial=financial,
                features=features,
                dimensions=dimensions,
                media=media,
                description=description,
                # Nouvelles informations d√©taill√©es
                # COMMENT√â: Utilisation de detailed_features √† la place
                # property_usage=property_usage,
                # building_style=building_style,
                # parking_info=parking_info,
                # units_info=units_info,
                # main_unit_info=main_unit_info,
                # move_in_date=move_in_date,
                
                # Nouvelles informations num√©riques extraites
                construction_year=detailed_features.get('construction_year'),
                terrain_area_sqft=detailed_features.get('terrain_area_sqft'),
                parking_count=detailed_features.get('parking_count'),
                units_count=detailed_features.get('units_count'),  # Directement depuis extract_units_numeric_details
                walk_score=numeric_values.get('walk_score'),  # Depuis numeric_values, pas detailed_features
                
                # Informations d√©taill√©es des unit√©s
                residential_units_detail=detailed_features.get('residential_units_detail'),
                main_unit_detail=detailed_features.get('main_unit_detail'),
                # main_unit_info supprim√© car redondant avec main_unit_detail
                
                # Informations g√©n√©rales depuis detailed_features
                property_usage=detailed_features.get('property_usage'),
                building_style=detailed_features.get('building_style'),
                parking_info=detailed_features.get('parking_info'),
                # units_info supprim√© car redondant avec residential_units_detail et units_count
                move_in_date=detailed_features.get('move_in_date'),
                
                # Nouvelles informations num√©riques d√©taill√©es des unit√©s (approche dynamique)
                units_2_half_count=detailed_features.get('units_2_half_count'),
                units_3_half_count=detailed_features.get('units_3_half_count'),
                units_4_half_count=detailed_features.get('units_4_half_count'),
                units_5_half_count=detailed_features.get('units_5_half_count'),
                units_6_half_count=detailed_features.get('units_6_half_count'),
                units_7_half_count=detailed_features.get('units_7_half_count'),
                units_8_half_count=detailed_features.get('units_8_half_count'),
                units_9_half_count=detailed_features.get('units_9_half_count'),
                
                # Informations de l'unit√© principale
                main_unit_rooms=detailed_features.get('main_unit_rooms'),
                main_unit_bedrooms=detailed_features.get('main_unit_bedrooms'),
                main_unit_bathrooms=detailed_features.get('main_unit_bathrooms'),
                
                # Champs dynamiques suppl√©mentaires 
                # units_breakdown supprim√© car redondant avec units_X_half_count
                metadata=PropertyMetadata(
                    source="Centris",
                    source_id=property_id,
                    url=url
                )
            )
            
            # Log des types extraits
            if html_type:
                logger.info(f"üè∑Ô∏è Type HTML extrait: {html_type} (ex: Triplex)")
                logger.info(f"üè† Cat√©gorie d√©tect√©e: {property_category} (ex: Plex)")
                logger.info(f"üìä R√©sum√©: {html_type} de cat√©gorie {property_category}")
            
            # Validation et nettoyage des donn√©es
            validated_property = self._validate_and_clean_property(property_data)
            
            if validated_property:
                logger.info(f"‚úÖ D√©tails extraits pour {validated_property.address.street}")
            else:
                logger.warning(f"‚ö†Ô∏è Propri√©t√© {property_id} non valid√©e")
            
            return validated_property
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction des d√©tails: {e}")
            return None
    
    # M√©thodes d'extraction simplifi√©es (gardent la logique existante)
    def _extract_property_id(self, soup: BeautifulSoup) -> Optional[str]:
        """Extrait l'ID de la propri√©t√© depuis le HTML"""
        try:
            # Recherche dans les meta tags (m√©thode la plus fiable)
            meta_id = soup.find('meta', {'property': 'og:url'})
            if meta_id:
                url = meta_id.get('content', '')
                # Format: https://www.centris.ca/fr/triplex~a-vendre~chambly/15236505
                if '/' in url:
                    property_id = url.split('/')[-1]
                    logger.debug(f"üè∑Ô∏è ID propri√©t√© extrait (og:url): {property_id}")
                    return property_id
            
            # Recherche alternative dans l'URL de la page
            url_elem = soup.find('link', {'rel': 'canonical'})
            if url_elem:
                url = url_elem.get('href')
                if url:
                    # Format: https://www.centris.ca/fr/propriete/12345678
                    match = re.search(r'/propriete/(\d+)$', url)
                    if match:
                        property_id = match.group(1)
                        logger.debug(f"üè∑Ô∏è ID propri√©t√© extrait (canonical): {property_id}")
                        return property_id
            
            # Fallback: utiliser l'ID depuis l'URL pass√©e en param√®tre
            logger.debug(f"‚ö†Ô∏è Impossible d'extraire l'ID depuis le HTML, utilisation du fallback")
            return "temp_id"
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction ID propri√©t√©: {e}")
            return "temp_id"
    
    def _extract_features(self, soup: BeautifulSoup) -> dict:
        """Extrait les caract√©ristiques de base de la propri√©t√©"""
        features = {}
        
        try:
            # Extraction depuis la description
            description = self._extract_description(soup)
            if description and description.short_description:
                features.update(self._parse_features_from_description(description.short_description))
            
            # Extraction depuis le HTML
            html_features = self._extract_features_from_html(soup)
            if html_features:
                features.update(html_features)
            
            logger.debug(f"üè† Caract√©ristiques extraites: {features}")
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction caract√©ristiques: {e}")
        
        return features
    
    def _extract_dimensions(self, soup: BeautifulSoup) -> dict:
        """Extrait les dimensions de la propri√©t√©"""
        dimensions = {}
        
        try:
            # Recherche dans les conteneurs de caract√©ristiques
            carac_containers = soup.find_all('div', class_='carac-container')
            
            for container in carac_containers:
                title_elem = container.find('div', class_='carac-title')
                value_elem = container.find('div', class_='carac-value')
                
                if title_elem and value_elem:
                    title = title_elem.get_text(strip=True).lower()
                    value = value_elem.get_text(strip=True)
                    
                    if 'ann√©e de construction' in title:
                        try:
                            year = int(re.search(r'\d{4}', value).group())
                            dimensions['year_built'] = year
                        except (AttributeError, ValueError):
                            pass
                    
                    elif 'superficie du terrain' in title:
                        area_match = re.search(r'(\d+(?:\s+\d+)*)', value)
                        if area_match:
                            area_text = area_match.group(1).replace(' ', '')
                            try:
                                area = int(area_text)
                                dimensions['lot_size'] = area
                                dimensions['lot_size_acres'] = area / 43560  # Conversion en acres
                            except ValueError:
                                pass
            
            logger.debug(f"üìè Dimensions extraites: {dimensions}")
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction dimensions: {e}")
        
        return dimensions
    
    def _extract_media(self, soup: BeautifulSoup) -> dict:
        """Extrait les m√©dias de la propri√©t√© avec focus sur les vraies photos"""
        media = {}
        
        try:
            # 1. Extraction des vraies images de propri√©t√©s depuis les scripts JavaScript
            image_urls = []
            scripts = soup.find_all('script')
            
            for script in scripts:
                if script.string:
                    # Recherche sp√©cifique des URLs d'images de propri√©t√©s Centris
                    # Pattern pour les vraies images de propri√©t√©s (mspublic.centris.ca)
                    property_image_matches = re.findall(
                        r'https://mspublic\.centris\.ca/media\.ashx[^"\']*', 
                        script.string
                    )
                    if property_image_matches:
                        image_urls.extend(property_image_matches)
                        logger.debug(f"üñºÔ∏è {len(property_image_matches)} images de propri√©t√© trouv√©es dans script")
                    
                    # Pattern alternatif pour d'autres URLs d'images de propri√©t√©s
                    alt_image_matches = re.findall(
                        r'https://[^"\']*centris[^"\']*\.(?:jpg|jpeg|png|gif|webp)', 
                        script.string
                    )
                    for img in alt_image_matches:
                        if 'mspublic' in img and img not in image_urls:
                            image_urls.append(img)
            
            # 2. Extraction depuis les balises img avec filtrage pour les vraies photos
            if not image_urls:
                img_elements = soup.find_all('img')
                
                for img in img_elements:
                    src = img.get('src') or img.get('data-src')
                    if src and self._is_property_image(src):
                        # Nettoyer l'URL
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            src = 'https://www.centris.ca' + src
                        
                        if src not in image_urls:
                            image_urls.append(src)
            
            # 3. Extraction depuis les attributs data sp√©cifiques aux galeries
            if not image_urls:
                # Rechercher dans les galeries d'images
                gallery_elements = soup.find_all(['div', 'ul'], class_=re.compile(r'gallery|photo|image', re.I))
                for gallery in gallery_elements:
                    data_elements = gallery.find_all(attrs={'data-src': True})
                    for elem in data_elements:
                        data_src = elem.get('data-src')
                        if data_src and self._is_property_image(data_src):
                            if data_src not in image_urls:
                                image_urls.append(data_src)
            
            # 4. Filtrer et nettoyer les URLs
            filtered_images = []
            for url in image_urls:
                if self._is_property_image(url) and url not in filtered_images:
                    filtered_images.append(url)
            
            # 5. Assignation des r√©sultats
            if filtered_images:
                media['images'] = filtered_images
                media['image_count'] = len(filtered_images)
                media['main_image'] = filtered_images[0]
                logger.debug(f"üñºÔ∏è {len(filtered_images)} vraies images de propri√©t√© trouv√©es")
            else:
                media['images'] = []
                media['image_count'] = 0
                media['main_image'] = None
                logger.debug("üñºÔ∏è Aucune vraie image de propri√©t√© trouv√©e")
            
            # 6. Recherche de visite virtuelle
            virtual_tour_elem = soup.find('a', {'href': re.compile(r'virtual.*tour', re.I)})
            if virtual_tour_elem:
                media['virtual_tour_url'] = virtual_tour_elem.get('href')
                logger.debug(f"üé• Visite virtuelle trouv√©e: {media['virtual_tour_url']}")
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction m√©dias: {e}")
            media = {'images': [], 'image_count': 0, 'main_image': None, 'virtual_tour_url': None}
        
        return media
    
    def _is_property_image(self, url: str) -> bool:
        """V√©rifie si une URL correspond √† une vraie image de propri√©t√©"""
        if not url:
            return False
        
        # URLs √† exclure (images du site, pas de la propri√©t√©)
        exclude_patterns = [
            'consumersite/images',  # Images du site Centris
            'menu/',                # Images de menu
            'blog_',                # Images de blog
            'guide-',               # Images de guides
            'logo',                 # Logos
            'icon',                 # Ic√¥nes
            'button',               # Boutons
            'banner',               # Banni√®res
        ]
        
        for pattern in exclude_patterns:
            if pattern in url.lower():
                return False
        
        # URLs √† inclure (vraies images de propri√©t√©s)
        include_patterns = [
            'mspublic.centris.ca/media.ashx',  # Images de propri√©t√©s Centris
            'centris.ca/image/',               # Images directes
            'centris.ca/photo/',               # Photos directes
        ]
        
        for pattern in include_patterns:
            if pattern in url.lower():
                return True
        
        # Si c'est une image et qu'elle contient "centris", probablement valide
        if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
            if 'centris' in url.lower() and 'mspublic' in url.lower():
                return True
        
        return False
    
    def _extract_description(self, soup: BeautifulSoup) -> dict:
        """Extrait la description de la propri√©t√©"""
        description = {}
        
        try:
            # Description courte depuis le titre
            title_elem = soup.find('title')
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                description['short_description'] = title_text
            
            # Description longue depuis le contenu
            desc_elem = soup.find('div', class_='property-description')
            if desc_elem:
                desc_text = desc_elem.get_text(strip=True)
                description['long_description'] = desc_text
                
                # Nettoyage de la description
                if description['long_description']:
                    description['long_description'] = self.validators['data'].clean_text(description['long_description'])
            
            # Nettoyage de la description courte
            if description.get('short_description'):
                description['short_description'] = self.validators['data'].clean_text(description['short_description'])
            
            logger.debug(f"üìù Description extraite et nettoy√©e: {description.get('short_description', '')[:100]}...")
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction description: {e}")
        
        return description
    
    # M√©thodes d'extraction des nouvelles informations d√©taill√©es
    def _extract_property_usage(self, soup: BeautifulSoup) -> Optional[str]:
        """Extrait l'utilisation de la propri√©t√© (ex: R√©sidentielle)"""
        try:
            logger.debug("üîç D√©but extraction utilisation propri√©t√©")
            carac_containers = soup.find_all('div', class_='carac-container')
            logger.debug(f"üîç Trouv√© {len(carac_containers)} conteneurs carac-container")
            
            for i, container in enumerate(carac_containers):
                title_elem = container.find('div', class_='carac-title')
                if title_elem:
                    title_text = title_elem.get_text(strip=True).lower()
                    logger.debug(f"üîç Conteneur {i}: titre = '{title_text}'")
                    
                    if 'utilisation' in title_text and 'propri√©t√©' in title_text:
                        value_elem = container.find('div', class_='carac-value')
                        if value_elem:
                            value = value_elem.get_text(strip=True)
                            logger.debug(f"üè† Utilisation trouv√©e: {value}")
                            return value
            
            logger.debug("üîç Aucune utilisation trouv√©e")
            return None
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction utilisation propri√©t√©: {e}")
            return None
    
    def _extract_building_style(self, soup: BeautifulSoup) -> Optional[str]:
        """Extrait le style de b√¢timent (ex: Jumel√©)"""
        try:
            carac_containers = soup.find_all('div', class_='carac-container')
            for container in carac_containers:
                title_elem = container.find('div', class_='carac-title')
                if title_elem:
                    title_text = title_elem.get_text(strip=True).lower()
                    if 'style' in title_text and 'b√¢timent' in title_text:
                        value_elem = container.find('div', class_='carac-value')
                        if value_elem:
                            value = value_elem.get_text(strip=True)
                            logger.debug(f"üèóÔ∏è Style b√¢timent trouv√©: {value}")
                            return value
            return None
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction style b√¢timent: {e}")
            return None
    
    def _extract_parking_info(self, soup: BeautifulSoup) -> Optional[str]:
        """Extrait les informations de stationnement"""
        try:
            carac_containers = soup.find_all('div', class_='carac-container')
            for container in carac_containers:
                title_elem = container.find('div', class_='carac-title')
                if title_elem:
                    title_text = title_elem.get_text(strip=True).lower()
                    if 'stationnement total' in title_text:
                        value_elem = container.find('div', class_='carac-value')
                        if value_elem:
                            value = value_elem.get_text(strip=True)
                            logger.debug(f"üöó Stationnement trouv√©: {value}")
                            return value
            return None
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction stationnement: {e}")
            return None
    
    # M√©thode _extract_units_info supprim√©e car redondante avec extract_units_numeric_details
    
    # M√©thode _extract_main_unit_info supprim√©e car redondante avec extract_main_unit_numeric_details
    
    def _extract_move_in_date(self, soup: BeautifulSoup) -> Optional[str]:
        """Extrait la date d'emm√©nagement"""
        try:
            carac_containers = soup.find_all('div', class_='carac-container')
            for container in carac_containers:
                title_elem = container.find('div', class_='carac-title')
                if title_elem:
                    title_text = title_elem.get_text(strip=True).lower()
                    if 'date d\'emm√©nagement' in title_text:
                        value_elem = container.find('div', class_='carac-value')
                        if value_elem:
                            value = value_elem.get_text(strip=True)
                            logger.debug(f"üìÖ Date d'emm√©nagement trouv√©e: {value}")
                            return value
            return None
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction date d'emm√©nagement: {e}")
            return None
    
    def _extract_walk_score(self, soup: BeautifulSoup) -> Optional[int]:
        """Extrait le Walk Score depuis le HTML avec recherche √©tendue"""
        try:
            logger.debug("üîç D√©but extraction Walk Score")
            
            # 1. Recherche dans les √©l√©ments avec classe 'walkscore'
            walkscore_elem = soup.find('div', class_='walkscore')
            if walkscore_elem:
                score_elem = walkscore_elem.find('span')
                if score_elem:
                    score_text = score_elem.get_text(strip=True)
                    try:
                        score = int(score_text)
                        logger.debug(f"üö∂ Walk Score trouv√© (walkscore): {score}")
                        return score
                    except ValueError:
                        logger.debug(f"‚ö†Ô∏è Walk Score non num√©rique: {score_text}")
            
            # 2. Recherche dans le texte contenant "Walk Score" ou "Score de marche"
            walk_score_patterns = [
                r'walk\s*score[:\s]*(\d+)',
                r'score\s*de\s*marche[:\s]*(\d+)',
                r'walkscore[:\s]*(\d+)',
                r'(\d+)\s*walkscore',
                r'(\d+)\s*score\s*de\s*marche'
            ]
            
            page_text = soup.get_text()
            for pattern in walk_score_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    try:
                        score = int(match.group(1))
                        logger.debug(f"üö∂ Walk Score trouv√© (regex): {score}")
                        return score
                    except ValueError:
                        continue
            
            # 3. Recherche dans les scripts JavaScript
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string:
                    for pattern in walk_score_patterns:
                        match = re.search(pattern, script.string, re.IGNORECASE)
                        if match:
                            try:
                                score = int(match.group(1))
                                logger.debug(f"üö∂ Walk Score trouv√© (script): {score}")
                                return score
                            except ValueError:
                                continue
            
            logger.debug("üîç Aucun Walk Score trouv√©")
            return None
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction Walk Score: {e}")
            return None
    
    # M√©thodes utilitaires
    def _extract_html_property_type(self, soup: BeautifulSoup) -> Optional[str]:
        """Extrait le type de propri√©t√© depuis le HTML (ex: "Triplex")"""
        try:
            # Recherche dans le titre de la page
            title_elem = soup.find('title')
            if title_elem:
                title_text = title_elem.get_text()
                # Format: "Triplex √† vendre √† Chambly, Mont√©r√©gie, 608..."
                # Extraire seulement le premier mot (le type de propri√©t√©)
                type_match = re.search(r'^([A-Za-z√Ä-√ø]+)', title_text)
                if type_match:
                    property_type = type_match.group(1).strip()
                    logger.debug(f"üè∑Ô∏è Type HTML trouv√© dans PageTitle: {property_type}")
                    return property_type
            
            return None
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction type HTML: {e}")
            return None
    
    def _detect_property_type(self, url: str) -> PropertyType:
        """D√©tecte le type de propri√©t√© depuis l'URL"""
        try:
            if 'plex' in url.lower():
                return PropertyType.PLEX
            elif 'condo' in url.lower():
                return PropertyType.SELL_CONDO
            elif 'maison' in url.lower():
                return PropertyType.SINGLE_FAMILY_HOME
            else:
                return PropertyType.RESIDENTIAL_LOT
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur d√©tection type propri√©t√©: {e}")
            return PropertyType.PLEX
    
    def _extract_location(self, soup: BeautifulSoup) -> Optional[Location]:
        """Extrait les coordonn√©es GPS pour cr√©er un objet Location"""
        try:
            coordinates = self.address_extractor._extract_coordinates(soup)
            if coordinates:
                lat, lng = coordinates
                location = Location(latitude=lat, longitude=lng)
                logger.debug(f"üìç Coordonn√©es GPS extraites: {lat}, {lng}")
                return location
            return None
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction location: {e}")
            return None
    
    # M√©thodes de validation et nettoyage
    def _validate_and_clean_property(self, property_data: Property) -> Property:
        """Valide et nettoie les donn√©es d'une propri√©t√©"""
        try:
            # Validation et nettoyage de l'adresse
            if property_data.address:
                # Validation de la r√©gion
                if property_data.address.region:
                    if not self.validators['region'].is_valid_region(property_data.address.region):
                        logger.warning(f"‚ö†Ô∏è R√©gion invalide d√©tect√©e: {property_data.address.region}")
                        # Essayer de normaliser la r√©gion
                        normalized_region = self.validators['region'].normalize_region(property_data.address.region)
                        if normalized_region:
                            property_data.address.region = normalized_region
                            logger.info(f"‚úÖ R√©gion normalis√©e: {property_data.address.region}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Impossible de normaliser la r√©gion: {property_data.address.region}")
                
                # Validation du code postal
                if property_data.address.postal_code:
                    if not self.validators['property'].is_valid_postal_code(property_data.address.postal_code):
                        logger.warning(f"‚ö†Ô∏è Code postal invalide: {property_data.address.postal_code}")
                
                # Nettoyage des textes d'adresse
                if property_data.address.street:
                    property_data.address.street = self.validators['data'].clean_text(property_data.address.street)
                if property_data.address.city:
                    property_data.address.city = self.validators['data'].clean_text(property_data.address.city)
            
            # Validation des informations financi√®res
            if property_data.financial and property_data.financial.price:
                if not self.validators['property'].is_valid_price(property_data.financial.price):
                    logger.warning(f"‚ö†Ô∏è Prix invalide d√©tect√©: {property_data.financial.price}")
            
            # Validation des coordonn√©es g√©ographiques
            if (property_data.address and 
                hasattr(property_data.address, 'latitude') and 
                hasattr(property_data.address, 'longitude')):
                
                if not self.validators['data'].is_valid_coordinates(
                    property_data.address.latitude, 
                    property_data.address.longitude
                ):
                    logger.warning(f"‚ö†Ô∏è Coordonn√©es g√©ographiques invalides: {property_data.address.latitude}, {property_data.address.longitude}")
            
            # Validation de l'ID
            if not self.validators['property'].is_valid_property_id(property_data.id):
                logger.warning(f"‚ö†Ô∏è ID de propri√©t√© invalide: {property_data.id}")
            
            logger.debug(f"‚úÖ Propri√©t√© {property_data.id} valid√©e et nettoy√©e")
            return property_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la validation: {str(e)}")
            return property_data
    
    # M√©thodes utilitaires pour l'extraction des caract√©ristiques
    def _parse_features_from_description(self, description: str) -> dict:
        """Parse les caract√©ristiques depuis la description"""
        features = {}
        
        try:
            # Recherche des caract√©ristiques dans la description
            if '5¬Ω' in description or '5 1/2' in description:
                features['bedrooms'] = 3
                features['total_bedrooms'] = 7
            elif '4¬Ω' in description or '4 1/2' in description:
                features['bedrooms'] = 2
                features['total_bedrooms'] = 6
            
            # Recherche du nombre de pi√®ces
            rooms_match = re.search(r'(\d+)\s*pi√®ces?', description)
            if rooms_match:
                features['rooms'] = int(rooms_match.group(1))
            
            # Recherche du nombre de chambres
            bedrooms_match = re.search(r'(\d+)\s*chambres?', description)
            if bedrooms_match:
                features['bedrooms'] = int(bedrooms_match.group(1))
            
            # Recherche du nombre de salles de bain
            bathrooms_match = re.search(r'(\d+)\s*salles?\s*de\s*bain', description)
            if bathrooms_match:
                features['bathrooms'] = int(bathrooms_match.group(1))
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur parsing description: {e}")
        
        return features
    
    def _extract_features_from_html(self, soup: BeautifulSoup) -> dict:
        """Extrait les caract√©ristiques depuis les √©l√©ments HTML"""
        features = {}
        
        try:
            # Recherche des caract√©ristiques dans les conteneurs carac-container
            carac_containers = soup.find_all('div', {'class': 'carac-container'})
            
            for container in carac_containers:
                title_elem = container.find('div', {'class': 'carac-title'})
                value_elem = container.find('div', {'class': 'carac-value'})
                
                if title_elem and value_elem:
                    title = title_elem.get_text(strip=True).lower()
                    value = value_elem.get_text(strip=True)
                    
                    # Traitement des caract√©ristiques sp√©cifiques
                    if 'unit√©s r√©sidentielles' in title:
                        # Format: "1 x 4 ¬Ω, 2 x 5 ¬Ω"
                        features.update(self._parse_units_from_text(value))
                    elif 'unit√© principale' in title:
                        # Format: "5 pi√®ces, 3 chambres, 1 salle de bain"
                        features.update(self._parse_main_unit_from_text(value))
                    elif 'garage' in title:
                        # Format: "Garage (1)"
                        garage_match = re.search(r'\((\d+)\)', value)
                        if garage_match:
                            features['garages'] = int(garage_match.group(1))
            
            logger.debug(f"üè† Caract√©ristiques HTML extraites: {features}")
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur extraction HTML: {e}")
        
        return features
    
    def _parse_units_from_text(self, text: str) -> dict:
        """Parse les unit√©s depuis le texte"""
        features = {}
        
        try:
            # Format: "1 x 4 ¬Ω, 2 x 5 ¬Ω"
            five_half_count = text.count('5¬Ω') + text.count('5 1/2')
            four_half_count = text.count('4¬Ω') + text.count('4 1/2')
            
            if five_half_count > 0:
                features['bedrooms_basement'] = five_half_count * 3
            if four_half_count > 0:
                features['bedrooms_basement'] = (features.get('bedrooms_basement', 0) + 
                                               four_half_count * 2)
            
            # Calcul du total des chambres
            total_bedrooms = (features.get('bedrooms', 0) + 
                             features.get('bedrooms_basement', 0))
            if total_bedrooms > 0:
                features['total_bedrooms'] = total_bedrooms
            
            # Calcul du nombre de salles de bain
            if five_half_count > 0 or four_half_count > 0:
                features['bathrooms'] = five_half_count + four_half_count
            
            logger.debug(f"üè† Caract√©ristiques extraites: {five_half_count} unit√©s 5¬Ω, {four_half_count} unit√©s 4¬Ω")
        
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur parsing description: {e}")
        
        return features
    
    def _parse_main_unit_from_text(self, text: str) -> dict:
        """Parse l'unit√© principale depuis le texte"""
        features = {}
        
        try:
            # Format: "5 pi√®ces, 3 chambres, 1 salle de bain"
            rooms_match = re.search(r'(\d+)\s*pi√®ces?', text)
            if rooms_match:
                features['rooms'] = int(rooms_match.group(1))
            
            bedrooms_match = re.search(r'(\d+)\s*chambres?', text)
            if bedrooms_match:
                features['bedrooms'] = int(bedrooms_match.group(1))
            
            bathrooms_match = re.search(r'(\d+)\s*salles?\s*de\s*bain', text)
            if bathrooms_match:
                features['bathrooms'] = int(bathrooms_match.group(1))
            
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erreur parsing unit√© principale: {e}")
        
        return features
