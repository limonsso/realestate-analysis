#!/usr/bin/env python3
"""
Script de d√©bogage d√©taill√© pour SummaryExtractor
Analyse chaque √©tape de l'extraction des propri√©t√©s
"""

import asyncio
import structlog
from src.extractors.centris_extractor import CentrisExtractor
from config.settings import config, LocationConfig
from src.utils.logging import setup_logging
from pydantic import BaseModel
from typing import List
from enum import Enum
from bs4 import BeautifulSoup

# Configuration du logging
setup_logging()
logger = structlog.get_logger(__name__)

# D√©finition locale des mod√®les
class PropertyType(str, Enum):
    PLEX = "Plex"
    SINGLE_FAMILY_HOME = "SingleFamilyHome"
    SELL_CONDO = "SellCondo"

class SearchQuery(BaseModel):
    locations: List[LocationConfig]
    property_types: List[PropertyType]
    price_min: float = 200000
    price_max: float = 260000

async def debug_extraction_details():
    """D√©bogage d√©taill√© de l'extraction"""
    logger.info("üîç D√©marrage du d√©bogage d√©taill√© de l'extraction...")
    
    extractor = None
    try:
        # Initialisation
        extractor = CentrisExtractor(config.centris)
        
        # Test avec une recherche simple
        search_query = SearchQuery(
            locations=[
                LocationConfig(
                    type="GeographicArea",
                    value="Mont√©r√©gie",
                    type_id="RARA16"
                )
            ],
            property_types=[PropertyType.PLEX],
            price_min=200000,
            price_max=260000
        )
        
        logger.info(f"üîç Test de recherche: {search_query.locations[0].value}")
        
        # R√©cup√©ration des pages HTML
        pages = await extractor.search_manager.search_with_pagination(search_query, max_pages=1)
        
        if not pages:
            logger.error("‚ùå Aucune page r√©cup√©r√©e")
            return
            
        first_page = pages[0]
        logger.info(f"üìÑ Page analys√©e: {len(first_page)} caract√®res")
        
        # Test direct du parsing HTML
        logger.info(f"\nüß™ TEST DIRECT DU PARSING HTML")
        logger.info("="*60)
        
        soup = BeautifulSoup(first_page, 'html.parser')
        
        # Recherche des conteneurs de propri√©t√©s
        property_containers = soup.find_all('div', class_='property-thumbnail-item')
        logger.info(f"üîç Conteneurs trouv√©s: {len(property_containers)}")
        
        if not property_containers:
            # Essayer d'autres s√©lecteurs
            property_containers = soup.find_all('div', class_='thumbnailItem')
            logger.info(f"üîç Conteneurs thumbnailItem: {len(property_containers)}")
            
        if not property_containers:
            property_containers = soup.find_all('div', attrs={'itemscope': True, 'itemtype': 'http://schema.org/Product'})
            logger.info(f"üîç Conteneurs schema.org: {len(property_containers)}")
        
        if not property_containers:
            logger.error("‚ùå Aucun conteneur de propri√©t√© trouv√©")
            return
            
        # Analyser le premier conteneur en d√©tail
        first_container = property_containers[0]
        logger.info(f"\nüîç ANALYSE DU PREMIER CONTENEUR")
        logger.info("="*60)
        
        # Afficher la structure HTML du conteneur
        logger.info(f"üìã Structure HTML du conteneur:")
        logger.info("-" * 40)
        container_html = str(first_container)[:1000]  # Limiter √† 1000 caract√®res
        logger.info(container_html)
        
        # Test des m√©thodes d'extraction une par une
        logger.info(f"\nüß™ TEST DES M√âTHODES D'EXTRACTION")
        logger.info("="*60)
        
        # 1. Test extraction ID
        logger.info("üîë Test extraction ID...")
        try:
            id_element = first_container.find('a', href=True)
            if id_element:
                href = id_element.get('href', '')
                logger.info(f"‚úÖ Lien trouv√©: {href}")
                if '/property/' in href:
                    property_id = href.split('/property/')[-1].split('/')[0]
                    logger.info(f"‚úÖ ID extrait: {property_id}")
                else:
                    logger.warning(f"‚ö†Ô∏è Format de lien non reconnu: {href}")
            else:
                logger.warning("‚ö†Ô∏è Aucun lien trouv√©")
                
            # Test des attributs data
            data_id = first_container.get('data-property-id') or first_container.get('data-id')
            if data_id:
                logger.info(f"‚úÖ ID dans data: {data_id}")
            else:
                logger.warning("‚ö†Ô∏è Aucun ID dans les attributs data")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction ID: {e}")
        
        # 2. Test extraction adresse
        logger.info("\nüìç Test extraction adresse...")
        try:
            address_element = (
                first_container.find('div', {'class': 'location-container'}) or
                first_container.find('div', {'class': 'address'}) or
                first_container.find('span', {'class': 'address'})
            )
            
            if address_element:
                logger.info(f"‚úÖ √âl√©ment adresse trouv√©: {address_element.name}.{address_element.get('class', [])}")
                
                # Chercher le texte d'adresse
                address_text_element = address_element.find('div', {'class': 'address'})
                if address_text_element:
                    address_text = address_text_element.get_text(strip=True)
                    logger.info(f"‚úÖ Texte adresse: '{address_text}'")
                else:
                    address_text = address_element.get_text(strip=True)
                    logger.info(f"‚úÖ Texte adresse (conteneur): '{address_text}'")
            else:
                logger.warning("‚ö†Ô∏è Aucun √©l√©ment adresse trouv√©")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction adresse: {e}")
        
        # 3. Test extraction prix
        logger.info("\nüí∞ Test extraction prix...")
        try:
            price_element = (
                first_container.find('div', {'class': 'plex-revenue'}) or
                first_container.find('div', {'class': 'price'}) or
                first_container.find('span', {'class': 'price'})
            )
            
            if price_element:
                logger.info(f"‚úÖ √âl√©ment prix trouv√©: {price_element.name}.{price_element.get('class', [])}")
                price_text = price_element.get_text(strip=True)
                logger.info(f"‚úÖ Texte prix: '{price_text}'")
                
                # Test du nettoyage du prix
                price_clean = ''.join(filter(str.isdigit, price_text))
                if price_clean:
                    logger.info(f"‚úÖ Prix num√©rique extrait: {price_clean}")
                else:
                    logger.warning(f"‚ö†Ô∏è Impossible d'extraire le prix num√©rique de: '{price_text}'")
            else:
                logger.warning("‚ö†Ô∏è Aucun √©l√©ment prix trouv√©")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction prix: {e}")
        
        # 4. Test extraction type
        logger.info("\nüè† Test extraction type...")
        try:
            type_element = (
                first_container.find('div', {'class': 'category'}) or
                first_container.find('span', {'class': 'property-type'}) or
                first_container.find('div', {'class': 'property-type'})
            )
            
            if type_element:
                logger.info(f"‚úÖ √âl√©ment type trouv√©: {type_element.name}.{type_element.get('class', [])}")
                type_text = type_element.get_text(strip=True)
                logger.info(f"‚úÖ Texte type: '{type_text}'")
            else:
                logger.warning("‚ö†Ô∏è Aucun √©l√©ment type trouv√©")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction type: {e}")
        
        # Test complet de l'extraction
        logger.info(f"\nüß™ TEST COMPLET DE L'EXTRACTION")
        logger.info("="*60)
        
        try:
            summaries = await extractor.extract_summaries(search_query)
            if summaries:
                logger.info(f"‚úÖ Extraction r√©ussie: {len(summaries)} propri√©t√©s")
                for i, summary in enumerate(summaries[:3]):
                    logger.info(f"   üè† Propri√©t√© {i+1}: {summary.id} - {summary.price}$ - {summary.address.street}")
            else:
                logger.warning("‚ö†Ô∏è Aucune propri√©t√© extraite")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        # R√©sum√© du d√©bogage
        logger.info(f"\nüìä R√âSUM√â DU D√âBOGAGE D√âTAILL√â")
        logger.info("="*60)
        logger.info(f"üìÑ Page analys√©e: {len(first_page)} caract√®res")
        logger.info(f"üîç Conteneurs trouv√©s: {len(property_containers)}")
        logger.info(f"üìè Premier conteneur: {len(str(first_container))} caract√®res")
        
        if len(property_containers) > 0:
            logger.info("‚úÖ Les conteneurs de propri√©t√©s sont trouv√©s")
            logger.info("üîß Le probl√®me est dans l'extraction des donn√©es individuelles")
        else:
            logger.warning("‚ö†Ô∏è Aucun conteneur de propri√©t√© trouv√©")
            
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du d√©bogage d√©taill√©: {e}")
        import traceback
        logger.error(traceback.format_exc())
        
    finally:
        if extractor:
            await extractor.close()
            logger.info("üîå CentrisExtractor ferm√©")

if __name__ == "__main__":
    asyncio.run(debug_extraction_details())
