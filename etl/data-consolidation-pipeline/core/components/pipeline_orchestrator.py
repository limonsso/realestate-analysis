#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¼ ORCHESTRATEUR DE PIPELINE - Composant principal
==================================================

Module principal d'orchestration du pipeline ETL complet
Coordonne tous les composants et gÃ¨re le flux de donnÃ©es
"""

import pandas as pd
import logging
from typing import Dict, List, Optional, Any, Tuple
import warnings
from datetime import datetime
import json
import os
from pathlib import Path

# Imports des composants spÃ©cialisÃ©s
from .data_extractor import DataExtractor
from .data_consolidator import DataConsolidator
from .data_cleaner import DataCleaner
from .data_enricher import DataEnricher
from .data_validator import DataValidator

# Imports des modules externes (avec gestion d'erreur pour compatibilitÃ©)
try:
    from ...config.consolidation_config import ConsolidationConfig
    from ...export.advanced_exporter import AdvancedExporter
    from ...performance.performance_optimizer import PerformanceOptimizer
    from ...dashboard.validation_dashboard import ValidationDashboard
    EXTERNAL_MODULES_AVAILABLE = True
except ImportError:
    try:
        from config.consolidation_config import ConsolidationConfig
        from export.advanced_exporter import AdvancedExporter
        from performance.performance_optimizer import PerformanceOptimizer
        from dashboard.validation_dashboard import ValidationDashboard
        EXTERNAL_MODULES_AVAILABLE = True
    except ImportError:
        EXTERNAL_MODULES_AVAILABLE = False
        
        # Fallback classes pour quand les modules externes ne sont pas disponibles
        class ConsolidationConfig:
            def __init__(self):
                self.consolidation_groups = {}
        
        class AdvancedExporter:
            def __init__(self):
                pass
            
            def export_data(self, *args, **kwargs):
                return {"success": False, "message": "AdvancedExporter not available"}
        
        class PerformanceOptimizer:
            def __init__(self):
                pass
            
            def optimize_dataframe(self, df):
                return df
        
        class ValidationDashboard:
            def __init__(self):
                pass
            
            def generate_dashboard(self, *args, **kwargs):
                return {"success": False, "message": "ValidationDashboard not available"}

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

class PipelineOrchestrator:
    """
    Orchestrateur principal du pipeline ETL
    Coordonne tous les composants et gÃ¨re le flux de donnÃ©es
    """
    
    def __init__(self, config: ConsolidationConfig = None):
        """
        Initialise l'orchestrateur du pipeline
        
        Args:
            config: Configuration de consolidation
        """
        self.config = config or ConsolidationConfig()
        self.pipeline_history = []
        self.pipeline_stats = {}
        
        # === INITIALISATION DES COMPOSANTS ===
        logger.info("ğŸ¼ === INITIALISATION PIPELINE ORCHESTRATOR ===")
        
        # Validation de la configuration
        if not self.config.validate_configuration():
            raise ValueError("âŒ Configuration de consolidation invalide")
        
        # Initialisation des composants
        self.data_extractor = DataExtractor()
        self.data_consolidator = DataConsolidator(self.config)
        self.data_cleaner = DataCleaner()
        self.data_enricher = DataEnricher()
        self.data_validator = DataValidator()
        
        # Initialisation des modules externes
        self.advanced_exporter = AdvancedExporter()
        self.performance_optimizer = PerformanceOptimizer()
        self.validation_dashboard = ValidationDashboard()
        
        logger.info("âœ… Tous les composants initialisÃ©s avec succÃ¨s")
        self.config.log_configuration()
    
    def run_complete_pipeline(self, input_source: str = "mongodb", 
                             input_config: Dict = None, 
                             output_config: Dict = None) -> Dict[str, Any]:
        """
        ExÃ©cute le pipeline ETL complet
        
        Args:
            input_source: Source des donnÃ©es ("mongodb", "csv", "json", "test")
            input_config: Configuration d'entrÃ©e
            output_config: Configuration de sortie
            
        Returns:
            Dict avec les rÃ©sultats complets du pipeline
        """
        pipeline_start = datetime.now()
        logger.info("ğŸš€ === DÃ‰MARRAGE PIPELINE ETL COMPLET ===")
        
        try:
            # === PHASE 1: EXTRACTION ===
            logger.info("ğŸ“¥ === PHASE 1: EXTRACTION ===")
            df_extracted = self._execute_extraction_phase(input_source, input_config)
            
            if df_extracted is None or df_extracted.empty:
                raise ValueError("âŒ Aucune donnÃ©e extraite")
            
            logger.info(f"âœ… Extraction rÃ©ussie: {df_extracted.shape[0]} lignes Ã— {df_extracted.shape[1]} colonnes")
            
            # === PHASE 2: NETTOYAGE ===
            logger.info("ğŸ§¹ === PHASE 2: NETTOYAGE ===")
            df_cleaned = self._execute_cleaning_phase(df_extracted)
            
            # === PHASE 3: CONSOLIDATION ===
            logger.info("ğŸ”— === PHASE 3: CONSOLIDATION ===")
            df_consolidated = self._execute_consolidation_phase(df_cleaned)
            
            # === PHASE 4: ENRICHISSEMENT ===
            logger.info("ğŸš€ === PHASE 4: ENRICHISSEMENT ===")
            df_enriched = self._execute_enrichment_phase(df_consolidated)
            
            # === PHASE 5: VALIDATION ===
            logger.info("âœ… === PHASE 5: VALIDATION ===")
            validation_results = self._execute_validation_phase(df_enriched)
            
            # === PHASE 6: OPTIMISATION ===
            logger.info("âš¡ === PHASE 6: OPTIMISATION ===")
            df_optimized = self._execute_optimization_phase(df_enriched)
            
            # === PHASE 7: EXPORT ===
            logger.info("ğŸ“¤ === PHASE 7: EXPORT ===")
            export_results = self._execute_export_phase(df_optimized, output_config)
            
            # === PHASE 8: GÃ‰NÃ‰RATION DES RAPPORTS ===
            logger.info("ğŸ“Š === PHASE 8: GÃ‰NÃ‰RATION DES RAPPORTS ===")
            report_results = self._execute_reporting_phase(df_optimized, validation_results, export_results)
            
            # === PHASE 9: DASHBOARD ===
            logger.info("ğŸ“ˆ === PHASE 9: DASHBOARD ===")
            dashboard_results = self._execute_dashboard_phase(df_optimized, validation_results)
            
            # Compilation des rÃ©sultats finaux
            pipeline_results = self._compile_pipeline_results(
                df_extracted, df_cleaned, df_consolidated, df_enriched, 
                df_optimized, validation_results, export_results, 
                report_results, dashboard_results
            )
            
            # Statistiques du pipeline
            pipeline_end = datetime.now()
            self.pipeline_stats = {
                'start_time': pipeline_start,
                'end_time': pipeline_end,
                'total_duration': (pipeline_end - pipeline_start).total_seconds(),
                'phases_executed': 9,
                'input_shape': df_extracted.shape if df_extracted is not None else (0, 0),
                'output_shape': df_optimized.shape if df_optimized is not None else (0, 0),
                'data_reduction_percentage': self._calculate_data_reduction(df_extracted, df_optimized),
                'overall_quality_score': validation_results.get('overall_status', {}).get('quality_score', 0.0),
                'export_success': export_results.get('success', False),
                'dashboard_generated': dashboard_results.get('success', False)
            }
            
            # Enregistrement dans l'historique
            self.pipeline_history.append({
                'timestamp': pipeline_start.isoformat(),
                'input_source': input_source,
                'results': pipeline_results,
                'stats': self.pipeline_stats
            })
            
            logger.info("ğŸ‰ === PIPELINE ETL COMPLET TERMINÃ‰ AVEC SUCCÃˆS ===")
            logger.info(f"â±ï¸ DurÃ©e totale: {self.pipeline_stats['total_duration']:.2f}s")
            logger.info(f"ğŸ“Š RÃ©duction donnÃ©es: {self.pipeline_stats['data_reduction_percentage']:.1f}%")
            logger.info(f"â­ Score qualitÃ©: {self.pipeline_stats['overall_quality_score']:.1%}")
            
            return pipeline_results
            
        except Exception as e:
            logger.error(f"âŒ Erreur dans le pipeline: {e}")
            self._handle_pipeline_error(e, pipeline_start)
            raise
    
    def _execute_extraction_phase(self, input_source: str, input_config: Dict = None) -> pd.DataFrame:
        """ExÃ©cution de la phase d'extraction"""
        try:
            logger.info(f"ğŸ“¥ Extraction depuis: {input_source}")
            
            # Extraction des donnÃ©es
            df = self.data_extractor.extract_data(input_source, input_config)
            
            # Validation des donnÃ©es extraites
            if not self.data_extractor.validate_extracted_data(df):
                logger.warning("âš ï¸ Validation des donnÃ©es extraites Ã©chouÃ©e")
            
            # Statistiques d'extraction
            extraction_stats = self.data_extractor.get_extraction_stats()
            logger.info(f"ğŸ“Š Statistiques d'extraction: {extraction_stats}")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ Erreur phase extraction: {e}")
            raise
    
    def _execute_cleaning_phase(self, df: pd.DataFrame) -> pd.DataFrame:
        """ExÃ©cution de la phase de nettoyage"""
        try:
            logger.info("ğŸ§¹ DÃ©but du nettoyage des donnÃ©es")
            
            # Nettoyage des donnÃ©es
            df_cleaned = self.data_cleaner.clean_data(df)
            
            # Validation de la qualitÃ© aprÃ¨s nettoyage
            quality_scores = self.data_cleaner.validate_data_quality(df_cleaned)
            logger.info(f"ğŸ“Š Scores de qualitÃ© aprÃ¨s nettoyage: {quality_scores}")
            
            # Statistiques de nettoyage
            cleaning_stats = self.data_cleaner.get_cleaning_stats()
            logger.info(f"ğŸ“Š Statistiques de nettoyage: {cleaning_stats}")
            
            return df_cleaned
            
        except Exception as e:
            logger.error(f"âŒ Erreur phase nettoyage: {e}")
            raise
    
    def _execute_consolidation_phase(self, df: pd.DataFrame) -> pd.DataFrame:
        """ExÃ©cution de la phase de consolidation"""
        try:
            logger.info("ğŸ”— DÃ©but de la consolidation des variables")
            
            # Consolidation des variables
            df_consolidated = self.data_consolidator.consolidate_variables(df)
            
            # RÃ©sultats de consolidation
            consolidation_results = self.data_consolidator.get_consolidation_results()
            consolidation_stats = self.data_consolidator.get_consolidation_stats()
            
            logger.info(f"ğŸ“Š RÃ©sultats de consolidation: {len(consolidation_results)} groupes traitÃ©s")
            logger.info(f"ğŸ“Š Statistiques de consolidation: {consolidation_stats}")
            
            return df_consolidated
            
        except Exception as e:
            logger.error(f"âŒ Erreur phase consolidation: {e}")
            raise
    
    def _execute_enrichment_phase(self, df: pd.DataFrame) -> pd.DataFrame:
        """ExÃ©cution de la phase d'enrichissement"""
        try:
            logger.info("ğŸš€ DÃ©but de l'enrichissement des donnÃ©es")
            
            # Enrichissement des donnÃ©es
            df_enriched = self.data_enricher.enrich_data(df)
            
            # RÃ©sultats d'enrichissement
            enrichment_results = self.data_enricher.get_enrichment_results()
            enrichment_stats = self.data_enricher.get_enrichment_stats()
            enriched_columns = self.data_enricher.get_enriched_columns()
            
            logger.info(f"ğŸ“Š RÃ©sultats d'enrichissement: {len(enrichment_results)} phases")
            logger.info(f"ğŸ“Š Statistiques d'enrichissement: {enrichment_stats}")
            logger.info(f"â• Colonnes enrichies: {enriched_columns}")
            
            return df_enriched
            
        except Exception as e:
            logger.error(f"âŒ Erreur phase enrichissement: {e}")
            raise
    
    def _execute_validation_phase(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ExÃ©cution de la phase de validation"""
        try:
            logger.info("âœ… DÃ©but de la validation des donnÃ©es")
            
            # Validation complÃ¨te des donnÃ©es
            validation_results = self.data_validator.validate_data(df)
            
            # Statistiques de validation
            validation_stats = self.data_validator.get_validation_stats()
            
            logger.info(f"ğŸ“Š RÃ©sultats de validation: {len(validation_results)} catÃ©gories")
            logger.info(f"ğŸ“Š Statistiques de validation: {validation_stats}")
            
            # GÃ©nÃ©ration du rapport de validation
            validation_report = self.data_validator.generate_validation_report()
            logger.info("ğŸ“‹ Rapport de validation gÃ©nÃ©rÃ©")
            
            return validation_results
            
        except Exception as e:
            logger.error(f"âŒ Erreur phase validation: {e}")
            raise
    
    def _execute_optimization_phase(self, df: pd.DataFrame) -> pd.DataFrame:
        """ExÃ©cution de la phase d'optimisation"""
        try:
            logger.info("âš¡ DÃ©but de l'optimisation des performances")
            
            # Optimisation des performances
            df_optimized = self.performance_optimizer.optimize_dataframe(df)
            
            # Statistiques d'optimisation
            optimization_stats = self.performance_optimizer.get_optimization_stats()
            logger.info(f"ğŸ“Š Statistiques d'optimisation: {optimization_stats}")
            
            return df_optimized
            
        except Exception as e:
            logger.error(f"âŒ Erreur phase optimisation: {e}")
            # En cas d'erreur, retourner les donnÃ©es non optimisÃ©es
            logger.warning("âš ï¸ Retour des donnÃ©es non optimisÃ©es")
            return df
    
    def _execute_export_phase(self, df: pd.DataFrame, output_config: Dict = None) -> Dict[str, Any]:
        """ExÃ©cution de la phase d'export"""
        try:
            logger.info("ğŸ“¤ DÃ©but de l'export des donnÃ©es")
            
            # Configuration d'export par dÃ©faut
            if not output_config:
                output_config = {
                    'output_dir': 'exports/',
                    'formats': ['csv', 'parquet'],
                    'filename_prefix': 'real_estate_data_consolidated'
                }
            
            # Export des donnÃ©es
            export_results = self.advanced_exporter.export_data(df, output_config)
            
            logger.info(f"ğŸ“¤ Export terminÃ©: {export_results}")
            return export_results
            
        except Exception as e:
            logger.error(f"âŒ Erreur phase export: {e}")
            return {'success': False, 'error': str(e)}
    
    def _execute_reporting_phase(self, df: pd.DataFrame, validation_results: Dict, 
                                export_results: Dict) -> Dict[str, Any]:
        """ExÃ©cution de la phase de gÃ©nÃ©ration de rapports"""
        try:
            logger.info("ğŸ“Š DÃ©but de la gÃ©nÃ©ration des rapports")
            
            # CrÃ©ation du rÃ©pertoire de rapports
            reports_dir = Path('exports/reports')
            reports_dir.mkdir(parents=True, exist_ok=True)
            
            # Rapport de validation
            validation_report = self.data_validator.generate_validation_report()
            validation_report_path = reports_dir / f"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            
            with open(validation_report_path, 'w', encoding='utf-8') as f:
                f.write(validation_report)
            
            # Rapport de pipeline
            pipeline_report = self._generate_pipeline_report(df, validation_results, export_results)
            pipeline_report_path = reports_dir / f"pipeline_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            
            with open(pipeline_report_path, 'w', encoding='utf-8') as f:
                f.write(pipeline_report)
            
            report_results = {
                'success': True,
                'validation_report_path': str(validation_report_path),
                'pipeline_report_path': str(pipeline_report_path),
                'reports_generated': 2
            }
            
            logger.info(f"ğŸ“Š Rapports gÃ©nÃ©rÃ©s: {report_results}")
            return report_results
            
        except Exception as e:
            logger.error(f"âŒ Erreur phase rapports: {e}")
            return {'success': False, 'error': str(e)}
    
    def _execute_dashboard_phase(self, df: pd.DataFrame, validation_results: Dict) -> Dict[str, Any]:
        """ExÃ©cution de la phase de gÃ©nÃ©ration du dashboard"""
        try:
            logger.info("ğŸ“ˆ DÃ©but de la gÃ©nÃ©ration du dashboard")
            
            # GÃ©nÃ©ration du dashboard de validation
            dashboard_results = self.validation_dashboard.generate_dashboard(df, validation_results)
            
            logger.info(f"ğŸ“ˆ Dashboard gÃ©nÃ©rÃ©: {dashboard_results}")
            return dashboard_results
            
        except Exception as e:
            logger.error(f"âŒ Erreur phase dashboard: {e}")
            return {'success': False, 'error': str(e)}
    
    def _compile_pipeline_results(self, df_extracted: pd.DataFrame, df_cleaned: pd.DataFrame,
                                 df_consolidated: pd.DataFrame, df_enriched: pd.DataFrame,
                                 df_optimized: pd.DataFrame, validation_results: Dict,
                                 export_results: Dict, report_results: Dict,
                                 dashboard_results: Dict) -> Dict[str, Any]:
        """Compilation des rÃ©sultats finaux du pipeline"""
        try:
            pipeline_results = {
                'pipeline_status': 'SUCCESS',
                'timestamp': datetime.now().isoformat(),
                'data_flow': {
                    'extraction': {
                        'shape': df_extracted.shape if df_extracted is not None else (0, 0),
                        'columns': list(df_extracted.columns) if df_extracted is not None else [],
                        'memory_usage_mb': df_extracted.memory_usage(deep=True).sum() / 1024 / 1024 if df_extracted is not None else 0
                    },
                    'cleaning': {
                        'shape': df_cleaned.shape if df_cleaned is not None else (0, 0),
                        'columns': list(df_cleaned.columns) if df_cleaned is not None else [],
                        'memory_usage_mb': df_cleaned.memory_usage(deep=True).sum() / 1024 / 1024 if df_cleaned is not None else 0
                    },
                    'consolidation': {
                        'shape': df_consolidated.shape if df_consolidated is not None else (0, 0),
                        'columns': list(df_consolidated.columns) if df_consolidated is not None else [],
                        'memory_usage_mb': df_consolidated.memory_usage(deep=True).sum() / 1024 / 1024 if df_consolidated is not None else 0
                    },
                    'enrichment': {
                        'shape': df_enriched.shape if df_enriched is not None else (0, 0),
                        'columns': list(df_enriched.columns) if df_enriched is not None else [],
                        'memory_usage_mb': df_enriched.memory_usage(deep=True).sum() / 1024 / 1024 if df_enriched is not None else 0
                    },
                    'optimization': {
                        'shape': df_optimized.shape if df_optimized is not None else (0, 0),
                        'columns': list(df_optimized.columns) if df_optimized is not None else [],
                        'memory_usage_mb': df_optimized.memory_usage(deep=True).sum() / 1024 / 1024 if df_optimized is not None else 0
                    }
                },
                'validation_results': validation_results,
                'export_results': export_results,
                'report_results': report_results,
                'dashboard_results': dashboard_results,
                'component_results': {
                    'extractor': self.data_extractor.get_extraction_stats(),
                    'cleaner': self.data_cleaner.get_cleaning_stats(),
                    'consolidator': self.data_consolidator.get_consolidation_stats(),
                    'enricher': self.data_enricher.get_enrichment_stats(),
                    'validator': self.data_validator.get_validation_stats()
                }
            }
            
            return pipeline_results
            
        except Exception as e:
            logger.error(f"âŒ Erreur compilation rÃ©sultats: {e}")
            return {'pipeline_status': 'ERROR', 'error': str(e)}
    
    def _calculate_data_reduction(self, df_input: pd.DataFrame, df_output: pd.DataFrame) -> float:
        """Calcul du pourcentage de rÃ©duction des donnÃ©es"""
        try:
            if df_input is None or df_output is None:
                return 0.0
            
            input_size = df_input.memory_usage(deep=True).sum()
            output_size = df_output.memory_usage(deep=True).sum()
            
            if input_size == 0:
                return 0.0
            
            reduction = ((input_size - output_size) / input_size) * 100
            return round(reduction, 1)
            
        except Exception:
            return 0.0
    
    def _generate_pipeline_report(self, df: pd.DataFrame, validation_results: Dict, 
                                 export_results: Dict) -> str:
        """GÃ©nÃ©ration du rapport de pipeline"""
        try:
            report = []
            report.append("# ğŸ“Š RAPPORT DE PIPELINE ETL")
            report.append("")
            report.append(f"**Date de gÃ©nÃ©ration:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report.append(f"**Statut du pipeline:** âœ… SuccÃ¨s")
            report.append("")
            
            # RÃ©sumÃ© des donnÃ©es
            report.append("## ğŸ“ˆ RÃ‰SUMÃ‰ DES DONNÃ‰ES")
            report.append("")
            report.append(f"- **Forme finale:** {df.shape[0]} lignes Ã— {df.shape[1]} colonnes")
            report.append(f"- **MÃ©moire utilisÃ©e:** {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
            report.append("")
            
            # RÃ©sultats de validation
            if 'overall_status' in validation_results:
                overall = validation_results['overall_status']
                report.append("## âœ… RÃ‰SULTATS DE VALIDATION")
                report.append("")
                report.append(f"- **Statut global:** {overall.get('status', 'N/A')}")
                report.append(f"- **Score de qualitÃ©:** {overall.get('quality_score', 0):.1%}")
                report.append(f"- **Message:** {overall.get('message', 'N/A')}")
                report.append("")
            
            # RÃ©sultats d'export
            if export_results.get('success'):
                report.append("## ğŸ“¤ RÃ‰SULTATS D'EXPORT")
                report.append("")
                report.append(f"- **Statut:** âœ… SuccÃ¨s")
                report.append(f"- **Formats exportÃ©s:** {export_results.get('formats', [])}")
                report.append(f"- **Fichiers gÃ©nÃ©rÃ©s:** {export_results.get('files_generated', 0)}")
                report.append("")
            
            # Statistiques du pipeline
            report.append("## ğŸ“Š STATISTIQUES DU PIPELINE")
            report.append("")
            report.append(f"- **DurÃ©e totale:** {self.pipeline_stats.get('total_duration', 0):.2f}s")
            report.append(f"- **Phases exÃ©cutÃ©es:** {self.pipeline_stats.get('phases_executed', 0)}")
            report.append(f"- **RÃ©duction des donnÃ©es:** {self.pipeline_stats.get('data_reduction_percentage', 0):.1f}%")
            report.append("")
            
            return "\n".join(report)
            
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration rapport pipeline: {e}")
            return f"Erreur lors de la gÃ©nÃ©ration du rapport: {str(e)}"
    
    def _handle_pipeline_error(self, error: Exception, start_time: datetime):
        """Gestion des erreurs du pipeline"""
        try:
            error_info = {
                'timestamp': datetime.now().isoformat(),
                'error_type': type(error).__name__,
                'error_message': str(error),
                'pipeline_duration': (datetime.now() - start_time).total_seconds()
            }
            
            logger.error(f"âŒ Erreur pipeline enregistrÃ©e: {error_info}")
            
            # Enregistrement de l'erreur dans l'historique
            self.pipeline_history.append({
                'timestamp': start_time.isoformat(),
                'status': 'ERROR',
                'error': error_info
            })
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la gestion d'erreur: {e}")
    
    def get_pipeline_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du pipeline"""
        return self.pipeline_stats.copy()
    
    def get_pipeline_history(self) -> List[Dict[str, Any]]:
        """Retourne l'historique des pipelines"""
        return self.pipeline_history.copy()
    
    def get_component_status(self) -> Dict[str, Any]:
        """Retourne le statut de tous les composants"""
        return {
            'data_extractor': 'âœ… InitialisÃ©',
            'data_consolidator': 'âœ… InitialisÃ©',
            'data_cleaner': 'âœ… InitialisÃ©',
            'data_enricher': 'âœ… InitialisÃ©',
            'data_validator': 'âœ… InitialisÃ©',
            'advanced_exporter': 'âœ… InitialisÃ©',
            'performance_optimizer': 'âœ… InitialisÃ©',
            'validation_dashboard': 'âœ… InitialisÃ©'
        }
