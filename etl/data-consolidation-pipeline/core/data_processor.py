#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üìä PROCESSEUR DE DONN√âES
========================

G√®re l'extraction, la validation et le traitement des donn√©es
"""

import logging
import pandas as pd
from typing import Dict, Any, Optional, Union
from pathlib import Path
import json

logger = logging.getLogger(__name__)

class DataProcessor:
    """
    Processeur de donn√©es pour le pipeline ETL
    
    Responsable de l'extraction, de la validation
    et du traitement des donn√©es
    """
    
    def __init__(self, pipeline_manager):
        """
        Initialise le processeur de donn√©es
        
        Args:
            pipeline_manager: Instance du gestionnaire de pipeline
        """
        self.pipeline_manager = pipeline_manager
        self.data = None
        self.validation_results = {}
    
    def extract_data(self, source: str, source_path: str = None, 
                     mongodb_db: str = None, mongodb_collection: str = None,
                     mongodb_query: str = None, limit: Optional[int] = None,
                     mongodb_query_file: str = None) -> Optional[pd.DataFrame]:
        """
        Extrait les donn√©es selon la source sp√©cifi√©e
        
        Args:
            source: Source des donn√©es (mongodb, csv, json, test)
            source_path: Chemin du fichier (CSV/JSON) ou cha√Æne de connexion MongoDB
            mongodb_db: Nom de la base de donn√©es MongoDB
            mongodb_collection: Nom de la collection MongoDB
            mongodb_query: Requ√™te MongoDB au format JSON
            limit: Limite du nombre de documents MongoDB √† extraire
            mongodb_query_file: Chemin vers un fichier JSON contenant la requ√™te MongoDB
            
        Returns:
            DataFrame pandas avec les donn√©es extraites
        """
        logger.info(f"üì• === PHASE 1: EXTRACTION ===")
        
        try:
            if source == "mongodb":
                return self._extract_from_mongodb(
                    mongodb_db, mongodb_collection, mongodb_query, 
                    limit, mongodb_query_file
                )
            elif source == "csv":
                return self._extract_from_csv(source_path)
            elif source == "json":
                return self._extract_from_json(source_path)
            elif source == "test":
                return self._extract_test_data()
            else:
                logger.error(f"‚ùå Source non support√©e: {source}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction: {e}")
            return None
    
    def _extract_from_mongodb(self, db: str, collection: str, 
                              query: str = None, limit: int = None,
                              query_file: str = None) -> pd.DataFrame:
        """Extrait les donn√©es depuis MongoDB"""
        logger.info("üóÑÔ∏è Extraction depuis MongoDB...")
        
        # Lecture de la requ√™te depuis le fichier si sp√©cifi√©
        if query_file:
            logger.info(f"üìÅ Lecture de la requ√™te depuis le fichier: {query_file}")
            try:
                with open(query_file, 'r', encoding='utf-8') as f:
                    query = json.load(f)
                logger.info(f"‚úÖ Requ√™te MongoDB charg√©e depuis le fichier: {query}")
            except Exception as e:
                logger.error(f"‚ùå Erreur lecture fichier requ√™te: {e}")
                return pd.DataFrame()
        
        # Utilisation de l'orchestrateur pour l'extraction
        try:
            result = self.pipeline_manager.orchestrator.data_extractor.extract_data(
                "mongodb",
                {
                    "database": db,
                    "collection": collection,
                    "query": query,
                    "limit": limit
                }
            )
            
            if isinstance(result, dict) and 'dataframe' in result:
                df = result['dataframe']
                logger.info(f"‚úÖ MongoDB: {len(df)} propri√©t√©s extraites")
                return df
            else:
                logger.error("‚ùå Format de r√©ponse MongoDB invalide")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction MongoDB: {e}")
            return pd.DataFrame()
    
    def _extract_from_csv(self, file_path: str) -> pd.DataFrame:
        """Extrait les donn√©es depuis un fichier CSV"""
        logger.info(f"üìÑ Extraction depuis CSV: {file_path}")
        try:
            df = pd.read_csv(file_path)
            logger.info(f"‚úÖ CSV: {len(df)} lignes extraites")
            return df
        except Exception as e:
            logger.error(f"‚ùå Erreur lecture CSV: {e}")
            return pd.DataFrame()
    
    def _extract_from_json(self, file_path: str) -> pd.DataFrame:
        """Extrait les donn√©es depuis un fichier JSON"""
        logger.info(f"üìÑ Extraction depuis JSON: {file_path}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            df = pd.DataFrame(data)
            logger.info(f"‚úÖ JSON: {len(df)} lignes extraites")
            return df
        except Exception as e:
            logger.error(f"‚ùå Erreur lecture JSON: {e}")
            return pd.DataFrame()
    
    def _extract_test_data(self) -> pd.DataFrame:
        """G√©n√®re des donn√©es de test"""
        logger.info("üß™ G√©n√©ration de donn√©es de test...")
        try:
            # G√©n√©ration simple de donn√©es de test
            import numpy as np
            import pandas as pd
            
            # Cr√©ation de 1000 propri√©t√©s de test
            np.random.seed(42)
            num_properties = 1000
            
            data = {
                'price': np.random.uniform(200000, 2000000, num_properties),
                'prix': np.random.uniform(200000, 2000000, num_properties),
                'surface': np.random.uniform(50, 500, num_properties),
                'superficie': np.random.uniform(50, 500, num_properties),
                'bedrooms': np.random.randint(1, 6, num_properties),
                'chambres': np.random.randint(1, 6, num_properties),
                'bathrooms': np.random.randint(1, 4, num_properties),
                'salle_bain': np.random.randint(1, 4, num_properties),
                'latitude': np.random.uniform(45.0, 46.0, num_properties),
                'longitude': np.random.uniform(-74.0, -73.0, num_properties),
                'city': np.random.choice(['Montr√©al', 'Qu√©bec', 'Laval', 'Gatineau'], num_properties),
                'type': np.random.choice(['Maison', 'Appartement', 'Condo', 'Duplex'], num_properties),
                'year_built': np.random.randint(1950, 2024, num_properties),
                'annee_construction': np.random.randint(1950, 2024, num_properties)
            }
            
            df = pd.DataFrame(data)
            logger.info(f"‚úÖ Dataset de test g√©n√©r√©: {len(df)} lignes √ó {len(df.columns)} colonnes")
            return df
                
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration donn√©es de test: {e}")
            return pd.DataFrame()
    
    def validate_data(self, df: pd.DataFrame, validation_type: str = "initial") -> Dict[str, Any]:
        """
        Valide les donn√©es selon le type sp√©cifi√©
        
        Args:
            df: DataFrame √† valider
            validation_type: Type de validation (initial, final)
            
        Returns:
            Dict avec les r√©sultats de validation
        """
        logger.info(f"‚úÖ === PHASE 2: VALIDATION {validation_type.upper()} ===")
        
        if self.pipeline_manager.quality_validator is None:
            logger.warning("‚ö†Ô∏è QualityValidator non disponible, validation ignor√©e")
            return {"overall_score": 0.0, "status": "SKIP"}
        
        try:
            validation_result = self.pipeline_manager.quality_validator.validate_dataset(
                df, validation_type
            )
            
            score = validation_result.get('overall_score', 0.0)
            status = validation_result.get('status', 'UNKNOWN')
            
            logger.info(f"‚úÖ Validation {validation_type}: {score:.2%}")
            logger.info(f"üìä Statut: {status}")
            
            self.validation_results[validation_type] = validation_result
            return validation_result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur validation {validation_type}: {e}")
            return {"overall_score": 0.0, "status": "ERROR", "error": str(e)}
    
    def detect_similarities(self, df: pd.DataFrame) -> list:
        """
        D√©tecte les similarit√©s entre colonnes
        
        Args:
            df: DataFrame √† analyser
            
        Returns:
            Liste des groupes de similarit√©s d√©tect√©s
        """
        logger.info("üß† === PHASE 3: D√âTECTION INTELLIGENTE ===")
        
        if self.pipeline_manager.similarity_detector is None:
            logger.warning("‚ö†Ô∏è SimilarityDetector non disponible, d√©tection ignor√©e")
            return []
        
        try:
            similarity_groups = self.pipeline_manager.similarity_detector.detect_similar_columns(df)
            logger.info(f"üéØ {len(similarity_groups)} groupes de similarit√©s d√©tect√©s")
            return similarity_groups
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©tection similarit√©s: {e}")
            return []
    
    def process_data(self, df: pd.DataFrame, output_dir: str) -> pd.DataFrame:
        """
        Traite les donn√©es via l'orchestrateur modulaire
        
        Args:
            df: DataFrame √† traiter
            output_dir: R√©pertoire de sortie
            
        Returns:
            DataFrame trait√©
        """
        logger.info("üîß === PHASE 4: TRANSFORMATION MODULAIRE ===")
        
        try:
            df_processed = self.pipeline_manager.orchestrator.run_modular_pipeline_only(
                input_source="dataframe",
                input_config={"dataframe": df},
                output_config={"output_dir": output_dir}
            )
            
            if isinstance(df_processed, dict) and 'final_dataframe' in df_processed:
                df_processed = df_processed['final_dataframe']
                logger.info(f"‚úÖ Transformation termin√©e: {df_processed.shape[0]} lignes √ó {df_processed.shape[1]} colonnes")
            else:
                logger.warning("‚ö†Ô∏è Format de r√©ponse invalide, utilisation des donn√©es originales")
                df_processed = df.copy()
                
            return df_processed
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur transformation modulaire: {e}, utilisation des donn√©es originales")
            return df.copy()
