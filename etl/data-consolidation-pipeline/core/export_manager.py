#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üíæ GESTIONNAIRE D'EXPORT
========================

G√®re l'export des donn√©es dans diff√©rents formats
"""

import logging
import pandas as pd
from typing import Dict, List, Any, Optional
from pathlib import Path
import time

logger = logging.getLogger(__name__)

class ExportManager:
    """
    Gestionnaire d'export pour le pipeline ETL
    
    Responsable de l'export des donn√©es dans diff√©rents formats
    et de la gestion des fichiers de sortie
    """
    
    def __init__(self, pipeline_manager):
        """
        Initialise le gestionnaire d'export
        
        Args:
            pipeline_manager: Instance du gestionnaire de pipeline
        """
        self.pipeline_manager = pipeline_manager
        self.exported_files = {}
    
    def export_data(self, df: pd.DataFrame, pipeline_name: str, 
                    formats: List[str], output_dir: str) -> Dict[str, str]:
        """
        Exporte les donn√©es dans les formats sp√©cifi√©s
        
        Args:
            df: DataFrame √† exporter
            pipeline_name: Nom du pipeline pour l'export
            formats: Liste des formats d'export
            output_dir: R√©pertoire de sortie
            
        Returns:
            Dict avec les chemins des fichiers export√©s
        """
        logger.info("üíæ === PHASE 6: EXPORT MULTI-FORMATS ===")
        
        if self.pipeline_manager.exporter is None:
            logger.warning("‚ö†Ô∏è AdvancedExporter non disponible, export basique utilis√©")
            return self._basic_export(df, pipeline_name, formats, output_dir)
        
        try:
            logger.info(f"üíæ === EXPORT MULTI-FORMATS: {pipeline_name} ===")
            
            start_time = time.time()
            exported_files = self.pipeline_manager.exporter.export_dataset(
                df, pipeline_name, formats, output_dir
            )
            export_time = time.time() - start_time
            
            logger.info(f"üéØ Export termin√© en {export_time:.2f}s")
            logger.info(f"üìä {len(exported_files)}/{len(formats)} formats export√©s avec succ√®s")
            
            self.exported_files = exported_files
            return exported_files
            
        except Exception as e:
            logger.error(f"‚ùå Erreur export avanc√©: {e}, fallback vers export basique")
            return self._basic_export(df, pipeline_name, formats, output_dir)
    
    def _basic_export(self, df: pd.DataFrame, pipeline_name: str,
                      formats: List[str], output_dir: str) -> Dict[str, str]:
        """Export basique en cas d'√©chec de l'exporteur avanc√©"""
        logger.info("üì§ Export basique activ√©...")
        
        exported_files = {}
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        for format_type in formats:
            try:
                if format_type.lower() == "csv":
                    filename = f"real_estate_data_{pipeline_name}_{timestamp}.csv"
                    filepath = output_path / filename
                    df.to_csv(filepath, index=False, encoding='utf-8')
                    exported_files["csv"] = str(filepath)
                    logger.info(f"‚úÖ CSV export√©: {filepath}")
                    
                elif format_type.lower() == "json":
                    filename = f"real_estate_data_{pipeline_name}_{timestamp}.json"
                    filepath = output_path / filename
                    df.to_json(filepath, orient='records', indent=2, force_ascii=False)
                    exported_files["json"] = str(filepath)
                    logger.info(f"‚úÖ JSON export√©: {filepath}")
                    
                elif format_type.lower() == "parquet":
                    try:
                        filename = f"real_estate_data_{pipeline_name}_{timestamp}.parquet"
                        filepath = output_path / filename
                        df.to_parquet(filepath, index=False)
                        exported_files["parquet"] = str(filepath)
                        logger.info(f"‚úÖ Parquet export√©: {filepath}")
                    except ImportError:
                        logger.warning("‚ö†Ô∏è PyArrow non disponible, export Parquet ignor√©")
                        
                else:
                    logger.warning(f"‚ö†Ô∏è Format non support√©: {format_type}")
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur export {format_type}: {e}")
        
        logger.info(f"üìä {len(exported_files)}/{len(formats)} formats export√©s avec succ√®s")
        return exported_files
    
    def get_export_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© des exports effectu√©s"""
        return {
            "total_formats": len(self.exported_files),
            "exported_files": self.exported_files,
            "export_status": "success" if self.exported_files else "failed"
        }
    
    def cleanup_exports(self, keep_files: bool = True):
        """
        Nettoie les fichiers d'export temporaires
        
        Args:
            keep_files: Si False, supprime les fichiers export√©s
        """
        if not keep_files and self.exported_files:
            logger.info("üßπ Nettoyage des fichiers d'export...")
            for format_type, filepath in self.exported_files.items():
                try:
                    Path(filepath).unlink(missing_ok=True)
                    logger.info(f"üóëÔ∏è Fichier supprim√©: {filepath}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Impossible de supprimer {filepath}: {e}")
            self.exported_files = {}
