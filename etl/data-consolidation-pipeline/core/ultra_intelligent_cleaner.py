#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üß† NETTOYEUR ULTRA-INTELLIGENT - ORCHESTRATEUR PRINCIPAL
========================================================

Module principal d'orchestration du pipeline ETL ultra-intelligent
Bas√© sur les sp√©cifications du real_estate_prompt.md
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from pathlib import Path
import warnings
import time
from datetime import datetime
import json
import os

# Imports des modules sp√©cialis√©s
from config.consolidation_config import ConsolidationConfig
from intelligence.similarity_detector import SimilarityDetector
from validation.quality_validator import QualityValidator
from export.advanced_exporter import AdvancedExporter
from performance.performance_optimizer import PerformanceOptimizer
from utils.db import read_mongodb_to_dataframe, get_mongodb_stats
from utils.property_type_normalizer import PropertyTypeNormalizer

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

class UltraIntelligentCleaner:
    """
    Nettoyeur ultra-intelligent pour le pipeline ETL immobilier
    Orchestre toutes les phases: Extraction, Transformation, Enrichment, Validation, Loading
    """
    
    def __init__(self, config: ConsolidationConfig = None):
        """
        Initialise le nettoyeur ultra-intelligent
        
        Args:
            config: Configuration de consolidation
        """
        self.config = config or ConsolidationConfig()
        self.pipeline_history = []
        self.consolidation_results = {}
        self.quality_metrics = {}
        
        # === INITIALISATION DES MODULES ===
        logger.info("üß† === INITIALISATION ULTRA-INTELLIGENT CLEANER ===")
        
        # Validation de la configuration
        if not self.config.validate_configuration():
            raise ValueError("‚ùå Configuration de consolidation invalide")
        
        # Initialisation des composants
        self.similarity_detector = SimilarityDetector()
        self.quality_validator = QualityValidator()
        self.advanced_exporter = AdvancedExporter()
        self.performance_optimizer = PerformanceOptimizer()
        self.property_normalizer = PropertyTypeNormalizer()
        
        logger.info("‚úÖ Tous les modules initialis√©s avec succ√®s")
        self.config.log_configuration()
    
    def run_complete_pipeline(self, input_source: str = "mongodb", 
                             input_config: Dict = None, 
                             output_config: Dict = None) -> Dict[str, Any]:
        """
        Ex√©cute le pipeline ETL complet
        
        Args:
            input_source: Source des donn√©es ("mongodb", "csv", "json", etc.)
            input_config: Configuration d'entr√©e
            output_config: Configuration de sortie
            
        Returns:
            Dict avec les r√©sultats du pipeline
        """
        pipeline_start = datetime.now()
        logger.info("üöÄ === D√âMARRAGE PIPELINE ETL COMPLET ===")
        
        try:
            # === PHASE 1: EXTRACTION ===
            logger.info("üì• === PHASE 1: EXTRACTION ===")
            df = self._extract_data(input_source, input_config)
            
            if df is None or df.empty:
                raise ValueError("‚ùå Aucune donn√©e extraite")
            
            logger.info(f"‚úÖ Extraction r√©ussie: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
            
            # === PHASE 2: TRANSFORMATION ===
            logger.info("üîÑ === PHASE 2: TRANSFORMATION ===")
            df_transformed = self._transform_data(df)
            
            # === PHASE 3: ENRICHMENT ===
            logger.info("üöÄ === PHASE 3: ENRICHMENT ===")
            df_enriched = self._enrich_data(df_transformed)
            
            # === PHASE 3.1: CLUSTERING SPATIAL ===
            logger.info("üåç === PHASE 3.1: CLUSTERING SPATIAL DBSCAN ===")
            spatial_results = self.similarity_detector.spatial_clustering(df_enriched)
            if spatial_results.get("success"):
                df_enriched = spatial_results["df_with_clusters"]
                logger.info(f"‚úÖ Clustering spatial r√©ussi: {spatial_results['n_clusters']} zones cr√©√©es")
            else:
                logger.warning(f"‚ö†Ô∏è Clustering spatial √©chou√©: {spatial_results.get('error', 'Erreur inconnue')}")
            
            # === PHASE 3.2: CAT√âGORISATION AUTOMATIQUE ===
            logger.info("üè∑Ô∏è === PHASE 3.2: CAT√âGORISATION AUTOMATIQUE ===")
            df_categorized = self.categorize_investment_opportunities(df_enriched)
            
            # === PHASE 4: VALIDATION ===
            logger.info("‚úÖ === PHASE 4: VALIDATION ===")
            validation_results = self._validate_data(df_categorized)
            
            # === PHASE 5: LOAD ===
            logger.info("üíæ === PHASE 5: LOAD ===")
            export_results = self._load_data(df_categorized, output_config)
            
            # === R√âSULTATS FINAUX ===
            pipeline_end = datetime.now()
            pipeline_duration = (pipeline_end - pipeline_start).total_seconds()
            
            results = {
                "success": True,
                "pipeline_duration": pipeline_duration,
                "input_shape": df.shape,
                "output_shape": df_categorized.shape,
                "reduction_percentage": ((df.shape[1] - df_categorized.shape[1]) / df.shape[1]) * 100,
                "spatial_clustering": spatial_results,
                "categorization_stats": self._extract_categorization_stats(df_categorized),
                "validation_results": validation_results,
                "export_results": export_results,
                "final_dataframe": df_categorized
            }
            
            logger.info(f"üéâ === PIPELINE ETL TERMIN√â EN {pipeline_duration:.2f}s ===")
            logger.info(f"üìä R√©duction: {df.shape[1]} ‚Üí {df_categorized.shape[1]} colonnes ({results['reduction_percentage']:.1f}%)")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur pipeline: {e}")
            
            # Enregistrement de l'erreur
            error_results = {
                "pipeline_timestamp": pipeline_start.isoformat(),
                "pipeline_duration_seconds": (datetime.now() - pipeline_start).total_seconds(),
                "input_source": input_source,
                "status": "ERROR",
                "error": str(e)
            }
            
            self.pipeline_history.append(error_results)
            
            raise
    
    def _extract_data(self, input_source: str, input_config: Dict = None) -> pd.DataFrame:
        """Phase d'extraction des donn√©es"""
        logger.info(f"üì• Extraction depuis: {input_source}")
        
        if input_source.lower() == "mongodb":
            return self._extract_from_mongodb(input_config)
        elif input_source.lower() == "csv":
            return self._extract_from_csv(input_config)
        elif input_source.lower() == "json":
            return self._extract_from_json(input_config)
        else:
            raise ValueError(f"‚ùå Source non support√©e: {input_source}")
    
    def _extract_from_mongodb(self, input_config: Dict = None) -> pd.DataFrame:
        """Extraction depuis MongoDB"""
        try:
            # Configuration par d√©faut
            default_config = {
                "database": "real_estate_db",
                "collection": "properties",
                "limit": None,
                "query": {}
            }
            
            if input_config:
                default_config.update(input_config)
            
            logger.info(f"üóÑÔ∏è Connexion MongoDB: {default_config['database']}.{default_config['collection']}")
            
            # Test de connexion
            stats = get_mongodb_stats(default_config["database"], default_config["collection"])
            logger.info(f"üìä Statistiques MongoDB: {stats}")
            
            # Extraction des donn√©es
            df = read_mongodb_to_dataframe(
                database=default_config["database"],
                collection=default_config["collection"],
                query=default_config["query"],
                limit=default_config["limit"]
            )
            
            if df is None or df.empty:
                logger.warning("‚ö†Ô∏è Aucune donn√©e extraite de MongoDB - cr√©ation d'un dataset de test")
                df = self._create_test_dataset()
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction MongoDB: {e}")
            logger.info("üîÑ Cr√©ation d'un dataset de test...")
            return self._create_test_dataset()
    
    def _extract_from_csv(self, input_config: Dict = None) -> pd.DataFrame:
        """Extraction depuis un fichier CSV"""
        try:
            file_path = input_config.get("file_path", "data/real_estate_data.csv")
            logger.info(f"üìÅ Lecture CSV: {file_path}")
            
            df = pd.read_csv(file_path)
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lecture CSV: {e}")
            raise
    
    def _extract_from_json(self, input_config: Dict = None) -> pd.DataFrame:
        """Extraction depuis un fichier JSON"""
        try:
            file_path = input_config.get("file_path", "data/real_estate_data.json")
            logger.info(f"üìÅ Lecture JSON: {file_path}")
            
            df = pd.read_json(file_path)
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lecture JSON: {e}")
            raise
    
    def _create_test_dataset(self) -> pd.DataFrame:
        """Cr√©e un dataset de test synth√©tique"""
        logger.info("üß™ Cr√©ation d'un dataset de test synth√©tique")
        
        np.random.seed(42)
        n_samples = 1000
        
        # G√©n√©ration de donn√©es synth√©tiques
        data = {
            "price": np.random.uniform(100000, 2000000, n_samples),
            "prix": np.random.uniform(100000, 2000000, n_samples),
            "surface": np.random.uniform(50, 500, n_samples),
            "superficie": np.random.uniform(50, 500, n_samples),
            "bedrooms": np.random.randint(1, 6, n_samples),
            "chambres": np.random.randint(1, 6, n_samples),
            "bathrooms": np.random.randint(1, 4, n_samples),
            "salle_bain": np.random.randint(1, 4, n_samples),
            "latitude": np.random.uniform(45.0, 47.5, n_samples),
            "longitude": np.random.uniform(-74.5, -71.0, n_samples),
            "property_type": np.random.choice(["Maison", "Appartement", "Condo"], n_samples),
            "type_propriete": np.random.choice(["House", "Apartment", "Condo"], n_samples),
            "year_built": np.random.randint(1950, 2024, n_samples),
            "annee_construction": np.random.randint(1950, 2024, n_samples),
            "tax_municipal": np.random.uniform(1000, 10000, n_samples),
            "taxe_municipale": np.random.uniform(1000, 10000, n_samples)
        }
        
        df = pd.DataFrame(data)
        
        # Ajout de valeurs manquantes pour tester la consolidation
        for col in df.columns:
            if np.random.random() < 0.1:  # 10% de valeurs manquantes
                mask = np.random.choice([True, False], size=len(df), p=[0.1, 0.9])
                df.loc[mask, col] = np.nan
        
        logger.info(f"‚úÖ Dataset de test cr√©√©: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
        return df

    def _transform_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Phase de transformation des donn√©es"""
        logger.info("üîÑ === TRANSFORMATION DES DONN√âES ===")
        
        # === OPTIMISATION DES PERFORMANCES ===
        logger.info("üöÄ Optimisation des performances...")
        df = self.performance_optimizer.optimize_dataframe(df, "medium")
        
        # === D√âTECTION INTELLIGENTE DES SIMILARIT√âS ===
        logger.info("üß† D√©tection intelligente des similarit√©s...")
        similarity_groups = self.similarity_detector.detect_similar_columns(df)
        
        # === CONSOLIDATION MAXIMALE ===
        logger.info("üîó Consolidation maximale des variables...")
        df_consolidated = self._consolidate_variables(df)
        
        # === NETTOYAGE DES DONN√âES ===
        logger.info("üßπ Nettoyage des donn√©es...")
        df_cleaned = self._clean_data(df_consolidated)
        
        # === NORMALISATION DES TYPES DE PROPRI√âT√â ===
        logger.info("üè† Normalisation des types de propri√©t√©...")
        df_normalized = self._normalize_property_types(df_cleaned)
        
        logger.info(f"‚úÖ Transformation termin√©e: {df.shape[1]} ‚Üí {df_normalized.shape[1]} colonnes")
        return df_normalized
    
    def _consolidate_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Consolidation maximale des variables selon la strat√©gie avanc√©e
        Respecte exactement les 30 groupes de consolidation du real_estate_prompt.md
        """
        logger.info("üîó === CONSOLIDATION MAXIMALE DES VARIABLES ===")
        logger.info(f"üìä Colonnes initiales: {df.shape[1]}")
        
        df_consolidated = df.copy()
        consolidation_results = {}
        
        # === TRI DES GROUPES PAR PRIORIT√â ===
        priority_groups = sorted(
            self.config.CONSOLIDATION_GROUPS,
            key=lambda x: x.priority
        )
        
        logger.info(f"üéØ {len(priority_groups)} groupes de consolidation √† traiter")
        
        for group in priority_groups:
            logger.info(f"üîÑ Consolidation du groupe: {group.name} ‚Üí {group.final_column}")
            
            # === V√âRIFICATION DES COLONNES SOURCES DISPONIBLES ===
            available_columns = [col for col in group.source_columns if col in df_consolidated.columns]
            
            if not available_columns:
                logger.warning(f"‚ö†Ô∏è Aucune colonne source disponible pour {group.name}")
                continue
            
            logger.info(f"üìã Colonnes sources trouv√©es: {available_columns}")
            
            # === CONSOLIDATION INTELLIGENTE ===
            try:
                consolidated_column = self._consolidate_group(
                    df_consolidated, 
                    available_columns, 
                    group
                )
                
                if consolidated_column is not None:
                    # Ajout de la colonne consolid√©e
                    df_consolidated[group.final_column] = consolidated_column
                    
                    # Suppression des colonnes sources (apr√®s validation)
                    if self._validate_consolidated_column(consolidated_column, group):
                        for col in available_columns:
                            if col in df_consolidated.columns:
                                df_consolidated = df_consolidated.drop(columns=[col])
                        
                        # Statistiques de consolidation
                        non_null_count = consolidated_column.notna().sum()
                        total_count = len(consolidated_column)
                        completeness = (non_null_count / total_count) * 100
                        
                        consolidation_results[group.name] = {
                            "final_column": group.final_column,
                            "source_columns": available_columns,
                            "completeness": completeness,
                            "non_null_values": non_null_count,
                            "total_values": total_count,
                            "status": "success"
                        }
                        
                        logger.info(f"‚úÖ {group.name} consolid√©: {completeness:.1f}% de compl√©tude")
                    else:
                        logger.warning(f"‚ö†Ô∏è Validation √©chou√©e pour {group.name}")
                        consolidation_results[group.name] = {
                            "status": "validation_failed",
                            "error": "Validation de la colonne consolid√©e √©chou√©e"
                        }
                else:
                    logger.warning(f"‚ö†Ô∏è Consolidation √©chou√©e pour {group.name}")
                    consolidation_results[group.name] = {
                        "status": "consolidation_failed",
                        "error": "Impossible de cr√©er la colonne consolid√©e"
                    }
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur lors de la consolidation de {group.name}: {e}")
                consolidation_results[group.name] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # === R√âSULTATS FINAUX ===
        final_columns = df_consolidated.shape[1]
        reduction_percentage = ((df.shape[1] - final_columns) / df.shape[1]) * 100
        
        # === SUPPRESSION DES COLONNES M√âTADONN√âES ===
        logger.info("üóëÔ∏è Suppression des colonnes m√©tadonn√©es et utilitaires...")
        columns_to_remove = [col for col in self.config.COLUMNS_TO_REMOVE if col in df_consolidated.columns]
        
        if columns_to_remove:
            df_consolidated = df_consolidated.drop(columns=columns_to_remove)
            logger.info(f"üóëÔ∏è {len(columns_to_remove)} colonnes m√©tadonn√©es supprim√©es")
        
        # === FILTRAGE FINAL SELON SP√âCIFICATIONS ===
        logger.info("üìã Filtrage final selon real_estate_prompt.md...")
        try:
            from config.final_columns_config import FINAL_COLUMNS_LIST
            
            # Garder seulement les colonnes qui existent et sont dans la liste finale
            available_final_columns = [col for col in FINAL_COLUMNS_LIST if col in df_consolidated.columns]
            
            # Ajouter les colonnes essentielles qui pourraient avoir des noms diff√©rents
            essential_columns = ['_id', 'price', 'city', 'type', 'latitude', 'longitude']
            for col in essential_columns:
                if col in df_consolidated.columns and col not in available_final_columns:
                    available_final_columns.append(col)
            
            # Filtrer le DataFrame pour ne garder que ces colonnes
            df_consolidated = df_consolidated[available_final_columns]
            
            logger.info(f"üìã {len(available_final_columns)} colonnes finales conserv√©es selon le prompt")
            
        except ImportError:
            logger.warning("‚ö†Ô∏è Configuration final_columns_config non trouv√©e, conservation de toutes les colonnes")
        
        # === CALCUL FINAL DES M√âTRIQUES ===
        final_columns_after_cleanup = df_consolidated.shape[1]
        total_reduction_percentage = ((df.shape[1] - final_columns_after_cleanup) / df.shape[1]) * 100
        
        logger.info(f"üéâ === CONSOLIDATION TERMIN√âE ===")
        logger.info(f"üìä Colonnes initiales: {df.shape[1]}")
        logger.info(f"üìä Colonnes apr√®s consolidation: {final_columns}")
        logger.info(f"üìä Colonnes finales: {final_columns_after_cleanup}")
        logger.info(f"üìâ R√©duction totale: {total_reduction_percentage:.1f}%")
        
        # V√©rification de l'objectif de r√©duction
        if total_reduction_percentage >= 65:
            logger.info(f"üéØ Objectif de r√©duction atteint: {total_reduction_percentage:.1f}% >= 65%")
        else:
            logger.warning(f"‚ö†Ô∏è Objectif de r√©duction non atteint: {total_reduction_percentage:.1f}% < 65%")
        
        # Sauvegarde des r√©sultats
        self.consolidation_results = consolidation_results
        
        return df_consolidated
    
    def _consolidate_group(self, df: pd.DataFrame, source_columns: List[str], group: 'ConsolidationGroup') -> Optional[pd.Series]:
        """
        Consolide un groupe de colonnes sources en une colonne finale
        
        Args:
            df: DataFrame source
            source_columns: Liste des colonnes sources √† consolider
            group: Configuration du groupe de consolidation
            
        Returns:
            S√©rie consolid√©e ou None si √©chec
        """
        try:
            logger.info(f"üîÑ Consolidation du groupe {group.name} avec {len(source_columns)} colonnes sources")
            
            # === S√âLECTION DES COLONNES SOURCES ===
            available_data = df[source_columns].copy()
            
            # === STRAT√âGIE DE CONSOLIDATION PAR TYPE DE DONN√âES ===
            if group.data_type == "numeric":
                return self._consolidate_numeric_group(available_data, group)
            elif group.data_type == "categorical":
                return self._consolidate_categorical_group(available_data, group)
            elif group.data_type == "datetime":
                return self._consolidate_datetime_group(available_data, group)
            elif group.data_type == "mixed":
                return self._consolidate_mixed_group(available_data, group)
            else:
                logger.warning(f"‚ö†Ô∏è Type de donn√©es non support√©: {group.data_type}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erreur consolidation groupe {group.name}: {e}")
            return None
    
    def _consolidate_numeric_group(self, data: pd.DataFrame, group: 'ConsolidationGroup') -> pd.Series:
        """Consolide un groupe de colonnes num√©riques"""
        # Conversion en num√©rique et gestion des erreurs
        numeric_data = data.apply(pd.to_numeric, errors='coerce')
        
        # Fusion intelligente avec fillna en cascade
        consolidated = numeric_data.iloc[:, 0].copy()
        
        for col in numeric_data.columns[1:]:
            consolidated = consolidated.fillna(numeric_data[col])
        
        # Validation des r√®gles m√©tier
        if "positive" in group.validation_rules:
            consolidated = consolidated.where(consolidated > 0, np.nan)
        
        if "non_negative" in group.validation_rules:
            consolidated = consolidated.where(consolidated >= 0, np.nan)
        
        if "integer" in group.validation_rules:
            consolidated = consolidated.round(0)
        
        return consolidated
    
    def _consolidate_categorical_group(self, data: pd.DataFrame, group: 'ConsolidationGroup') -> pd.Series:
        """Consolide un groupe de colonnes cat√©gorielles"""
        # Fusion avec priorit√© √† la premi√®re colonne non-vide
        consolidated = data.iloc[:, 0].copy()
        
        for col in data.columns[1:]:
            mask = consolidated.isna() | (consolidated == '')
            consolidated[mask] = data[col][mask]
        
        return consolidated
    
    def _consolidate_datetime_group(self, data: pd.DataFrame, group: 'ConsolidationGroup') -> pd.Series:
        """Consolide un groupe de colonnes de dates"""
        # Conversion en datetime et fusion
        datetime_data = data.apply(pd.to_datetime, errors='coerce')
        
        consolidated = datetime_data.iloc[:, 0].copy()
        
        for col in datetime_data.columns[1:]:
            consolidated = consolidated.fillna(datetime_data[col])
        
        return consolidated
    
    def _consolidate_mixed_group(self, data: pd.DataFrame, group: 'ConsolidationGroup') -> pd.Series:
        """Consolide un groupe de colonnes de types mixtes"""
        # Strat√©gie de fusion intelligente pour types mixtes
        consolidated = data.iloc[:, 0].copy()
        
        for col in data.columns[1:]:
            mask = consolidated.isna() | (consolidated == '') | (consolidated == 'nan')
            consolidated[mask] = data[col][mask]
        
        return consolidated
    
    def _validate_consolidated_column(self, consolidated_column: pd.Series, group: 'ConsolidationGroup') -> bool:
        """
        Valide la colonne consolid√©e pour s'assurer qu'elle est coh√©rente
        et qu'elle ne contient pas trop de valeurs manquantes.
        """
        if consolidated_column.empty:
            logger.warning(f"‚ö†Ô∏è Colonne consolid√©e vide pour {group.name}")
            return False

        # V√©rifier la compl√©tude
        completeness = consolidated_column.notna().sum() / len(consolidated_column)
        if completeness < 0.9: # Exemple: 90% de compl√©tude requis
            logger.warning(f"‚ö†Ô∏è Compl√©tude insuffisante pour {group.name}: {completeness * 100:.1f}%")
            return False

        # V√©rifier la diversit√©
        unique_ratio = consolidated_column.nunique() / len(consolidated_column)
        if unique_ratio < 0.5: # Exemple: 50% de diversit√© requis
            logger.warning(f"‚ö†Ô∏è Diversit√© insuffisante pour {group.name}: {unique_ratio * 100:.1f}%")
            return False

        # V√©rifier les valeurs aberrantes
        if consolidated_column.dtype in ['int64', 'float64']:
            q1 = consolidated_column.quantile(0.25)
            q3 = consolidated_column.quantile(0.75)
            iqr = q3 - q1
            if iqr > 0:
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                outliers = ((consolidated_column < lower_bound) | (consolidated_column > upper_bound)).sum()
                if outliers / len(consolidated_column) > 0.1: # Exemple: 10% d'outliers
                    logger.warning(f"‚ö†Ô∏è Taux d'outliers √©lev√© pour {group.name}: {outliers / len(consolidated_column) * 100:.1f}%")
                    return False

        return True

    def _calculate_column_quality(self, series: pd.Series) -> float:
        """Calcule un score de qualit√© pour une colonne"""
        if series.empty:
            return 0.0
        
        # === COMPL√âTUDE ===
        completeness = 1 - (series.isnull().sum() / len(series))
        
        # === COH√âRENCE ===
        if series.dtype in ['int64', 'float64']:
            # Pour les colonnes num√©riques, coh√©rence bas√©e sur les outliers
            q1 = series.quantile(0.25)
            q3 = series.quantile(0.75)
            iqr = q3 - q1
            
            if iqr > 0:
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                outliers = ((series < lower_bound) | (series > upper_bound)).sum()
                coherence = 1 - (outliers / len(series))
            else:
                coherence = 1.0
        else:
            # Pour les colonnes cat√©gorielles, coh√©rence bas√©e sur la diversit√©
            unique_ratio = series.nunique() / len(series)
            coherence = 1 - unique_ratio if unique_ratio < 0.9 else 0.1
        
        # === SCORE COMPOSITE ===
        quality_score = (completeness * 0.7) + (coherence * 0.3)
        
        return quality_score
    
    def _apply_validation_rules(self, series: pd.Series, group: 'ConsolidationGroup') -> pd.Series:
        """Applique les r√®gles de validation √† une s√©rie"""
        validated_series = series.copy()
        
        for rule_name in group.validation_rules:
            if rule_name in self.config.VALIDATION_RULES:
                rule_func = self.config.VALIDATION_RULES[rule_name]
                
                try:
                    # Application de la r√®gle
                    if rule_name == "positive":
                        validated_series = validated_series[validated_series > 0]
                    elif rule_name == "non_negative":
                        validated_series = validated_series[validated_series >= 0]
                    elif rule_name == "reasonable_range":
                        validated_series = validated_series[
                            (validated_series > 0) & (validated_series < 1000000)
                        ]
                    elif rule_name == "geographic_bounds":
                        if "latitude" in group.name.lower():
                            validated_series = validated_series[
                                (validated_series >= 45.0) & (validated_series <= 47.5)
                            ]
                        elif "longitude" in group.name.lower():
                            validated_series = validated_series[
                                (validated_series >= -74.5) & (validated_series <= -71.0)
                            ]
                    elif rule_name == "year_range":
                        validated_series = validated_series[
                            (validated_series >= 1800) & (validated_series <= 2024)
                        ]
                    elif rule_name == "integer":
                        validated_series = validated_series[validated_series == validated_series.astype(int)]
                
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è R√®gle {rule_name} non applicable: {e}")
                    continue
        
        return validated_series
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Nettoie les donn√©es consolid√©es"""
        logger.info("üßπ === NETTOYAGE DES DONN√âES ===")
        
        df_cleaned = df.copy()
        
        # === SUPPRESSION DES LIGNES VIDE ===
        initial_rows = len(df_cleaned)
        df_cleaned = df_cleaned.dropna(how='all')
        rows_removed = initial_rows - len(df_cleaned)
        
        if rows_removed > 0:
            logger.info(f"üóëÔ∏è {rows_removed} lignes vides supprim√©es")
        
        # === NETTOYAGE DES VALEURS TEXTUELLES ===
        text_columns = df_cleaned.select_dtypes(include=['object']).columns
        for col in text_columns:
            if col in df_cleaned.columns:
                # Suppression des espaces en d√©but/fin
                df_cleaned[col] = df_cleaned[col].astype(str).str.strip()
                
                # Remplacement des cha√Ænes vides par NaN
                df_cleaned[col] = df_cleaned[col].replace(['', 'nan', 'None'], np.nan)
        
        # === D√âTECTION ET TRAITEMENT DES OUTLIERS ===
        numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if col in df_cleaned.columns:
                df_cleaned[col] = self._handle_outliers(df_cleaned[col])
        
        logger.info(f"‚úÖ Nettoyage termin√©: {len(df_cleaned)} lignes conserv√©es")
        return df_cleaned
    
    def _handle_outliers(self, series: pd.Series) -> pd.Series:
        """G√®re les outliers d'une s√©rie num√©rique"""
        if series.empty or series.isnull().all():
            return series
        
        # M√©thode IQR pour d√©tecter les outliers
        q1 = series.quantile(0.25)
        q3 = series.quantile(0.75)
        iqr = q3 - q1
        
        if iqr > 0:
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            # Remplacement des outliers par les bornes
            series_cleaned = series.copy()
            series_cleaned[series < lower_bound] = lower_bound
            series_cleaned[series > upper_bound] = upper_bound
            
            outliers_count = ((series < lower_bound) | (series > upper_bound)).sum()
            if outliers_count > 0:
                logger.debug(f"üîç {outliers_count} outliers trait√©s dans {series.name}")
            
            return series_cleaned
        
        return series
    
    def _normalize_property_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normalise les types de propri√©t√©"""
        logger.info("üè† === NORMALISATION DES TYPES DE PROPRI√âT√â ===")
        
        df_normalized = df.copy()
        
        # Recherche des colonnes de type de propri√©t√©
        property_type_columns = []
        for col in df_normalized.columns:
            if any(term in col.lower() for term in ['type', 'category', 'property_type']):
                property_type_columns.append(col)
        
        if not property_type_columns:
            logger.info("‚ÑπÔ∏è Aucune colonne de type de propri√©t√© trouv√©e")
            return df_normalized
        
        # Normalisation de chaque colonne
        for col in property_type_columns:
            if col in df_normalized.columns:
                logger.info(f"üîÑ Normalisation de la colonne: {col}")
                
                try:
                    normalized_values = self.property_normalizer.normalize_property_types(
                        df_normalized[col]
                    )
                    
                    if normalized_values is not None:
                        df_normalized[col] = normalized_values
                        logger.info(f"‚úÖ Colonne {col} normalis√©e")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur normalisation {col}: {e}")
        
        return df_normalized
    
    def _enrich_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Phase d'enrichissement des donn√©es"""
        logger.info("‚ú® === ENRICHISSEMENT DES DONN√âES ===")
        
        df_enriched = df.copy()
        
        # === CALCUL DES M√âTRIQUES D√âRIV√âES ===
        logger.info("üìä Calcul des m√©triques d√©riv√©es...")
        df_enriched = self._calculate_derived_metrics(df_enriched)
        
        # === G√âOLOCALISATION ET VALIDATION ===
        logger.info("üåç Validation et enrichissement g√©ographique...")
        df_enriched = self._enrich_geographic_data(df_enriched)
        
        # === CALCULS FINANCIERS ===
        logger.info("üí∞ Calculs financiers...")
        df_enriched = self._calculate_financial_metrics(df_enriched)
        
        # === OPTIMISATION FINALE ===
        logger.info("üöÄ Optimisation finale des performances...")
        df_enriched = self.performance_optimizer.optimize_dataframe(df_enriched, "aggressive")
        
        logger.info(f"‚úÖ Enrichissement termin√©: {df.shape[1]} ‚Üí {df_enriched.shape[1]} colonnes")
        return df_enriched
    
    def _calculate_derived_metrics(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calcule les m√©triques d√©riv√©es"""
        df_enriched = df.copy()
        
        # === PRIX AU M¬≤ ===
        price_columns = [col for col in df.columns if any(term in col.lower() for term in ['price', 'prix'])]
        surface_columns = [col for col in df.columns if any(term in col.lower() for term in ['surface', 'superficie', 'area'])]
        
        if price_columns and surface_columns:
            price_col = price_columns[0]
            surface_col = surface_columns[0]
            
            if price_col in df_enriched.columns and surface_col in df_enriched.columns:
                try:
                    df_enriched['price_per_sqm'] = (
                        df_enriched[price_col] / df_enriched[surface_col]
                    ).round(2)
                    logger.info("‚úÖ Prix au m¬≤ calcul√©")
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è Erreur calcul prix au m¬≤: {e}")
        
        # === √ÇGE DE LA PROPRI√âT√â ===
        year_columns = [col for col in df.columns if any(term in col.lower() for term in ['year', 'annee', 'construction'])]
        
        if year_columns:
            year_col = year_columns[0]
            if year_col in df_enriched.columns:
                try:
                    current_year = datetime.now().year
                    df_enriched['property_age'] = current_year - df_enriched[year_col]
                    df_enriched['property_age'] = df_enriched['property_age'].clip(lower=0)
                    logger.info("‚úÖ √Çge de la propri√©t√© calcul√©")
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è Erreur calcul √¢ge: {e}")
        
        # === RATIO TAXES/PRIX ===
        tax_columns = [col for col in df.columns if any(term in col.lower() for term in ['tax', 'taxe'])]
        
        if tax_columns and price_columns:
            tax_col = tax_columns[0]
            price_col = price_columns[0]
            
            if tax_col in df_enriched.columns and price_col in df_enriched.columns:
                try:
                    df_enriched['tax_price_ratio'] = (
                        df_enriched[tax_col] / df_enriched[price_col]
                    ).round(4)
                    logger.info("‚úÖ Ratio taxes/prix calcul√©")
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è Erreur calcul ratio taxes: {e}")
        
        return df_enriched
    
    def _enrich_geographic_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Enrichit les donn√©es g√©ographiques"""
        df_enriched = df.copy()
        
        # === VALIDATION DES COORDONN√âES ===
        lat_columns = [col for col in df.columns if any(term in col.lower() for term in ['lat', 'latitude'])]
        lng_columns = [col for col in df.columns if any(term in col.lower() for term in ['lng', 'long', 'longitude'])]
        
        if lat_columns and lng_columns:
            lat_col = lat_columns[0]
            lng_col = lng_columns[0]
            
            if lat_col in df_enriched.columns and lng_col in df_enriched.columns:
                # Validation des coordonn√©es
                valid_coords = (
                    (df_enriched[lat_col] >= 45.0) & (df_enriched[lat_col] <= 47.5) &
                    (df_enriched[lng_col] >= -74.5) & (df_enriched[lng_col] <= -71.0)
                )
                
                invalid_count = (~valid_coords).sum()
                if invalid_count > 0:
                    logger.warning(f"‚ö†Ô∏è {invalid_count} coordonn√©es invalides d√©tect√©es")
                
                # Filtrage des coordonn√©es valides
                df_enriched = df_enriched[valid_coords]
                
                # Calcul de la densit√© g√©ographique
                if len(df_enriched) > 0:
                    try:
                        # Approximation de la densit√© (nombre de propri√©t√©s par zone)
                        df_enriched['geographic_density'] = len(df_enriched) / (
                            (47.5 - 45.0) * (-71.0 - (-74.5))
                        )
                        logger.info("‚úÖ Densit√© g√©ographique calcul√©e")
                    except Exception as e:
                        logger.debug(f"‚ö†Ô∏è Erreur calcul densit√©: {e}")
        
        return df_enriched
    
    def _calculate_financial_metrics(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calcule les m√©triques financi√®res"""
        df_enriched = df.copy()
        
        # === ROI BRUT ===
        price_columns = [col for col in df.columns if any(term in col.lower() for term in ['price', 'prix'])]
        revenue_columns = [col for col in df.columns if any(term in col.lower() for term in ['revenue', 'revenu', 'income'])]
        
        if price_columns and revenue_columns:
            price_col = price_columns[0]
            revenue_col = revenue_columns[0]
            
            if price_col in df_enriched.columns and revenue_col in df_enriched.columns:
                try:
                    # ROI brut = (revenus annuels / prix d'achat) * 100
                    df_enriched['roi_brut'] = (
                        (df_enriched[revenue_col] / df_enriched[price_col]) * 100
                    ).round(2)
                    
                    # Filtrage des valeurs aberrantes
                    df_enriched['roi_brut'] = df_enriched['roi_brut'].clip(0, 50)
                    
                    logger.info("‚úÖ ROI brut calcul√©")
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è Erreur calcul ROI: {e}")
        
        # === ROI NET ===
        expense_columns = [col for col in df.columns if any(term in col.lower() for term in ['expense', 'depense', 'charge'])]
        
        if price_columns and revenue_columns and expense_columns:
            price_col = price_columns[0]
            revenue_col = revenue_columns[0]
            expense_col = expense_columns[0]
            
            if all(col in df_enriched.columns for col in [price_col, revenue_col, expense_col]):
                try:
                    # ROI net = ((revenus - d√©penses) / prix d'achat) * 100
                    df_enriched['roi_net'] = (
                        ((df_enriched[revenue_col] - df_enriched[expense_col]) / df_enriched[price_col]) * 100
                    ).round(2)
                    
                    # Filtrage des valeurs aberrantes
                    df_enriched['roi_net'] = df_enriched['roi_net'].clip(-10, 40)
                    
                    logger.info("‚úÖ ROI net calcul√©")
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è Erreur calcul ROI net: {e}")
        
        return df_enriched

    def _validate_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Phase de validation des donn√©es"""
        logger.info("‚úÖ === VALIDATION DES DONN√âES ===")
        
        # === VALIDATION DE QUALIT√â COMPL√àTE ===
        logger.info("üîç Validation de qualit√© compl√®te...")
        validation_results = self.quality_validator.validate_dataset(
            df, 
            dataset_name="real_estate_consolidated"
        )
        
        # === VALIDATION SP√âCIFIQUE AU DOMAINE ===
        logger.info("üè† Validation sp√©cifique au domaine immobilier...")
        domain_validation = self._validate_domain_specific_rules(df)
        
        # === VALIDATION DES M√âTRIQUES DE CONSOLIDATION ===
        logger.info("üìä Validation des m√©triques de consolidation...")
        consolidation_validation = self._validate_consolidation_metrics(df)
        
        # === COMPILATION DES R√âSULTATS ===
        all_validation_results = {
            "quality_validation": validation_results,
            "domain_validation": domain_validation,
            "consolidation_validation": consolidation_validation,
            "overall_status": self._determine_overall_validation_status(
                validation_results, domain_validation, consolidation_validation
            )
        }
        
        # Enregistrement des m√©triques
        self.quality_metrics = all_validation_results
        
        logger.info(f"‚úÖ Validation termin√©e - Statut: {all_validation_results['overall_status']}")
        return all_validation_results
    
    def _validate_domain_specific_rules(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Valide les r√®gles sp√©cifiques au domaine immobilier"""
        validation_results = {
            "status": "PASS",
            "rules_checked": [],
            "violations": [],
            "warnings": []
        }
        
        # === VALIDATION DES PRIX ===
        price_columns = [col for col in df.columns if any(term in col.lower() for term in ['price', 'prix'])]
        for col in price_columns:
            if col in df.columns:
                series = df[col].dropna()
                if len(series) > 0:
                    # V√©rification des prix n√©gatifs
                    negative_prices = (series < 0).sum()
                    if negative_prices > 0:
                        validation_results["violations"].append({
                            "rule": "Prix n√©gatifs interdits",
                            "column": col,
                            "count": negative_prices,
                            "severity": "ERROR"
                        })
                    
                    # V√©rification des prix extr√™mes
                    q99 = series.quantile(0.99)
                    extreme_prices = (series > q99 * 10).sum()
                    if extreme_prices > 0:
                        validation_results["warnings"].append({
                            "rule": "Prix extr√™mes d√©tect√©s",
                            "column": col,
                            "count": extreme_prices,
                            "severity": "WARNING"
                        })
                    
                    validation_results["rules_checked"].append(f"Prix - {col}")
        
        # === VALIDATION DES SURFACES ===
        surface_columns = [col for col in df.columns if any(term in col.lower() for term in ['surface', 'superficie', 'area'])]
        for col in surface_columns:
            if col in df.columns:
                series = df[col].dropna()
                if len(series) > 0:
                    # V√©rification des surfaces n√©gatives
                    negative_surfaces = (series < 0).sum()
                    if negative_surfaces > 0:
                        validation_results["violations"].append({
                            "rule": "Surfaces n√©gatives interdites",
                            "column": col,
                            "count": negative_surfaces,
                            "severity": "ERROR"
                        })
                    
                    # V√©rification des surfaces extr√™mes
                    q99 = series.quantile(0.99)
                    extreme_surfaces = (series > q99 * 5).sum()
                    if extreme_surfaces > 0:
                        validation_results["warnings"].append({
                            "rule": "Surfaces extr√™mes d√©tect√©es",
                            "column": col,
                            "count": extreme_surfaces,
                            "severity": "WARNING"
                        })
                    
                    validation_results["rules_checked"].append(f"Surface - {col}")
        
        # === VALIDATION DES COORDONN√âES ===
        lat_columns = [col for col in df.columns if any(term in col.lower() for term in ['lat', 'latitude'])]
        lng_columns = [col for col in df.columns if any(term in col.lower() for term in ['lng', 'long', 'longitude'])]
        
        if lat_columns and lng_columns:
            lat_col = lat_columns[0]
            lng_col = lng_columns[0]
            
            if lat_col in df.columns and lng_col in df.columns:
                # V√©rification des coordonn√©es dans les limites du Qu√©bec
                valid_coords = (
                    (df[lat_col] >= 45.0) & (df[lat_col] <= 47.5) &
                    (df[lng_col] >= -74.5) & (df[lng_col] <= -71.0)
                )
                
                invalid_coords = (~valid_coords).sum()
                if invalid_coords > 0:
                    validation_results["violations"].append({
                        "rule": "Coordonn√©es hors limites Qu√©bec",
                        "columns": [lat_col, lng_col],
                        "count": invalid_coords,
                        "severity": "ERROR"
                    })
                
                validation_results["rules_checked"].append("Coordonn√©es g√©ographiques")
        
        # === D√âTERMINATION DU STATUT ===
        if any(v["severity"] == "ERROR" for v in validation_results["violations"]):
            validation_results["status"] = "FAIL"
        elif any(v["severity"] == "WARNING" for v in validation_results["warnings"]):
            validation_results["status"] = "WARNING"
        else:
            validation_results["status"] = "PASS"
        
        return validation_results
    
    def _validate_consolidation_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Valide les m√©triques de consolidation"""
        validation_results = {
            "status": "PASS",
            "metrics": {},
            "targets_met": []
        }
        
        # === V√âRIFICATION DES OBJECTIFS ===
        initial_columns = self.consolidation_results.get("initial_columns", 0)
        final_columns = len(df.columns)
        
        if initial_columns > 0:
            reduction_percentage = ((initial_columns - final_columns) / initial_columns) * 100
            
            # Objectif de r√©duction
            target_reduction = self.config.TARGET_REDUCTION_PERCENTAGE
            if reduction_percentage >= target_reduction:
                validation_results["targets_met"].append(f"R√©duction colonnes: {reduction_percentage:.1f}% ‚â• {target_reduction}%")
            else:
                validation_results["warnings"] = [f"R√©duction insuffisante: {reduction_percentage:.1f}% < {target_reduction}%"]
            
            validation_results["metrics"]["reduction_percentage"] = reduction_percentage
            validation_results["metrics"]["target_reduction"] = target_reduction
        
        # === V√âRIFICATION DE LA QUALIT√â ===
        quality_score = self.quality_metrics.get("quality_validation", {}).get("overall_score", 0)
        if quality_score >= 0.8:
            validation_results["targets_met"].append(f"Qualit√© des donn√©es: {quality_score:.1%} ‚â• 80%")
        else:
            validation_results["warnings"] = [f"Qualit√© insuffisante: {quality_score:.1%} < 80%"]
        
        validation_results["metrics"]["quality_score"] = quality_score
        
        # === D√âTERMINATION DU STATUT ===
        if validation_results["warnings"]:
            validation_results["status"] = "WARNING"
        elif validation_results["targets_met"]:
            validation_results["status"] = "PASS"
        
        return validation_results
    
    def _determine_overall_validation_status(self, quality_validation: Dict, 
                                           domain_validation: Dict, 
                                           consolidation_validation: Dict) -> str:
        """D√©termine le statut global de validation"""
        statuses = [
            quality_validation.get("status", "UNKNOWN"),
            domain_validation.get("status", "UNKNOWN"),
            consolidation_validation.get("status", "UNKNOWN")
        ]
        
        if "FAIL" in statuses:
            return "FAIL"
        elif "WARNING" in statuses:
            return "WARNING"
        elif all(status == "PASS" for status in statuses):
            return "PASS"
        else:
            return "UNKNOWN"
    
    def _load_data(self, df: pd.DataFrame, output_config: Dict = None) -> Dict[str, Any]:
        """Phase de chargement et export des donn√©es"""
        logger.info("üíæ === CHARGEMENT ET EXPORT DES DONN√âES ===")
        
        # === CONFIGURATION D'EXPORT ===
        default_output_config = {
            "formats": ["parquet", "csv", "geojson"],
            "output_directory": "exports",
            "include_metadata": True,
            "chunked_export": False
        }
        
        if output_config:
            default_output_config.update(output_config)
        
        # === EXPORT MULTI-FORMATS ===
        logger.info("üì§ Export multi-formats...")
        
        if default_output_config["chunked_export"] and len(df) > 10000:
            # Export par chunks pour les gros datasets
            export_results = self.advanced_exporter.export_chunked(
                df, 
                dataset_name="real_estate_consolidated",
                chunk_size=10000,
                output_dir=default_output_config["output_directory"]
            )
        else:
            # Export standard
            export_results = self.advanced_exporter.export_dataset(
                df,
                dataset_name="real_estate_consolidated",
                formats=default_output_config["formats"],
                output_dir=default_output_config["output_directory"]
            )
        
        # === EXPORT AVEC M√âTADONN√âES ===
        if default_output_config["include_metadata"]:
            logger.info("üìã Export avec m√©tadonn√©es...")
            
            metadata = {
                "pipeline_version": self.config.PIPELINE_VERSION,
                "consolidation_config": {
                    "groups_processed": self.consolidation_results.get("groups_processed", 0),
                    "reduction_percentage": self.consolidation_results.get("reduction_percentage", 0),
                    "target_reduction": self.config.TARGET_REDUCTION_PERCENTAGE
                },
                "quality_metrics": {
                    "overall_score": self.quality_metrics.get("quality_validation", {}).get("overall_score", 0),
                    "validation_status": self.quality_metrics.get("overall_status", "UNKNOWN")
                },
                "export_timestamp": datetime.now().isoformat()
            }
            
            metadata_export = self.advanced_exporter.export_with_metadata(
                df,
                dataset_name="real_estate_consolidated",
                metadata=metadata,
                output_dir=default_output_config["output_directory"]
            )
            
            export_results["metadata_export"] = metadata_export
        
        # === G√âN√âRATION DES RAPPORTS ===
        logger.info("üìä G√©n√©ration des rapports...")
        
        # Rapport de qualit√©
        quality_report_path = os.path.join(
            default_output_config["output_directory"], 
            "quality_report.md"
        )
        quality_report = self.quality_validator.generate_quality_report(
            output_path=quality_report_path
        )
        
        # Rapport d'export
        export_report_path = os.path.join(
            default_output_config["output_directory"], 
            "export_report.md"
        )
        export_report = self.advanced_exporter.generate_export_report(
            output_path=export_report_path
        )
        
        # Rapport de performance
        performance_report_path = os.path.join(
            default_output_config["output_directory"], 
            "performance_report.md"
        )
        performance_report = self.performance_optimizer.generate_performance_report(
            output_path=performance_report_path
        )
        
        # Rapport de similarit√©s
        similarity_report_path = os.path.join(
            default_output_config["output_directory"], 
            "similarity_report.md"
        )
        similarity_report = self.similarity_detector.generate_similarity_report(
            df, 
            output_path=similarity_report_path
        )
        
        # === COMPILATION DES R√âSULTATS ===
        load_results = {
            "export_results": export_results,
            "reports_generated": {
                "quality": quality_report_path,
                "export": export_report_path,
                "performance": performance_report_path,
                "similarity": similarity_report_path
            },
            "dataset_final_shape": df.shape,
            "export_timestamp": datetime.now().isoformat()
        }
        
        logger.info("‚úÖ Export et rapports termin√©s")
        logger.info(f"üìÅ Fichiers export√©s dans: {default_output_config['output_directory']}")
        
        return load_results
    
    def get_pipeline_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© du pipeline"""
        if not self.pipeline_history:
            return {"message": "Aucun pipeline ex√©cut√©"}
        
        # Dernier pipeline
        latest_pipeline = self.pipeline_history[-1]
        
        # Statistiques globales
        total_pipelines = len(self.pipeline_history)
        successful_pipelines = len([p for p in self.pipeline_history if p.get("status") == "SUCCESS"])
        failed_pipelines = len([p for p in self.pipeline_history if p.get("status") == "ERROR"])
        
        # M√©triques de consolidation
        consolidation_stats = self.consolidation_results or {}
        
        # M√©triques de qualit√©
        quality_stats = self.quality_metrics or {}
        
        return {
            "pipeline_summary": {
                "total_executions": total_pipelines,
                "successful_executions": successful_pipelines,
                "failed_executions": failed_pipelines,
                "success_rate": successful_pipelines / total_pipelines if total_pipelines > 0 else 0
            },
            "latest_execution": {
                "timestamp": latest_pipeline.get("pipeline_timestamp"),
                "duration_seconds": latest_pipeline.get("pipeline_duration_seconds"),
                "status": latest_pipeline.get("status"),
                "input_shape": latest_pipeline.get("input_shape"),
                "output_shape": latest_pipeline.get("output_shape")
            },
            "consolidation_metrics": consolidation_stats,
            "quality_metrics": quality_stats,
            "performance_summary": self.performance_optimizer.get_performance_summary()
        }
    
    def generate_comprehensive_report(self, output_path: str = None) -> str:
        """
        G√©n√®re un rapport complet du pipeline
        
        Args:
            output_path: Chemin de sauvegarde (optionnel)
            
        Returns:
            Contenu du rapport
        """
        logger.info("üìä === G√âN√âRATION RAPPORT COMPLET ===")
        
        # R√©cup√©ration du r√©sum√©
        summary = self.get_pipeline_summary()
        
        # G√©n√©ration du rapport
        report_content = []
        report_content.append("# " + "="*80)
        report_content.append("# RAPPORT COMPLET PIPELINE ETL ULTRA-INTELLIGENT")
        report_content.append("# " + "="*80)
        report_content.append(f"# Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_content.append(f"# Version: {self.config.PIPELINE_VERSION}")
        report_content.append("# " + "="*80 + "\n")
        
        # R√©sum√© ex√©cutif
        report_content.append("## R√âSUM√â EX√âCUTIF")
        pipeline_summary = summary.get("pipeline_summary", {})
        report_content.append(f"**Pipelines ex√©cut√©s:** {pipeline_summary.get('total_executions', 0)}")
        report_content.append(f"**Succ√®s:** {pipeline_summary.get('successful_executions', 0)}")
        report_content.append(f"**√âchecs:** {pipeline_summary.get('failed_executions', 0)}")
        report_content.append(f"**Taux de succ√®s:** {pipeline_summary.get('success_rate', 0):.1%}")
        report_content.append("")
        
        # Derni√®re ex√©cution
        latest = summary.get("latest_execution", {})
        report_content.append("## DERNI√àRE EX√âCUTION")
        report_content.append(f"**Statut:** {latest.get('status', 'UNKNOWN')}")
        report_content.append(f"**Date:** {latest.get('timestamp', 'N/A')}")
        report_content.append(f"**Dur√©e:** {latest.get('duration_seconds', 0):.2f}s")
        report_content.append(f"**Forme initiale:** {latest.get('input_shape', 'N/A')}")
        report_content.append(f"**Forme finale:** {latest.get('output_shape', 'N/A')}")
        report_content.append("")
        
        # M√©triques de consolidation
        consolidation = summary.get("consolidation_metrics", {})
        if consolidation:
            report_content.append("## M√âTRIQUES DE CONSOLIDATION")
            report_content.append(f"**Groupes trait√©s:** {consolidation.get('groups_processed', 0)}")
            report_content.append(f"**Colonnes consolid√©es:** {consolidation.get('columns_consolidated', 0)}")
            report_content.append(f"**Colonnes supprim√©es:** {consolidation.get('columns_removed', 0)}")
            report_content.append(f"**R√©duction:** {consolidation.get('reduction_percentage', 0):.1f}%")
            report_content.append("")
        
        # M√©triques de qualit√©
        quality = summary.get("quality_metrics", {})
        if quality:
            report_content.append("## M√âTRIQUES DE QUALIT√â")
            quality_validation = quality.get("quality_validation", {})
            report_content.append(f"**Score global:** {quality_validation.get('overall_score', 0):.1%}")
            report_content.append(f"**Statut validation:** {quality.get('overall_status', 'UNKNOWN')}")
            report_content.append("")
        
        # R√©sum√© des performances
        performance = summary.get("performance_summary", {})
        if "optimization_summary" in performance:
            opt_summary = performance["optimization_summary"]
            report_content.append("## R√âSUM√â DES PERFORMANCES")
            report_content.append(f"**Optimisations effectu√©es:** {opt_summary.get('total_optimizations', 0)}")
            report_content.append(f"**M√©moire √©conomis√©e:** {opt_summary.get('total_memory_saved_mb', 0):.2f} MB")
            report_content.append(f"**Temps moyen d'optimisation:** {opt_summary.get('average_optimization_time_seconds', 0):.3f}s")
            report_content.append("")
        
        # Configuration
        report_content.append("## CONFIGURATION")
        report_content.append(f"**Version pipeline:** {self.config.PIPELINE_VERSION}")
        report_content.append(f"**Mission:** {self.config.MISSION}")
        report_content.append(f"**Objectif:** {self.config.OBJECTIVE}")
        report_content.append(f"**R√©duction cible:** {self.config.TARGET_REDUCTION_PERCENTAGE}%")
        report_content.append(f"**R√©cup√©ration cible:** +{self.config.MIN_VALUES_RECOVERED_PERCENTAGE}%")
        report_content.append("")
        
        # Recommandations
        report_content.append("## RECOMMANDATIONS")
        if pipeline_summary.get("success_rate", 0) >= 0.9:
            report_content.append("‚úÖ **Excellent** - Le pipeline fonctionne parfaitement")
        elif pipeline_summary.get("success_rate", 0) >= 0.7:
            report_content.append("‚úÖ **Bon** - Le pipeline fonctionne bien avec quelques am√©liorations possibles")
        elif pipeline_summary.get("success_rate", 0) >= 0.5:
            report_content.append("‚ö†Ô∏è **Moyen** - Am√©liorations recommand√©es")
        else:
            report_content.append("‚ùå **Faible** - Actions correctives n√©cessaires")
        
        report_content.append("")
        report_content.append("# " + "="*80)
        report_content.append("# FIN DU RAPPORT")
        report_content.append("# " + "="*80)
        
        report_text = "\n".join(report_content)
        
        # Sauvegarde si un chemin est fourni
        if output_path:
            try:
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(report_text)
                logger.info(f"üìÑ Rapport complet sauvegard√©: {output_path}")
            except Exception as e:
                logger.error(f"‚ùå Erreur sauvegarde rapport: {e}")
        
        return report_text

    def _extract_categorization_stats(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Extrait les statistiques de cat√©gorisation des opportunit√©s
        
        Args:
            df: DataFrame avec colonnes de cat√©gorisation
            
        Returns:
            Dict avec statistiques de cat√©gorisation
        """
        stats = {}
        
        # === STATISTIQUES DES SEGMENTS ROI ===
        if 'segment_roi' in df.columns:
            stats['segments_roi'] = df['segment_roi'].value_counts().to_dict()
        
        # === STATISTIQUES DES CLASSES PRIX ===
        if 'classe_prix' in df.columns:
            stats['classes_prix'] = df['classe_prix'].value_counts().to_dict()
        
        # === STATISTIQUES DES TYPES D'OPPORTUNIT√âS ===
        if 'type_opportunite' in df.columns:
            stats['types_opportunites'] = df['type_opportunite'].value_counts().to_dict()
        
        # === STATISTIQUES DES CLASSES D'INVESTISSEMENT ===
        if 'classe_investissement' in df.columns:
            stats['classes_investissement'] = df['classe_investissement'].value_counts().to_dict()
        
        # === STATISTIQUES DES ZONES SPATIALES ===
        if 'spatial_zone' in df.columns:
            stats['zones_spatiales'] = df['spatial_zone'].value_counts().to_dict()
        
        # === STATISTIQUES DU SCORE QUALIT√â ===
        if 'score_qualite' in df.columns:
            stats['score_qualite'] = {
                'moyenne': df['score_qualite'].mean(),
                'ecart_type': df['score_qualite'].std(),
                'min': df['score_qualite'].min(),
                'max': df['score_qualite'].max(),
                'quartiles': df['score_qualite'].quantile([0.25, 0.5, 0.75]).to_dict()
            }
        
        return stats

    def categorize_investment_opportunities(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Cat√©gorisation automatique des opportunit√©s d'investissement
        Respecte les sp√©cifications du real_estate_prompt.md
        
        Args:
            df: DataFrame avec variables consolid√©es
            
        Returns:
            DataFrame avec nouvelles colonnes de cat√©gorisation
        """
        logger.info("üè∑Ô∏è === CAT√âGORISATION AUTOMATIQUE DES OPPORTUNIT√âS ===")
        
        df_categorized = df.copy()
        
        # === SEGMENTS ROI ===
        if 'roi_brut' in df_categorized.columns:
            logger.info("üí∞ Cat√©gorisation des segments ROI")
            df_categorized['segment_roi'] = pd.cut(
                df_categorized['roi_brut'],
                bins=[-np.inf, 0, 0.05, 0.10, 0.15, np.inf],
                labels=['Perte', 'Faible', 'Moyen', '√âlev√©', 'Premium'],
                include_lowest=True
            )
        
        # === CLASSES PRIX ===
        if 'price_final' in df_categorized.columns:
            logger.info("üíé Cat√©gorisation des classes prix")
            price_quantiles = df_categorized['price_final'].quantile([0.25, 0.5, 0.75])
            df_categorized['classe_prix'] = pd.cut(
                df_categorized['price_final'],
                bins=[0, price_quantiles[0.25], price_quantiles[0.5], 
                      price_quantiles[0.75], np.inf],
                labels=['√âconomique', 'Moyen', '√âlev√©', 'Premium'],
                include_lowest=True
            )
        
        # === TYPES D'OPPORTUNIT√âS ===
        logger.info("üéØ Classification des types d'opportunit√©s")
        df_categorized['type_opportunite'] = 'Standard'
        
        # Opportunit√©s √† fort potentiel
        high_potential_mask = (
            (df_categorized.get('roi_brut', 0) > 0.10) &
            (df_categorized.get('potentiel_plus_value', 0) > 0.20)
        )
        df_categorized.loc[high_potential_mask, 'type_opportunite'] = 'Fort Potentiel'
        
        # Opportunit√©s de r√©novation
        renovation_mask = (
            (df_categorized.get('year_built_final', 2024) < 1980) &
            (df_categorized.get('price_final', 0) < df_categorized.get('evaluation_total_final', 0) * 0.8)
        )
        df_categorized.loc[renovation_mask, 'type_opportunite'] = 'R√©novation'
        
        # Opportunit√©s de revenus
        revenue_mask = (
            (df_categorized.get('revenue_final', 0) > 0) &
            (df_categorized.get('roi_brut', 0) > 0.08)
        )
        df_categorized.loc[revenue_mask, 'type_opportunite'] = 'Revenus'
        
        # Opportunit√©s de plus-value
        appreciation_mask = (
            (df_categorized.get('potentiel_plus_value', 0) > 0.30) &
            (df_categorized.get('evaluation_year_final', 2024) < 2020)
        )
        df_categorized.loc[appreciation_mask, 'type_opportunite'] = 'Plus-Value'
        
        # === ZONES G√âOGRAPHIQUES DE PERFORMANCE ===
        if 'spatial_zone' in df_categorized.columns:
            logger.info("üåç Analyse des zones g√©ographiques de performance")
            
            # Calcul de la performance par zone
            zone_performance = df_categorized.groupby('spatial_zone').agg({
                'roi_brut': 'mean',
                'price_final': 'median',
                'revenue_final': 'mean'
            }).round(4)
            
            # Classification des zones
            zone_performance['classe_zone'] = pd.qcut(
                zone_performance['roi_brut'].fillna(0),
                q=4,
                labels=['Zone Faible', 'Zone Moyenne', 'Zone √âlev√©e', 'Zone Premium']
            )
            
            # Ajout de la classe de zone au DataFrame principal
            df_categorized = df_categorized.merge(
                zone_performance[['classe_zone']],
                left_on='spatial_zone',
                right_index=True,
                how='left'
            )
        
        # === SCORE QUALIT√â GLOBAL ===
        logger.info("üìä Calcul du score qualit√© global")
        quality_scores = []
        
        for idx, row in df_categorized.iterrows():
            score = 0
            
            # Compl√©tude des donn√©es (0-30 points)
            completeness = 1 - (row.isna().sum() / len(row))
            score += completeness * 30
            
            # Coh√©rence des donn√©es (0-25 points)
            if 'roi_brut' in row and 'price_final' in row and 'revenue_final' in row:
                if pd.notna(row['roi_brut']) and pd.notna(row['price_final']) and pd.notna(row['revenue_final']):
                    if row['roi_brut'] >= 0 and row['price_final'] > 0:
                        score += 25
            
            # Qualit√© g√©ographique (0-20 points)
            if 'latitude_final' in row and 'longitude_final' in row:
                if pd.notna(row['latitude_final']) and pd.notna(row['longitude_final']):
                    # V√©rification que les coordonn√©es sont dans des ranges raisonnables
                    if (45.0 <= row['latitude_final'] <= 75.0 and 
                        -80.0 <= row['longitude_final'] <= -50.0):  # Qu√©bec approximatif
                        score += 20
            
            # M√©triques financi√®res (0-25 points)
            if 'roi_brut' in row and pd.notna(row['roi_brut']):
                if 0 <= row['roi_brut'] <= 0.5:  # ROI raisonnable
                    score += 25
            
            quality_scores.append(min(score, 100))  # Plafonn√© √† 100
        
        df_categorized['score_qualite'] = quality_scores
        
        # === CLASSIFICATION FINALE DES OPPORTUNIT√âS ===
        logger.info("üèÜ Classification finale des opportunit√©s")
        df_categorized['classe_investissement'] = pd.cut(
            df_categorized['score_qualite'],
            bins=[0, 40, 60, 80, 100],
            labels=['Risqu√©', 'Moyen', 'Bon', 'Excellent'],
            include_lowest=True
        )
        
        # === STATISTIQUES DE CAT√âGORISATION ===
        categorization_stats = {
            'segments_roi': df_categorized.get('segment_roi', pd.Series()).value_counts().to_dict(),
            'classes_prix': df_categorized.get('classe_prix', pd.Series()).value_counts().to_dict(),
            'types_opportunites': df_categorized['type_opportunite'].value_counts().to_dict(),
            'classes_investissement': df_categorized['classe_investissement'].value_counts().to_dict(),
            'score_qualite_moyen': df_categorized['score_qualite'].mean(),
            'score_qualite_std': df_categorized['score_qualite'].std()
        }
        
        logger.info(f"üìä Statistiques de cat√©gorisation: {categorization_stats}")
        logger.info(f"‚úÖ Cat√©gorisation termin√©e: {len(df_categorized)} propri√©t√©s analys√©es")
        
        return df_categorized
