#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ OPTIMISEUR DE PERFORMANCE - ACC√âL√âRATION ET OPTIMISATION
===========================================================

Module d'optimisation des performances avec Dask, Modin, Numba, Cython
Bas√© sur les sp√©cifications du real_estate_prompt.md
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any, Union, Callable
import warnings
import time
import psutil
import gc
import os
from pathlib import Path
import json
from datetime import datetime

# Imports conditionnels pour les biblioth√®ques d'optimisation
try:
    import dask.dataframe as dd
    import dask.array as da
    DASK_AVAILABLE = True
except ImportError:
    DASK_AVAILABLE = False
    warnings.warn("Dask non disponible - parall√©lisation limit√©e")

try:
    import modin.pandas as mpd
    MODIN_AVAILABLE = True
except ImportError:
    MODIN_AVAILABLE = False
    warnings.warn("Modin non disponible - parall√©lisation pandas limit√©e")

try:
    import numba
    from numba import jit, prange
    NUMBA_AVAILABLE = True
    JIT_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
    JIT_AVAILABLE = False
    warnings.warn("Numba non disponible - compilation JIT limit√©e")

try:
    import pyarrow as pa
    PYARROW_AVAILABLE = True
except ImportError:
    PYARROW_AVAILABLE = False
    warnings.warn("PyArrow non disponible - optimisations m√©moire limit√©es")

try:
    from memory_profiler import profile
    MEMORY_PROFILER_AVAILABLE = True
except ImportError:
    MEMORY_PROFILER_AVAILABLE = False
    warnings.warn("Memory Profiler non disponible - profilage m√©moire limit√©")

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

class PerformanceOptimizer:
    """
    Optimiseur de performance pour le pipeline ETL
    Int√®gre Dask + Modin + Numba + PyArrow + Memory Profiler
    """
    
    def __init__(self, performance_config: Dict = None):
        """
        Initialise l'optimiseur de performance
        
        Args:
            performance_config: Configuration de performance
        """
        self.performance_config = performance_config or self._default_performance_config()
        self.performance_metrics = {}
        self.optimization_history = []
        self.memory_usage_history = []
        
        logger.info("üöÄ PerformanceOptimizer initialis√©")
        logger.info(f"‚ö° Dask: {'‚úÖ' if DASK_AVAILABLE else '‚ùå'}")
        logger.info(f"üîÑ Modin: {'‚úÖ' if MODIN_AVAILABLE else '‚ùå'}")
        logger.info(f"‚öôÔ∏è Numba: {'‚úÖ' if NUMBA_AVAILABLE else '‚ùå'}")
        logger.info(f"üèπ PyArrow: {'‚úÖ' if PYARROW_AVAILABLE else '‚ùå'}")
        logger.info(f"üíæ Memory Profiler: {'‚úÖ' if MEMORY_PROFILER_AVAILABLE else '‚ùå'}")
    
    def _default_performance_config(self) -> Dict:
        """Configuration de performance par d√©faut"""
        return {
            "parallel_processing": {
                "enabled": True,
                "n_workers": min(8, os.cpu_count() or 1),  # Augment√© pour performance maximale
                "chunk_size": 10000,
                "memory_limit": "4GB"  # Augment√© pour gros datasets
            },
            "dask_optimization": {
                "enabled": True,  # Activ√© par d√©faut comme sp√©cifi√©
                "partition_size": "100MB",
                "npartitions": None,  # Auto-d√©tection
                "persist_intermediate": True
            },
            "modin_optimization": {
                "enabled": True,  # Activ√© par d√©faut comme sp√©cifi√©
                "engine": "ray",  # Utilise Ray pour parall√©lisation
                "partition_size": "100MB"
            },
            "memory_optimization": {
                "enabled": True,
                "dtype_optimization": True,
                "categorical_optimization": True,
                "chunk_processing": True,
                "garbage_collection": True
            },
            "numba_optimization": {
                "enabled": True,
                "parallel": True,
                "fastmath": True
            },
            "pyarrow_optimization": {
                "enabled": True,
                "use_pyarrow": True,
                "compression": "snappy"
            }
        }
    
    def optimize_dataframe(self, df: pd.DataFrame, optimization_level: str = "medium") -> pd.DataFrame:
        """
        Optimise un DataFrame selon le niveau sp√©cifi√©
        
        Args:
            df: DataFrame √† optimiser
            optimization_level: Niveau d'optimisation (light, medium, aggressive)
            
        Returns:
            DataFrame optimis√©
        """
        logger.info(f"üöÄ === OPTIMISATION DATAFRAME: {optimization_level.upper()} ===")
        
        optimization_start = time.time()
        initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        
        # === OPTIMISATION M√âMOIRE ===
        if self.performance_config["memory_optimization"]["enabled"]:
            logger.info("üíæ Optimisation m√©moire...")
            df = self._optimize_memory_usage(df, optimization_level)
        
        # === OPTIMISATION DES TYPES ===
        if self.performance_config["memory_optimization"]["dtype_optimization"]:
            logger.info("üîß Optimisation des types de donn√©es...")
            df = self._optimize_data_types(df, optimization_level)
        
        # === OPTIMISATION CAT√âGORIELLE ===
        if self.performance_config["memory_optimization"]["categorical_optimization"]:
            logger.info("üìä Optimisation cat√©gorielle...")
            df = self._optimize_categorical_columns(df, optimization_level)
        
        # === OPTIMISATION PYARROW ===
        if self.performance_config["pyarrow_optimization"]["enabled"] and PYARROW_AVAILABLE:
            logger.info("üèπ Optimisation PyArrow...")
            df = self._apply_pyarrow_optimizations(df)
        
        # === CALCUL DES M√âTRIQUES ===
        optimization_end = time.time()
        final_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        
        memory_reduction = ((initial_memory - final_memory) / initial_memory) * 100
        optimization_time = optimization_end - optimization_start
        
        # Enregistrement des m√©triques
        optimization_record = {
            "timestamp": datetime.now().isoformat(),
            "optimization_level": optimization_level,
            "initial_memory_mb": initial_memory,
            "final_memory_mb": final_memory,
            "memory_reduction_percent": memory_reduction,
            "optimization_time_seconds": optimization_time,
            "shape": df.shape
        }
        
        self.optimization_history.append(optimization_record)
        
        logger.info(f"‚úÖ Optimisation termin√©e en {optimization_time:.2f}s")
        logger.info(f"üíæ M√©moire: {initial_memory:.2f} MB ‚Üí {final_memory:.2f} MB")
        logger.info(f"üìâ R√©duction: {memory_reduction:.1f}%")
        
        return df
    
    def _optimize_memory_usage(self, df: pd.DataFrame, level: str) -> pd.DataFrame:
        """Optimise l'utilisation m√©moire du DataFrame"""
        optimized_df = df.copy()
        
        # === OPTIMISATION DES TYPES NUM√âRIQUES ===
        if level in ["medium", "aggressive"]:
            for col in optimized_df.select_dtypes(include=[np.number]).columns:
                col_type = optimized_df[col].dtype
                
                if col_type == 'int64':
                    # Conversion en int32 ou int16 si possible
                    c_min = optimized_df[col].min()
                    c_max = optimized_df[col].max()
                    
                    if c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        optimized_df[col] = optimized_df[col].astype(np.int32)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        optimized_df[col] = optimized_df[col].astype(np.int16)
                
                elif col_type == 'float64':
                    # Conversion en float32 si possible
                    if optimized_df[col].notna().all():
                        c_min = optimized_df[col].min()
                        c_max = optimized_df[col].max()
                        
                        if c_min >= -3.4e38 and c_max <= 3.4e38:
                            optimized_df[col] = optimized_df[col].astype(np.float32)
        
        # === OPTIMISATION DES TYPES OBJET ===
        if level in ["medium", "aggressive"]:
            for col in optimized_df.select_dtypes(include=['object']).columns:
                # Conversion en string[pyarrow] si disponible
                if PYARROW_AVAILABLE and self.performance_config["pyarrow_optimization"]["use_pyarrow"]:
                    optimized_df[col] = optimized_df[col].astype(self.performance_config["pyarrow_optimization"]["string_dtype"])
        
        return optimized_df
    
    def _optimize_data_types(self, df: pd.DataFrame, level: str) -> pd.DataFrame:
        """Optimise les types de donn√©es pour la performance"""
        optimized_df = df.copy()
        
        # === D√âTECTION AUTOMATIQUE DES TYPES ===
        for col in optimized_df.columns:
            if optimized_df[col].dtype == 'object':
                # Tentative de conversion en num√©rique
                try:
                    pd.to_numeric(optimized_df[col], errors='raise')
                    optimized_df[col] = pd.to_numeric(optimized_df[col], errors='coerce')
                    logger.debug(f"üî¢ Colonne '{col}' convertie en num√©rique")
                except:
                    # Tentative de conversion en datetime
                    try:
                        pd.to_datetime(optimized_df[col], errors='raise')
                        optimized_df[col] = pd.to_datetime(optimized_df[col], errors='coerce')
                        logger.debug(f"üìÖ Colonne '{col}' convertie en datetime")
                    except:
                        pass
        
        return optimized_df
    
    def _optimize_categorical_columns(self, df: pd.DataFrame, level: str) -> pd.DataFrame:
        """Optimise les colonnes cat√©gorielles"""
        optimized_df = df.copy()
        
        # Seuil pour la conversion en cat√©gorie
        categorical_threshold = 0.5 if level == "light" else 0.7 if level == "medium" else 0.9
        
        for col in optimized_df.select_dtypes(include=['object']).columns:
            try:
                # V√©rifier si la colonne peut √™tre convertie en cat√©gorie
                # en testant si les valeurs sont hashables
                sample_values = optimized_df[col].dropna().head(100)
                hashable_values = []
                
                for val in sample_values:
                    try:
                        hash(val)  # Test si la valeur est hashable
                        hashable_values.append(val)
                    except TypeError:
                        # Valeur non-hashable, la traiter comme une cha√Æne
                        hashable_values.append(str(val))
                
                if len(hashable_values) > 0:
                    # Calculer le ratio d'unicit√© sur les valeurs hashables
                    unique_count = len(set(hashable_values))
                    unique_ratio = unique_count / len(optimized_df)
                    
                    if unique_ratio < categorical_threshold:
                        # Convertir en cat√©gorie en g√©rant les valeurs non-hashables
                        safe_series = optimized_df[col].apply(
                            lambda x: str(x) if x is not None else None
                        )
                        optimized_df[col] = safe_series.astype('category')
                        logger.debug(f"üìä Colonne '{col}' convertie en cat√©gorie (ratio: {unique_ratio:.2f})")
                else:
                    logger.debug(f"‚ö†Ô∏è Colonne '{col}' ignor√©e - aucune valeur hashable")
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible d'optimiser la colonne '{col}' en cat√©gorie: {e}")
                continue
        
        return optimized_df
    
    def _apply_pyarrow_optimizations(self, df: pd.DataFrame) -> pd.DataFrame:
        """Applique les optimisations PyArrow"""
        if not PYARROW_AVAILABLE:
            return df
        
        try:
            # Conversion en format PyArrow
            optimized_df = df.copy()
            
            # Optimisation des colonnes string
            for col in optimized_df.select_dtypes(include=['object']).columns:
                if self.performance_config["pyarrow_optimization"]["use_pyarrow"]:
                    optimized_df[col] = optimized_df[col].astype(self.performance_config["pyarrow_optimization"]["string_dtype"])
            
            return optimized_df
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Optimisation PyArrow √©chou√©e: {e}")
            return df
    
    def parallelize_operation(self, df: pd.DataFrame, operation: Callable, 
                             operation_name: str = "operation", **kwargs) -> pd.DataFrame:
        """
        Parall√©lise une op√©ration sur le DataFrame
        
        Args:
            df: DataFrame √† traiter
            operation: Fonction √† appliquer
            operation_name: Nom de l'op√©ration pour le logging
            **kwargs: Arguments suppl√©mentaires pour l'op√©ration
            
        Returns:
            DataFrame trait√©
        """
        if not self.performance_config["parallel_processing"]["enabled"]:
            logger.info(f"‚ö° Parall√©lisation d√©sactiv√©e - ex√©cution s√©quentielle")
            return operation(df, **kwargs)
        
        if not DASK_AVAILABLE:
            logger.warning("‚ö†Ô∏è Dask non disponible - ex√©cution s√©quentielle")
            return operation(df, **kwargs)
        
        logger.info(f"‚ö° === PARALL√âLISATION: {operation_name} ===")
        
        # === CONVERSION EN DASK DATAFRAME ===
        logger.info("üîÑ Conversion en Dask DataFrame...")
        ddf = dd.from_pandas(df, npartitions=self._calculate_optimal_partitions(df))
        
        # === APPLICATION DE L'OP√âRATION ===
        logger.info(f"üöÄ Application de {operation_name} en parall√®le...")
        start_time = time.time()
        
        try:
            # Application de l'op√©ration
            result_ddf = operation(ddf, **kwargs)
            
            # Conversion de retour en pandas
            result_df = result_ddf.compute()
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            logger.info(f"‚úÖ Op√©ration parall√©lis√©e termin√©e en {processing_time:.2f}s")
            
            return result_df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur parall√©lisation: {e}")
            logger.info("üîÑ Retour √† l'ex√©cution s√©quentielle...")
            return operation(df, **kwargs)
    
    def _calculate_optimal_partitions(self, df: pd.DataFrame) -> int:
        """Calcule le nombre optimal de partitions pour Dask"""
        config = self.performance_config["parallel_processing"]
        n_workers = config["n_workers"]
        
        # Calcul bas√© sur la taille du DataFrame
        total_rows = len(df)
        chunk_size = config["chunk_size"]
        
        optimal_partitions = max(1, min(n_workers * 2, total_rows // chunk_size))
        
        return optimal_partitions
    
    def optimize_numeric_operations(self, df: pd.DataFrame, operations: List[str]) -> pd.DataFrame:
        """
        Optimise les op√©rations num√©riques avec Numba
        
        Args:
            df: DataFrame √† optimiser
            operations: Liste des op√©rations √† optimiser
            
        Returns:
            DataFrame avec op√©rations optimis√©es
        """
        if not NUMBA_AVAILABLE or not self.performance_config["numba_optimization"]["enabled"]:
            logger.info("‚ö†Ô∏è Numba non disponible - op√©rations standard")
            return df
        
        logger.info("‚öôÔ∏è === OPTIMISATION NUM√âRIQUE AVEC NUMBA ===")
        
        optimized_df = df.copy()
        
        for operation in operations:
            if operation == "rolling_mean":
                optimized_df = self._apply_numba_rolling_mean(optimized_df)
            elif operation == "rolling_std":
                optimized_df = self._apply_numba_rolling_std(optimized_df)
            elif operation == "z_score":
                optimized_df = self._apply_numba_z_score(optimized_df)
            elif operation == "outlier_detection":
                optimized_df = self._apply_numba_outlier_detection(optimized_df)
        
        return optimized_df
    
    @staticmethod
    def _numba_rolling_mean(arr, window):
        if JIT_AVAILABLE:
            return jit(nopython=True, parallel=True, fastmath=True, cache=True)(_numba_rolling_mean_impl)(arr, window)
        else:
            return _numba_rolling_mean_impl(arr, window)
    
    @staticmethod
    def _numba_rolling_mean_impl(arr, window):
        """Calcul de moyenne mobile optimis√© avec Numba"""
        n = len(arr)
        result = np.empty(n)
        result[:window-1] = np.nan
        
        for i in range(window-1, n):
            result[i] = np.mean(arr[i-window+1:i+1])
        
        return result
    
    @staticmethod
    def _numba_rolling_std(arr, window):
        if JIT_AVAILABLE:
            return jit(nopython=True, parallel=True, fastmath=True, cache=True)(_numba_rolling_std_impl)(arr, window)
        else:
            return _numba_rolling_std_impl(arr, window)
    
    @staticmethod
    def _numba_rolling_std_impl(arr, window):
        """Calcul d'√©cart-type mobile optimis√© avec Numba"""
        n = len(arr)
        result = np.empty(n)
        result[:window-1] = np.nan
        
        for i in range(window-1, n):
            result[i] = np.std(arr[i-window+1:i+1])
        
        return result
    
    @staticmethod
    def _numba_z_score(arr):
        if JIT_AVAILABLE:
            return jit(nopython=True, parallel=True, fastmath=True, cache=True)(_numba_z_score_impl)(arr)
        else:
            return _numba_z_score_impl(arr)
    
    @staticmethod
    def _numba_z_score_impl(arr):
        """Calcul de Z-score optimis√© avec Numba"""
        mean = np.mean(arr)
        std = np.std(arr)
        
        if std == 0:
            return np.zeros_like(arr)
        
        return (arr - mean) / std
    
    @staticmethod
    def _numba_outlier_detection(arr, threshold=3.0):
        if JIT_AVAILABLE:
            return jit(nopython=True, parallel=True, fastmath=True, cache=True)(_numba_outlier_detection_impl)(arr, threshold)
        else:
            return _numba_outlier_detection_impl(arr, threshold)
    
    @staticmethod
    def _numba_outlier_detection_impl(arr, threshold=3.0):
        """D√©tection d'outliers optimis√©e avec Numba"""
        z_scores = np.abs((arr - np.mean(arr)) / np.std(arr))
        return z_scores > threshold
    
    def _apply_numba_rolling_mean(self, df: pd.DataFrame) -> pd.DataFrame:
        """Applique la moyenne mobile optimis√©e par Numba"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if df[col].notna().sum() > 10:  # Au moins 10 valeurs non-nulles
                window = min(10, len(df) // 10)  # Fen√™tre adaptative
                df[f"{col}_rolling_mean"] = self._numba_rolling_mean(
                    df[col].fillna(0).values, window
                )
        
        return df
    
    def _apply_numba_rolling_std(self, df: pd.DataFrame) -> pd.DataFrame:
        """Applique l'√©cart-type mobile optimis√© par Numba"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if df[col].notna().sum() > 10:
                window = min(10, len(df) // 10)
                df[f"{col}_rolling_std"] = self._numba_rolling_std(
                    df[col].fillna(0).values, window
                )
        
        return df
    
    def _apply_numba_z_score(self, df: pd.DataFrame) -> pd.DataFrame:
        """Applique le Z-score optimis√© par Numba"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if df[col].notna().sum() > 10:
                df[f"{col}_z_score"] = self._numba_z_score(df[col].fillna(0).values)
        
        return df
    
    def _apply_numba_outlier_detection(self, df: pd.DataFrame) -> pd.DataFrame:
        """Applique la d√©tection d'outliers optimis√©e par Numba"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if df[col].notna().sum() > 10:
                df[f"{col}_is_outlier"] = self._numba_outlier_detection(
                    df[col].fillna(0).values, threshold=3.0
                )
        
        return df
    
    def monitor_memory_usage(self, df: pd.DataFrame, operation_name: str = "operation") -> Dict[str, float]:
        """
        Surveille l'utilisation m√©moire
        
        Args:
            df: DataFrame √† surveiller
            operation_name: Nom de l'op√©ration
            
        Returns:
            Dict avec les m√©triques m√©moire
        """
        if not self.performance_config["monitoring"]["memory_tracking"]:
            return {}
        
        # === UTILISATION M√âMOIRE DU DATAFRAME ===
        df_memory = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
        
        # === UTILISATION M√âMOIRE SYST√àME ===
        process = psutil.Process()
        process_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # === UTILISATION M√âMOIRE SYST√àME ===
        system_memory = psutil.virtual_memory()
        system_used = system_memory.used / 1024 / 1024 / 1024  # GB
        system_total = system_memory.total / 1024 / 1024 / 1024  # GB
        system_percent = system_memory.percent
        
        memory_metrics = {
            "timestamp": datetime.now().isoformat(),
            "operation": operation_name,
            "dataframe_memory_mb": df_memory,
            "process_memory_mb": process_memory,
            "system_memory_used_gb": system_used,
            "system_memory_total_gb": system_total,
            "system_memory_percent": system_percent
        }
        
        self.memory_usage_history.append(memory_metrics)
        
        logger.info(f"üíæ M√©moire - DataFrame: {df_memory:.2f} MB, Process: {process_memory:.2f} MB")
        logger.info(f"üíæ Syst√®me: {system_percent:.1f}% ({system_used:.2f}/{system_total:.2f} GB)")
        
        return memory_metrics
    
    def optimize_garbage_collection(self, force: bool = False) -> Dict[str, Any]:
        """
        Optimise la gestion de la m√©moire avec garbage collection
        
        Args:
            force: Force la collecte des d√©chets
            
        Returns:
            Dict avec les m√©triques de GC
        """
        if not self.performance_config["monitoring"]["gc_optimization"]:
            return {}
        
        logger.info("üóëÔ∏è === OPTIMISATION GARBAGE COLLECTION ===")
        
        # === M√âTRIQUES AVANT GC ===
        before_memory = psutil.virtual_memory().used / 1024 / 1024 / 1024  # GB
        before_objects = len(gc.get_objects())
        
        # === COLLECTE DES D√âCHETS ===
        start_time = time.time()
        
        if force:
            collected = gc.collect()
        else:
            collected = gc.collect()
        
        end_time = time.time()
        gc_time = end_time - start_time
        
        # === M√âTRIQUES APR√àS GC ===
        after_memory = psutil.virtual_memory().used / 1024 / 1024 / 1024  # GB
        after_objects = len(gc.get_objects())
        
        # === CALCUL DES AM√âLIORATIONS ===
        memory_freed = before_memory - after_memory
        objects_freed = before_objects - after_objects
        
        gc_metrics = {
            "timestamp": datetime.now().isoformat(),
            "before_memory_gb": before_memory,
            "after_memory_gb": after_memory,
            "memory_freed_gb": memory_freed,
            "before_objects": before_objects,
            "after_objects": after_objects,
            "objects_freed": objects_freed,
            "gc_time_seconds": gc_time,
            "collected": collected
        }
        
        logger.info(f"‚úÖ GC termin√© en {gc_time:.3f}s")
        logger.info(f"üíæ M√©moire lib√©r√©e: {memory_freed:.3f} GB")
        logger.info(f"üóëÔ∏è Objets lib√©r√©s: {objects_freed}")
        
        return gc_metrics
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© des performances"""
        if not self.optimization_history:
            return {"message": "Aucune optimisation effectu√©e"}
        
        # === M√âTRIQUES D'OPTIMISATION ===
        total_optimizations = len(self.optimization_history)
        total_memory_saved = sum(
            record["memory_reduction_percent"] * record["initial_memory_mb"] / 100
            for record in self.optimization_history
        )
        avg_optimization_time = np.mean([
            record["optimization_time_seconds"] for record in self.optimization_history
        ])
        
        # === M√âTRIQUES M√âMOIRE ===
        memory_records = len(self.memory_usage_history)
        avg_dataframe_memory = np.mean([
            record["dataframe_memory_mb"] for record in self.memory_usage_history
        ]) if memory_records > 0 else 0
        
        # === M√âTRIQUES SYST√àME ===
        avg_system_memory = np.mean([
            record["system_memory_percent"] for record in self.memory_usage_history
        ]) if memory_records > 0 else 0
        
        return {
            "optimization_summary": {
                "total_optimizations": total_optimizations,
                "total_memory_saved_mb": total_memory_saved,
                "average_optimization_time_seconds": avg_optimization_time
            },
            "memory_summary": {
                "memory_tracking_records": memory_records,
                "average_dataframe_memory_mb": avg_dataframe_memory,
                "average_system_memory_percent": avg_system_memory
            },
            "capabilities": {
                "dask_available": DASK_AVAILABLE,
                "modin_available": MODIN_AVAILABLE,
                "numba_available": NUMBA_AVAILABLE,
                "pyarrow_available": PYARROW_AVAILABLE,
                "memory_profiler_available": MEMORY_PROFILER_AVAILABLE
            },
            "configuration": self.performance_config
        }
    
    def generate_performance_report(self, output_path: str = None) -> str:
        """
        G√©n√®re un rapport de performance complet
        
        Args:
            output_path: Chemin de sauvegarde (optionnel)
            
        Returns:
            Contenu du rapport
        """
        logger.info("üìä === G√âN√âRATION RAPPORT PERFORMANCE ===")
        
        # R√©cup√©ration du r√©sum√©
        summary = self.get_performance_summary()
        
        # G√©n√©ration du rapport
        report_content = []
        report_content.append("# " + "="*80)
        report_content.append("# RAPPORT DE PERFORMANCE ET OPTIMISATION")
        report_content.append("# " + "="*80)
        report_content.append(f"# Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_content.append("# " + "="*80 + "\n")
        
        # R√©sum√© ex√©cutif
        report_content.append("## R√âSUM√â EX√âCUTIF")
        if "optimization_summary" in summary:
            opt_summary = summary["optimization_summary"]
            report_content.append(f"**Optimisations effectu√©es:** {opt_summary['total_optimizations']}")
            report_content.append(f"**M√©moire totale √©conomis√©e:** {opt_summary['total_memory_saved_mb']:.2f} MB")
            report_content.append(f"**Temps moyen d'optimisation:** {opt_summary['average_optimization_time_seconds']:.3f}s")
        else:
            report_content.append("**Aucune optimisation effectu√©e**")
        report_content.append("")
        
        # Capacit√©s disponibles
        report_content.append("## CAPACIT√âS D'OPTIMISATION")
        capabilities = summary.get("capabilities", {})
        for capability, available in capabilities.items():
            status = "‚úÖ Disponible" if available else "‚ùå Non disponible"
            report_content.append(f"**{capability}:** {status}")
        report_content.append("")
        
        # Historique des optimisations
        if self.optimization_history:
            report_content.append("## HISTORIQUE DES OPTIMISATIONS")
            for i, record in enumerate(self.optimization_history[-5:], 1):  # 5 derni√®res
                report_content.append(f"### Optimisation {i}")
                report_content.append(f"**Niveau:** {record['optimization_level']}")
                report_content.append(f"**M√©moire initiale:** {record['initial_memory_mb']:.2f} MB")
                report_content.append(f"**M√©moire finale:** {record['final_memory_mb']:.2f} MB")
                report_content.append(f"**R√©duction:** {record['memory_reduction_percent']:.1f}%")
                report_content.append(f"**Temps:** {record['optimization_time_seconds']:.3f}s")
                report_content.append("")
        
        # Configuration
        report_content.append("## CONFIGURATION")
        config = summary.get("configuration", {})
        for section, settings in config.items():
            report_content.append(f"### {section.replace('_', ' ').title()}")
            for key, value in settings.items():
                report_content.append(f"**{key}:** {value}")
            report_content.append("")
        
        report_content.append("# " + "="*80)
        report_content.append("# FIN DU RAPPORT")
        report_content.append("# " + "="*80)
        
        report_text = "\n".join(report_content)
        
        # Sauvegarde si un chemin est fourni
        if output_path:
            try:
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(report_text)
                logger.info(f"üìÑ Rapport de performance sauvegard√©: {output_path}")
            except Exception as e:
                logger.error(f"‚ùå Erreur sauvegarde rapport: {e}")
        
        return report_text
    
    def benchmark_operation(self, df: pd.DataFrame, operation: Callable, 
                           operation_name: str = "operation", iterations: int = 3, **kwargs) -> Dict[str, Any]:
        """
        Benchmark d'une op√©ration pour mesurer les performances
        
        Args:
            df: DataFrame √† traiter
            operation: Fonction √† benchmarker
            operation_name: Nom de l'op√©ration
            iterations: Nombre d'it√©rations pour le benchmark
            **kwargs: Arguments suppl√©mentaires pour l'op√©ration
            
        Returns:
            Dict avec les m√©triques de benchmark
        """
        logger.info(f"‚è±Ô∏è === BENCHMARK: {operation_name} ===")
        
        # === M√âTRIQUES AVANT ===
        before_memory = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
        before_shape = df.shape
        
        # === EX√âCUTION DU BENCHMARK ===
        execution_times = []
        memory_usage = []
        
        for i in range(iterations):
            logger.info(f"üîÑ It√©ration {i+1}/{iterations}...")
            
            # Mesure du temps
            start_time = time.time()
            result_df = operation(df, **kwargs)
            end_time = time.time()
            
            execution_time = end_time - start_time
            execution_times.append(execution_time)
            
            # Mesure de la m√©moire
            result_memory = result_df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
            memory_usage.append(result_memory)
            
            logger.info(f"‚è±Ô∏è It√©ration {i+1}: {execution_time:.3f}s, {result_memory:.2f} MB")
        
        # === CALCUL DES M√âTRIQUES ===
        avg_execution_time = np.mean(execution_times)
        std_execution_time = np.std(execution_times)
        avg_memory_usage = np.mean(memory_usage)
        
        # === M√âTRIQUES FINALES ===
        after_shape = result_df.shape
        memory_change = ((avg_memory_usage - before_memory) / before_memory) * 100
        
        benchmark_results = {
            "operation_name": operation_name,
            "iterations": iterations,
            "execution_times": execution_times,
            "average_execution_time_seconds": avg_execution_time,
            "std_execution_time_seconds": std_execution_time,
            "memory_usage_mb": memory_usage,
            "average_memory_usage_mb": avg_memory_usage,
            "memory_change_percent": memory_change,
            "before_shape": before_shape,
            "after_shape": after_shape,
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"üìä === R√âSULTATS BENCHMARK ===")
        logger.info(f"‚è±Ô∏è Temps moyen: {avg_execution_time:.3f}s ¬± {std_execution_time:.3f}s")
        logger.info(f"üíæ M√©moire moyenne: {avg_memory_usage:.2f} MB")
        logger.info(f"üìâ Changement m√©moire: {memory_change:.1f}%")
        logger.info(f"üìê Forme: {before_shape} ‚Üí {after_shape}")
        
        return benchmark_results

    def enable_all_optimizations(self) -> Dict[str, bool]:
        """
        Active toutes les optimisations disponibles pour performance maximale
        Respecte les sp√©cifications du real_estate_prompt.md
        """
        logger.info("üöÄ === ACTIVATION DE TOUTES LES OPTIMISATIONS ===")
        
        optimizations_status = {}
        
        # === DASK OPTIMIZATION ===
        if DASK_AVAILABLE and self.performance_config["dask_optimization"]["enabled"]:
            try:
                import dask.dataframe as dd
                self.use_dask = True
                self.dask_config = self.performance_config["dask_optimization"]
                optimizations_status["dask"] = True
                logger.info("‚úÖ Dask activ√© pour traitement parall√®le")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Dask non activ√©: {e}")
                optimizations_status["dask"] = False
        else:
            optimizations_status["dask"] = False
        
        # === MODIN OPTIMIZATION ===
        if MODIN_AVAILABLE and self.performance_config["modin_optimization"]["enabled"]:
            try:
                import modin.pandas as mpd
                self.use_modin = True
                self.modin_config = self.performance_config["modin_optimization"]
                optimizations_status["modin"] = True
                logger.info("‚úÖ Modin activ√© pour pandas parall√®le")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Modin non activ√©: {e}")
                optimizations_status["modin"] = False
        else:
            optimizations_status["modin"] = False
        
        # === NUMBA OPTIMIZATION ===
        if NUMBA_AVAILABLE and self.performance_config["numba_optimization"]["enabled"]:
            try:
                self.use_numba = True
                self.numba_config = self.performance_config["numba_optimization"]
                optimizations_status["numba"] = True
                logger.info("‚úÖ Numba activ√© pour compilation JIT")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Numba non activ√©: {e}")
                optimizations_status["numba"] = False
        else:
            optimizations_status["numba"] = False
        
        # === PYARROW OPTIMIZATION ===
        if PYARROW_AVAILABLE and self.performance_config["pyarrow_optimization"]["enabled"]:
            try:
                self.use_pyarrow = True
                self.pyarrow_config = self.performance_config["pyarrow_optimization"]
                optimizations_status["pyarrow"] = True
                logger.info("‚úÖ PyArrow activ√© pour optimisations m√©moire")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è PyArrow non activ√©: {e}")
                optimizations_status["pyarrow"] = False
        else:
            optimizations_status["pyarrow"] = False
        
        # === MEMORY OPTIMIZATION ===
        if self.performance_config["memory_optimization"]["enabled"]:
            self.use_memory_optimization = True
            optimizations_status["memory"] = True
            logger.info("‚úÖ Optimisations m√©moire activ√©es")
        else:
            optimizations_status["memory"] = False
        
        # === R√âSUM√â DES OPTIMISATIONS ===
        active_count = sum(optimizations_status.values())
        total_count = len(optimizations_status)
        
        logger.info(f"üéØ Optimisations activ√©es: {active_count}/{total_count}")
        logger.info(f"üìä Status: {optimizations_status}")
        
        return optimizations_status
