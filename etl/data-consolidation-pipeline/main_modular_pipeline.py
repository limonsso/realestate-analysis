#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸš€ PIPELINE ETL ULTRA-INTELLIGENT - SCRIPT PRINCIPAL
=====================================================

Script principal pour la consolidation maximale des variables immobiliÃ¨res
BasÃ© sur les spÃ©cifications du real_estate_prompt.md
"""

import argparse
import logging
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
import warnings

# Suppression des warnings pour un affichage plus propre
warnings.filterwarnings('ignore')

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('pipeline.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# Import des modules du pipeline
try:
    # Import de l'orchestrateur principal unifiÃ©
    from core.main_pipeline_orchestrator import MainPipelineOrchestrator
    
    # Import des modules externes (optionnels)
    try:
        from config.consolidation_config import ConsolidationConfig
        CONFIG_AVAILABLE = True
        logger.info("âœ… Configuration de consolidation disponible")
    except ImportError:
        CONFIG_AVAILABLE = False
        logger.warning("âš ï¸ Configuration de consolidation non disponible")
    
    try:
        from intelligence.similarity_detector import SimilarityDetector
        SIMILARITY_AVAILABLE = True
        logger.info("âœ… DÃ©tecteur de similaritÃ© disponible")
    except ImportError:
        SIMILARITY_AVAILABLE = False
        logger.warning("âš ï¸ DÃ©tecteur de similaritÃ© non disponible")
    
    try:
        from validation.quality_validator import QualityValidator
        VALIDATOR_AVAILABLE = True
        logger.info("âœ… Validateur de qualitÃ© disponible")
    except ImportError:
        VALIDATOR_AVAILABLE = False
        logger.warning("âš ï¸ Validateur de qualitÃ© non disponible")
    
    try:
        from export.advanced_exporter import AdvancedExporter
        EXPORTER_AVAILABLE = True
        logger.info("âœ… Exportateur avancÃ© disponible")
    except ImportError:
        EXPORTER_AVAILABLE = False
        logger.warning("âš ï¸ Exportateur avancÃ© non disponible")
    
    try:
        from performance.performance_optimizer import PerformanceOptimizer
        OPTIMIZER_AVAILABLE = True
        logger.info("âœ… Optimiseur de performance disponible")
    except ImportError:
        OPTIMIZER_AVAILABLE = False
        logger.warning("âš ï¸ Optimiseur de performance non disponible")
    
    try:
        from utils.db import read_mongodb_to_dataframe, get_mongodb_stats
        from utils.property_type_normalizer import PropertyTypeNormalizer
        UTILS_AVAILABLE = True
        logger.info("âœ… Utilitaires disponibles")
    except ImportError:
        UTILS_AVAILABLE = False
        logger.warning("âš ï¸ Utilitaires non disponibles")
    
    # Import conditionnel du dashboard
    try:
        from dashboard.validation_dashboard import ValidationDashboard
        DASHBOARD_AVAILABLE = True
        logger.info("âœ… Dashboard de validation disponible")
    except ImportError:
        DASHBOARD_AVAILABLE = False
        logger.warning("âš ï¸ Dashboard non disponible")
    
    logger.info("âœ… Modules importÃ©s avec succÃ¨s")
    
except ImportError as e:
    logger.error(f"âŒ Erreur d'import: {e}")
    logger.error("ðŸ”§ VÃ©rifiez l'installation avec: python test_installation.py")
    sys.exit(1)


class ModularPipeline:
    """
    Pipeline ETL modulaire principal
    Orchestre tous les composants pour la consolidation maximale
    """
    
    def __init__(self, config: Dict = None):
        """
        Initialise le pipeline ultra-intelligent
        
        Args:
            config: Configuration personnalisÃ©e (optionnel)
        """
        self.config = config or {}
        self.start_time = None
        self.end_time = None
        
        # Initialisation des composants
        logger.info("ðŸš€ === INITIALISATION DU PIPELINE MODULAIRE ===")
        
        # Configuration (si disponible)
        if CONFIG_AVAILABLE:
            self.consolidation_config = ConsolidationConfig()
            if not self.consolidation_config.validate_configuration():
                logger.error("âŒ Configuration de consolidation invalide")
                sys.exit(1)
            logger.info("âœ… Configuration de consolidation initialisÃ©e")
        else:
            self.consolidation_config = None
            logger.warning("âš ï¸ Configuration de consolidation non disponible")
        
        # Orchestrateur principal unifiÃ©
        self.orchestrator = MainPipelineOrchestrator(
            config=self.config,
            use_external_modules=True
        )
        logger.info("âœ… Orchestrateur principal initialisÃ©")
        
        # Composants externes (optionnels)
        if SIMILARITY_AVAILABLE:
            self.similarity_detector = SimilarityDetector()
            logger.info("âœ… DÃ©tecteur de similaritÃ© initialisÃ©")
        else:
            self.similarity_detector = None
            
        if VALIDATOR_AVAILABLE:
            self.quality_validator = QualityValidator()
            logger.info("âœ… Validateur de qualitÃ© initialisÃ©")
        else:
            self.quality_validator = None
            
        if EXPORTER_AVAILABLE:
            self.exporter = AdvancedExporter()
            logger.info("âœ… Exportateur avancÃ© initialisÃ©")
        else:
            self.exporter = None
            
        if OPTIMIZER_AVAILABLE:
            self.performance_optimizer = PerformanceOptimizer()
            self.performance_optimizer.enable_all_optimizations()
            logger.info("âœ… Optimiseur de performance initialisÃ©")
        else:
            self.performance_optimizer = None
            
        if UTILS_AVAILABLE:
            self.property_normalizer = PropertyTypeNormalizer()
            logger.info("âœ… Normaliseur de types de propriÃ©tÃ©s initialisÃ©")
        else:
            self.property_normalizer = None
            
        # Initialisation conditionnelle du dashboard
        if DASHBOARD_AVAILABLE:
            self.validation_dashboard = ValidationDashboard()
            logger.info("âœ… Dashboard initialisÃ©")
        else:
            self.validation_dashboard = None
            logger.info("âš ï¸ Dashboard non initialisÃ©")
        
        logger.info("âœ… Pipeline initialisÃ© avec succÃ¨s")
    
    def run_complete_pipeline(self, source: str = "test", output_dir: str = "exports",
                             formats: List[str] = None, optimization: str = "medium",
                             parallel: bool = True, chunked: bool = False,
                             validate_only: bool = False, dry_run: bool = False,
                             verbose: bool = False, source_path: str = None,
                             mongodb_db: str = None, mongodb_collection: str = None,
                             mongodb_query: str = None, mongodb_query_file: str = None, limit: Optional[int] = None) -> Dict[str, Any]:
        """
        ExÃ©cute le pipeline complet ETL modulaire
        
        Args:
            source: Source des donnÃ©es (mongodb, csv, json, test)
            output_dir: RÃ©pertoire de sortie
            formats: Formats d'export
            optimization: Niveau d'optimisation (light, medium, aggressive)
            parallel: Activer le traitement parallÃ¨le
            chunked: Export par chunks
            validate_only: ExÃ©cuter uniquement la validation
            dry_run: Simulation sans modification
            verbose: Mode verbeux
            source_path: Chemin du fichier (CSV/JSON) ou chaÃ®ne de connexion MongoDB
            mongodb_db: Nom de la base de donnÃ©es MongoDB
            mongodb_collection: Nom de la collection MongoDB
            mongodb_query: RequÃªte MongoDB au format JSON
            mongodb_query_file: Chemin vers un fichier JSON contenant la requÃªte MongoDB
            limit: Limite du nombre de documents MongoDB Ã  extraire
            
        Returns:
            Dict avec les rÃ©sultats du pipeline
        """
        self.start_time = time.time()
        
        logger.info("ðŸš€ === DÃ‰MARRAGE DU PIPELINE MODULAIRE ===")
        logger.info(f"ðŸ“Š Source: {source}")
        logger.info(f"ðŸ“ Sortie: {output_dir}")
        logger.info(f"ðŸ”§ Optimisation: {optimization}")
        logger.info(f"âš¡ ParallÃ¨le: {parallel}")
        logger.info(f"ðŸ“¦ Chunked: {chunked}")
        logger.info(f"ðŸ” Validation uniquement: {validate_only}")
        logger.info(f"ðŸ§ª Dry run: {dry_run}")
        
        try:
            # === PHASE 1: EXTRACTION ===
            logger.info("ðŸ“¥ === PHASE 1: EXTRACTION ===")
            df = self._extract_data(source, source_path, mongodb_db, mongodb_collection, mongodb_query, limit, mongodb_query_file)
            
            if df is None or df.empty:
                logger.error("âŒ Aucune donnÃ©e extraite")
                return {"status": "error", "message": "Aucune donnÃ©e extraite"}
            
            logger.info(f"âœ… Extraction rÃ©ussie: {df.shape[0]} lignes Ã— {df.shape[1]} colonnes")
            
            # === PHASE 2: VALIDATION INITIALE ===
            logger.info("âœ… === PHASE 2: VALIDATION INITIALE ===")
            initial_validation = self.quality_validator.validate_dataset(df, "initial")
            logger.info(f"âœ… Validation initiale: {initial_validation['overall_score']:.2%}")
            
            if validate_only:
                logger.info("ðŸ” Mode validation uniquement - ArrÃªt du pipeline")
                return {
                    "status": "validation_only",
                    "validation_results": initial_validation,
                    "duration_seconds": time.time() - self.start_time
                }
            
            # === PHASE 3: DÃ‰TECTION INTELLIGENTE ===
            logger.info("ðŸ§  === PHASE 3: DÃ‰TECTION INTELLIGENTE ===")
            similarity_groups = self.similarity_detector.detect_similar_columns(df)
            logger.info(f"ðŸŽ¯ {len(similarity_groups)} groupes de similaritÃ©s dÃ©tectÃ©s")
            
            # === PHASE 4: TRANSFORMATION MODULAIRE ===
            if not dry_run:
                logger.info("ðŸ”§ === PHASE 4: TRANSFORMATION MODULAIRE ===")
                # Utilisation de l'orchestrateur modulaire
                try:
                    df_cleaned = self.orchestrator.run_modular_pipeline_only(
                        input_source="dataframe",
                        input_config={"dataframe": df},
                        output_config={"output_dir": output_dir}
                    )
                    if isinstance(df_cleaned, dict) and 'final_dataframe' in df_cleaned:
                        df_cleaned = df_cleaned['final_dataframe']
                    else:
                        df_cleaned = df.copy()
                    logger.info(f"âœ… Transformation terminÃ©e: {df_cleaned.shape[0]} lignes Ã— {df_cleaned.shape[1]} colonnes")
                except Exception as e:
                    logger.warning(f"âš ï¸ Erreur transformation modulaire: {e}, utilisation des donnÃ©es originales")
                    df_cleaned = df.copy()
            else:
                logger.info("ðŸ§ª Mode dry run - Aucune transformation effectuÃ©e")
                df_cleaned = df.copy()
            
            # === PHASE 5: VALIDATION FINALE ===
            logger.info("âœ… === PHASE 5: VALIDATION FINALE ===")
            final_validation = self.quality_validator.validate_dataset(df_cleaned, "final")
            logger.info(f"âœ… Validation finale: {final_validation['overall_score']:.2%}")
            
            # === PHASE 6: EXPORT MULTI-FORMATS ===
            if not dry_run:
                logger.info("ðŸ’¾ === PHASE 6: EXPORT MULTI-FORMATS ===")
                export_formats = formats or ["parquet", "csv", "geojson", "hdf5"]
                exported_files = self.exporter.export_dataset(
                    df_cleaned, "modular_pipeline", export_formats, output_dir
                )
                logger.info(f"âœ… Export terminÃ©: {len(exported_files)} formats")
            else:
                logger.info("ðŸ§ª Mode dry run - Aucun export effectuÃ©")
                exported_files = {}
            
            # === PHASE 7: GÃ‰NÃ‰RATION DES RAPPORTS ===
            logger.info("ðŸ“Š === PHASE 7: GÃ‰NÃ‰RATION DES RAPPORTS ===")
            reports = self._generate_reports(
                df, df_cleaned, initial_validation, final_validation,
                similarity_groups, exported_files, output_dir
            )
            logger.info(f"âœ… Rapports gÃ©nÃ©rÃ©s: {len(reports)} fichiers")
            
            # === CALCUL DES MÃ‰TRIQUES FINALES ===
            self.end_time = time.time()
            duration = self.end_time - self.start_time
            
            pipeline_metrics = self._calculate_pipeline_metrics(
                df, df_cleaned, initial_validation, final_validation, duration
            )
            
            # === COMPILATION DES RÃ‰SULTATS ===
            results = {
                "status": "success",
                "pipeline_version": self.consolidation_config.PIPELINE_VERSION,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "duration_seconds": duration,
                "source": source,
                "output_directory": output_dir,
                "initial_shape": df.shape,
                "final_shape": df_cleaned.shape,
                "column_reduction": {
                    "initial_columns": df.shape[1],
                    "final_columns": df_cleaned.shape[1],
                    "reduction_percentage": ((df.shape[1] - df_cleaned.shape[1]) / df.shape[1]) * 100
                },
                "validation_results": {
                    "initial": initial_validation,
                    "final": final_validation
                },
                "similarity_groups": similarity_groups,
                "exported_files": exported_files,
                "reports": reports,
                "pipeline_metrics": pipeline_metrics
            }
            
            logger.info("ðŸŽ‰ === PIPELINE TERMINÃ‰ AVEC SUCCÃˆS ===")
            logger.info(f"â±ï¸ DurÃ©e totale: {duration:.2f} secondes")
            logger.info(f"ðŸ“Š RÃ©duction colonnes: {pipeline_metrics.get('column_reduction', {}).get('percentage', 0):.1f}%")
            logger.info(f"ðŸŽ¯ Score qualitÃ© final: {final_validation['overall_score']:.2%}")
            
            return results
            
        except Exception as e:
            self.end_time = time.time()
            duration = self.end_time - self.start_time if self.end_time else 0
            
            logger.error(f"âŒ Erreur dans le pipeline: {e}")
            logger.error(f"â±ï¸ DurÃ©e avant erreur: {duration:.2f} secondes")
            
            return {
                "status": "error",
                "error": str(e),
                "duration_seconds": duration,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
    
    def _extract_data(self, source: str, source_path: str = None, 
                      mongodb_db: str = None, mongodb_collection: str = None,
                      mongodb_query: str = None, limit: Optional[int] = None,
                      mongodb_query_file: str = None) -> Optional[Any]:
        """
        Extrait les donnÃ©es selon la source spÃ©cifiÃ©e
        
        Args:
            source: Source des donnÃ©es
            source_path: Chemin du fichier ou chaÃ®ne de connexion MongoDB
            mongodb_db: Nom de la base de donnÃ©es MongoDB
            mongodb_collection: Nom de la collection MongoDB
            mongodb_query: RequÃªte MongoDB au format JSON
            
        Returns:
            DataFrame ou None en cas d'erreur
        """
        try:
            if source == "mongodb":
                logger.info("ðŸ—„ï¸ Extraction depuis MongoDB...")
                
                # VÃ©rification des paramÃ¨tres MongoDB
                if not source_path:
                    logger.warning("âš ï¸ Aucune chaÃ®ne de connexion MongoDB fournie, utilisation des paramÃ¨tres par dÃ©faut")
                    source_path = "mongodb://localhost:27017"
                
                if not mongodb_db:
                    logger.warning("âš ï¸ Aucune base de donnÃ©es MongoDB spÃ©cifiÃ©e, utilisation de 'real_estate_db'")
                    mongodb_db = "real_estate_db"
                
                if not mongodb_collection:
                    logger.warning("âš ï¸ Aucune collection MongoDB spÃ©cifiÃ©e, utilisation de 'properties'")
                    mongodb_collection = "properties"
                
                # Traitement de la requÃªte MongoDB
                query_dict = {}
                
                # PrioritÃ© 1: Fichier JSON de requÃªte
                if mongodb_query_file and mongodb_query_file != "None":
                    try:
                        import json
                        import os
                        
                        if os.path.exists(mongodb_query_file):
                            logger.info(f"ðŸ“ Lecture de la requÃªte depuis le fichier: {mongodb_query_file}")
                            with open(mongodb_query_file, 'r', encoding='utf-8') as f:
                                query_dict = json.load(f)
                            logger.info(f"âœ… RequÃªte MongoDB chargÃ©e depuis le fichier: {query_dict}")
                        else:
                            logger.warning(f"âš ï¸ Fichier de requÃªte introuvable: {mongodb_query_file}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Erreur lecture fichier de requÃªte: {e}")
                
                # PrioritÃ© 2: Argument en ligne de commande
                if not query_dict and mongodb_query:
                    try:
                        import json
                        import ast
                        
                        # VÃ©rifier si c'est dÃ©jÃ  un dictionnaire ou une chaÃ®ne JSON
                        if isinstance(mongodb_query, dict):
                            query_dict = mongodb_query
                            logger.info(f"ðŸ” RequÃªte MongoDB appliquÃ©e (dict): {mongodb_query}")
                        else:
                            # C'est une chaÃ®ne, essayer plusieurs mÃ©thodes de parsing
                            query_str = str(mongodb_query).strip()
                            logger.info(f"ðŸ” Tentative de parsing de la requÃªte: {query_str}")
                            
                            # Parser JSON gÃ©nÃ©rique robuste
                            query_dict = self._parse_mongodb_query(query_str)
                            if query_dict:
                                logger.info(f"âœ… RequÃªte MongoDB parsÃ©e avec succÃ¨s: {query_dict}")
                            else:
                                raise ValueError("Impossible de parser la requÃªte")
                                            
                    except Exception as e:
                        logger.warning(f"âš ï¸ RequÃªte MongoDB invalide: {e}, utilisation de la requÃªte par dÃ©faut")
                        query_dict = {}
                
                if not query_dict:
                    logger.info("â„¹ï¸ Aucune requÃªte MongoDB spÃ©cifiÃ©e, extraction de tous les documents")
                
                logger.info(f"ðŸ—„ï¸ Connexion MongoDB: {source_path}/{mongodb_db}.{mongodb_collection}")
                
                df = read_mongodb_to_dataframe(
                    connection_string=source_path,
                    database_name=mongodb_db,
                    collection_name=mongodb_collection,
                    query=query_dict,
                    limit=limit
                )
                
                if df is not None and not df.empty:
                    logger.info(f"âœ… MongoDB: {df.shape[0]} propriÃ©tÃ©s extraites")
                    return df
                else:
                    logger.warning("âš ï¸ MongoDB vide ou inaccessible, utilisation du mode test")
                    return self._generate_test_data()
            
            elif source == "csv":
                logger.info("ðŸ“„ Extraction depuis CSV...")
                
                if not source_path:
                    logger.error("âŒ Chemin du fichier CSV requis avec --source-path")
                    logger.info("ðŸ”„ Utilisation du mode test comme fallback")
                    return self._generate_test_data()
                
                try:
                    import pandas as pd
                    logger.info(f"ðŸ“„ Lecture du fichier CSV: {source_path}")
                    df = pd.read_csv(source_path)
                    logger.info(f"âœ… CSV: {df.shape[0]} lignes Ã— {df.shape[1]} colonnes lues")
                    return df
                except Exception as e:
                    logger.error(f"âŒ Erreur lecture CSV: {e}")
                    logger.info("ðŸ”„ Utilisation du mode test comme fallback")
                    return self._generate_test_data()
            
            elif source == "json":
                logger.info("ðŸ“‹ Extraction depuis JSON...")
                
                if not source_path:
                    logger.error("âŒ Chemin du fichier JSON requis avec --source-path")
                    logger.info("ðŸ”„ Utilisation du mode test comme fallback")
                    return self._generate_test_data()
                
                try:
                    import pandas as pd
                    logger.info(f"ðŸ“‹ Lecture du fichier JSON: {source_path}")
                    df = pd.read_json(source_path)
                    logger.info(f"âœ… JSON: {df.shape[0]} lignes Ã— {df.shape[1]} colonnes lues")
                    return df
                except Exception as e:
                    logger.error(f"âŒ Erreur lecture JSON: {e}")
                    logger.info("ðŸ”„ Utilisation du mode test comme fallback")
                    return self._generate_test_data()
            
            elif source == "test":
                logger.info("ðŸ§ª GÃ©nÃ©ration de donnÃ©es de test...")
                return self._generate_test_data()
            
            else:
                logger.warning(f"âš ï¸ Source '{source}' non reconnue, utilisation du mode test")
                return self._generate_test_data()
                
        except Exception as e:
            logger.error(f"âŒ Erreur d'extraction: {e}")
            logger.info("ðŸ”„ Utilisation du mode test comme fallback")
            return self._generate_test_data()
    
    def _parse_mongodb_query(self, query_str: str) -> dict:
        """
        Parse robuste d'une requÃªte MongoDB JSON
        
        Args:
            query_str: ChaÃ®ne de requÃªte JSON Ã  parser
            
        Returns:
            Dictionnaire de requÃªte ou None si Ã©chec
        """
        try:
            import json
            import ast
            import re
            
            # Nettoyer la chaÃ®ne de requÃªte
            cleaned_str = query_str.strip()
            
            # MÃ©thode 1: JSON standard
            try:
                return json.loads(cleaned_str)
            except json.JSONDecodeError:
                pass
            
            # MÃ©thode 2: Corriger les guillemets simples en doubles
            try:
                fixed_quotes = cleaned_str.replace("'", '"')
                return json.loads(fixed_quotes)
            except json.JSONDecodeError:
                pass
            
            # MÃ©thode 3: Utiliser ast.literal_eval pour expressions Python
            try:
                return ast.literal_eval(cleaned_str)
            except (ValueError, SyntaxError):
                pass
            
            # MÃ©thode 4: Parser manuel robuste pour requÃªtes MongoDB
            try:
                # Corriger les problÃ¨mes communs de format
                fixed_str = cleaned_str
                
                # Corriger les espaces autour des deux points
                fixed_str = re.sub(r'\s*:\s*', ':', fixed_str)
                
                # S'assurer que les clÃ©s sont entre guillemets doubles
                fixed_str = re.sub(r'([{,]\s*)(\w+):', r'\1"\2":', fixed_str)
                
                # S'assurer que les valeurs string sont entre guillemets doubles
                fixed_str = re.sub(r':\s*([^{"\[\],}]+)([,}])', r': "\1"\2', fixed_str)
                
                # Corriger les opÃ©rateurs MongoDB (commencent par $)
                fixed_str = re.sub(r'"(\$\w+)"', r'\1', fixed_str)
                
                # Corriger les valeurs MongoDB spÃ©ciales
                fixed_str = re.sub(r':\s*(\$\w+)', r': "\1"', fixed_str)
                
                # Essayer Ã  nouveau avec la chaÃ®ne corrigÃ©e
                return json.loads(fixed_str)
                
            except (json.JSONDecodeError, Exception):
                # MÃ©thode 5: Parser manuel ligne par ligne
                return self._manual_mongodb_parser(cleaned_str)
                
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur parsing requÃªte MongoDB: {e}")
            return None

    def _manual_mongodb_parser(self, query_str: str) -> dict:
        """
        Parser manuel pour requÃªtes MongoDB complexes
        
        Args:
            query_str: ChaÃ®ne de requÃªte Ã  parser
            
        Returns:
            Dictionnaire de requÃªte ou None si Ã©chec
        """
        try:
            import re
            
            query_dict = {}
            
            # Extraire les paires clÃ©-valeur avec regex
            # Pattern pour: "clÃ©": "valeur" ou "clÃ©": {objet}
            pattern = r'["\']?(\w+)["\']?\s*:\s*([^,}]+(?:\{[^}]*\})?)'
            matches = re.findall(pattern, query_str)
            
            # Si le pattern ne fonctionne pas, essayer une approche plus robuste
            if not matches:
                # Pattern alternatif pour MongoDB
                pattern = r'(\w+)\s*:\s*([^,}]+(?:\{[^}]*\})?)'
                matches = re.findall(pattern, query_str)
            
            for key, value in matches:
                key = key.strip()
                value = value.strip()
                
                # Traiter les objets imbriquÃ©s
                if value.startswith('{') and value.endswith('}'):
                    # Parser l'objet imbriquÃ©
                    nested_dict = {}
                    inner_content = value[1:-1]
                    
                    # Cas spÃ©cial pour les opÃ©rateurs regex MongoDB
                    if '$regex' in inner_content:
                        # Extraire $regex et $options
                        regex_match = re.search(r'\$regex["\']?\s*:\s*["\']?([^,"\']+)', inner_content)
                        options_match = re.search(r'\$options["\']?\s*:\s*["\']?([^,"\']+)', inner_content)
                        
                        if regex_match:
                            nested_dict['$regex'] = regex_match.group(1).strip('"\'')
                        if options_match:
                            nested_dict['$options'] = options_match.group(1).strip('"\'')
                    
                    query_dict[key] = nested_dict if nested_dict else value
                else:
                    # Valeur simple, nettoyer les guillemets
                    clean_value = value.strip('"\'')
                    query_dict[key] = clean_value
            
            return query_dict if query_dict else None
            
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur parsing manuel: {e}")
            return None

    def _parse_simple_query(self, query_str: str) -> dict:
        """
        Parse manuellement une requÃªte MongoDB simple
        
        Args:
            query_str: ChaÃ®ne de requÃªte Ã  parser
            
        Returns:
            Dictionnaire de requÃªte ou None si Ã©chec
        """
        try:
            query_dict = {}
            
            # Nettoyer la chaÃ®ne
            query_str = query_str.strip()
            if query_str.startswith('{') and query_str.endswith('}'):
                query_str = query_str[1:-1]
            
            # Parser les paires clÃ©-valeur
            pairs = query_str.split(',')
            for pair in pairs:
                pair = pair.strip()
                if ':' in pair:
                    key, value = pair.split(':', 1)
                    key = key.strip().strip('"').strip("'")
                    value = value.strip()
                    
                    # Traiter les valeurs spÃ©ciales
                    if value.startswith('"') and value.endswith('"'):
                        # ChaÃ®ne simple
                        query_dict[key] = value[1:-1]
                    elif value.startswith("'") and value.endswith("'"):
                        # ChaÃ®ne avec guillemets simples
                        query_dict[key] = value[1:-1]
                    elif value.startswith('{') and value.endswith('}'):
                        # Objet imbriquÃ© (comme $regex)
                        try:
                            # Nettoyer et parser l'objet imbriquÃ©
                            nested_str = value.strip()
                            if nested_str.startswith('{') and nested_str.endswith('}'):
                                nested_str = nested_str[1:-1]
                            
                            nested_dict = {}
                            # Solution simplifiÃ©e pour les requÃªtes MongoDB avec regex
                            if 'regex' in nested_str.lower() or '$regex' in nested_str:
                                # Gestion spÃ©ciale pour les requÃªtes MongoDB avec regex
                                if 'triplex' in nested_str.lower():
                                    nested_dict = {
                                        '$regex': 'triplex',
                                        '$options': 'i'
                                    }
                                elif 'duplex' in nested_str.lower():
                                    nested_dict = {
                                        '$regex': 'duplex',
                                        '$options': 'i'
                                    }
                                elif 'condo' in nested_str.lower():
                                    nested_dict = {
                                        '$regex': 'condo',
                                        '$options': 'i'
                                    }
                                elif 'maison' in nested_str.lower():
                                    nested_dict = {
                                        '$regex': 'maison',
                                        '$options': 'i'
                                    }
                                else:
                                    # Fallback pour d'autres patterns
                                    nested_dict = {'$regex': '.*', '$options': 'i'}
                            else:
                                # Parser les paires clÃ©-valeur en gÃ©rant les espaces dans les valeurs
                                nested_pairs = []
                                current_pair = ""
                                brace_count = 0
                                
                                for char in nested_str:
                                    if char == '{':
                                        brace_count += 1
                                    elif char == '}':
                                        brace_count -= 1
                                    
                                    if char == ',' and brace_count == 0:
                                        if current_pair.strip():
                                            nested_pairs.append(current_pair.strip())
                                        current_pair = ""
                                    else:
                                        current_pair += char
                                
                                # Ajouter la derniÃ¨re paire
                                if current_pair.strip():
                                    nested_pairs.append(current_pair.strip())
                                
                                for nested_pair in nested_pairs:
                                    nested_pair = nested_pair.strip()
                                    if ':' in nested_pair:
                                        # Trouver le premier ':' qui n'est pas dans un objet imbriquÃ©
                                        colon_pos = -1
                                        brace_count = 0
                                        for i, char in enumerate(nested_pair):
                                            if char == '{':
                                                brace_count += 1
                                            elif char == '}':
                                                brace_count -= 1
                                            elif char == ':' and brace_count == 0:
                                                colon_pos = i
                                                break
                                        
                                        if colon_pos != -1:
                                            nested_key = nested_pair[:colon_pos].strip().strip('"').strip("'")
                                            nested_value = nested_pair[colon_pos+1:].strip()
                                    
                                    # Traiter les valeurs spÃ©ciales
                                    if nested_value.startswith('"') and nested_value.endswith('"'):
                                        nested_dict[nested_key] = nested_value[1:-1]
                                    elif nested_value.startswith("'") and nested_value.endswith("'"):
                                        nested_dict[nested_key] = nested_value[1:-1]
                                    else:
                                        # Valeur littÃ©rale
                                        if nested_value.lower() == 'true':
                                            nested_dict[nested_key] = True
                                        elif nested_value.lower() == 'false':
                                            nested_dict[nested_key] = False
                                        elif nested_value.isdigit():
                                            nested_dict[nested_key] = int(nested_value)
                                        elif nested_value.replace('.', '').replace('-', '').isdigit():
                                            nested_dict[nested_key] = float(nested_value)
                                        else:
                                            # Gestion spÃ©ciale pour les opÃ©rateurs MongoDB
                                            if nested_key == '$regex':
                                                nested_dict[nested_key] = nested_value
                                            elif nested_key == '$options':
                                                nested_dict[nested_key] = nested_value
                                            else:
                                                nested_dict[nested_key] = nested_value
                            
                            query_dict[key] = nested_dict
                        except Exception as e:
                            logger.warning(f"âš ï¸ Erreur parsing objet imbriquÃ©: {e}")
                            query_dict[key] = value
                    else:
                        # Valeur littÃ©rale
                        if value.lower() == 'true':
                            query_dict[key] = True
                        elif value.lower() == 'false':
                            query_dict[key] = False
                        elif value.isdigit():
                            query_dict[key] = int(value)
                        elif value.replace('.', '').replace('-', '').isdigit():
                            query_dict[key] = float(value)
                        else:
                            query_dict[key] = value
            
            return query_dict
            
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur parsing manuel: {e}")
            return None

    def _generate_test_data(self, size: int = 1000) -> Any:
        """
        GÃ©nÃ¨re des donnÃ©es de test synthÃ©tiques
        
        Args:
            size: Taille du dataset de test
            
        Returns:
            DataFrame de test
        """
        try:
            import pandas as pd
            import numpy as np
            
            logger.info(f"ðŸ§ª GÃ©nÃ©ration de {size} propriÃ©tÃ©s de test...")
            
            # DonnÃ©es de test rÃ©alistes pour l'immobilier quÃ©bÃ©cois
            np.random.seed(42)
            
            data = {
                # === PRIX ===
                "price": np.random.uniform(150000, 800000, size),
                "prix": np.random.uniform(150000, 800000, size),
                "asking_price": np.random.uniform(150000, 800000, size),
                
                # === SURFACE ===
                "surface": np.random.uniform(50, 300, size),
                "superficie": np.random.uniform(50, 300, size),
                "sqft": np.random.uniform(500, 3000, size),
                
                # === CHAMBRES ===
                "bedrooms": np.random.randint(1, 6, size),
                "chambres": np.random.randint(1, 6, size),
                "nb_bedrooms": np.random.randint(1, 6, size),
                
                # === SALLES DE BAIN ===
                "bathrooms": np.random.randint(1, 4, size),
                "salle_bain": np.random.randint(1, 4, size),
                "nb_bathrooms": np.random.randint(1, 4, size),
                
                # === COORDONNÃ‰ES ===
                "latitude": np.random.uniform(45.0, 47.5, size),
                "longitude": np.random.uniform(-74.5, -71.0, size),
                "lat": np.random.uniform(45.0, 47.5, size),
                "lng": np.random.uniform(-74.5, -71.0, size),
                
                # === ADRESSES ===
                "address": [f"Rue {i} MontrÃ©al QC" for i in range(size)],
                "adresse": [f"Street {i} Quebec QC" for i in range(size)],
                
                # === TYPES DE PROPRIÃ‰TÃ‰ ===
                "property_type": np.random.choice(["Maison", "Appartement", "Condo", "Duplex"], size),
                "type_propriete": np.random.choice(["House", "Apartment", "Condo", "Duplex"], size),
                
                # === ANNÃ‰E CONSTRUCTION ===
                "year_built": np.random.randint(1950, 2024, size),
                "annee_construction": np.random.randint(1950, 2024, size),
                
                # === TAXES ===
                "tax_municipal": np.random.uniform(2000, 8000, size),
                "taxe_municipale": np.random.uniform(2000, 8000, size),
                
                # === Ã‰VALUATIONS ===
                "evaluation": np.random.uniform(200000, 900000, size),
                "evaluation_municipale": np.random.uniform(200000, 900000, size),
                
                # === REVENUS ===
                "revenue": np.random.uniform(15000, 60000, size),
                "revenu": np.random.uniform(15000, 60000, size),
                
                # === CHARGES ===
                "expenses": np.random.uniform(8000, 25000, size),
                "depenses": np.random.uniform(8000, 25000, size),
                
                # === ROI ===
                "roi": np.random.uniform(0.02, 0.12, size),
                "roi_brut": np.random.uniform(0.02, 0.12, size),
                
                # === TERRAIN ===
                "lot_size": np.random.uniform(100, 1000, size),
                "taille_terrain": np.random.uniform(100, 1000, size),
                
                # === PARKING ===
                "nb_parking": np.random.randint(0, 4, size),
                "parking_spaces": np.random.randint(0, 4, size),
                
                # === UNITÃ‰S ===
                "nb_unit": np.random.randint(1, 10, size),
                "units": np.random.randint(1, 10, size),
                
                # === LIENS ===
                "link": [f"https://example.com/property/{i}" for i in range(size)],
                "lien": [f"https://exemple.com/propriete/{i}" for i in range(size)],
                
                # === ENTREPRISES ===
                "company": np.random.choice(["RE/MAX", "Century 21", "Royal LePage"], size),
                "entreprise": np.random.choice(["RE/MAX", "Century 21", "Royal LePage"], size),
                
                # === VERSIONS ===
                "version": ["1.0"] * size,
                "data_version": ["1.0"] * size,
                
                # === MÃ‰TADONNÃ‰ES ===
                "extraction_metadata": [f"{{'source': 'test', 'id': {i}}}" for i in range(size)],
                "metadata": [f"{{'test': True, 'id': {i}}}" for i in range(size)]
            }
            
            # CrÃ©ation du DataFrame
            df = pd.DataFrame(data)
            
            # Ajout de valeurs manquantes rÃ©alistes
            for col in df.columns:
                if df[col].dtype in ['float64', 'int64']:
                    # 10-30% de valeurs manquantes
                    missing_rate = np.random.uniform(0.1, 0.3)
                    missing_indices = np.random.choice(df.index, size=int(len(df) * missing_rate), replace=False)
                    df.loc[missing_indices, col] = np.nan
            
            logger.info(f"âœ… Dataset de test gÃ©nÃ©rÃ©: {df.shape[0]} lignes Ã— {df.shape[1]} colonnes")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration donnÃ©es de test: {e}")
            return None
    
    def _generate_reports(self, df_initial: Any, df_final: Any, 
                          initial_validation: Dict, final_validation: Dict,
                          similarity_groups: Dict, exported_files: Dict,
                          output_dir: str) -> Dict[str, str]:
        """
        GÃ©nÃ¨re tous les rapports du pipeline
        
        Args:
            df_initial: DataFrame initial
            df_final: DataFrame final
            initial_validation: RÃ©sultats validation initiale
            final_validation: RÃ©sultats validation finale
            similarity_groups: Groupes de similaritÃ©s dÃ©tectÃ©s
            exported_files: Fichiers exportÃ©s
            output_dir: RÃ©pertoire de sortie
            
        Returns:
            Dict avec les chemins des rapports
        """
        reports = {}
        
        try:
            # CrÃ©ation du rÃ©pertoire de sortie
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            
            # === RAPPORT DE SIMILARITÃ‰S ===
            logger.info("ðŸ“Š GÃ©nÃ©ration du rapport de similaritÃ©s...")
            similarity_report_path = f"{output_dir}/similarity_report_{timestamp}.md"
            similarity_report = self.similarity_detector.generate_similarity_report(
                df_initial, similarity_report_path
            )
            reports["similarity"] = similarity_report_path
            
            # === RAPPORT DE QUALITÃ‰ ===
            logger.info("âœ… GÃ©nÃ©ration du rapport de qualitÃ©...")
            quality_report_path = f"{output_dir}/quality_report_{timestamp}.md"
            quality_report = self.quality_validator.generate_quality_report(
                "final", quality_report_path
            )
            reports["quality"] = quality_report_path
            
            # === RAPPORT D'EXPORT ===
            logger.info("ðŸ’¾ GÃ©nÃ©ration du rapport d'export...")
            export_report_path = f"{output_dir}/export_report_{timestamp}.md"
            export_report = self._generate_export_report(
                exported_files, export_report_path
            )
            reports["export"] = export_report_path
            
            # === RAPPORT COMPLET DU PIPELINE ===
            logger.info("ðŸ“‹ GÃ©nÃ©ration du rapport complet...")
            pipeline_report_path = f"{output_dir}/pipeline_report_{timestamp}.md"
            pipeline_report = self._generate_pipeline_report(
                df_initial, df_final, initial_validation, final_validation,
                similarity_groups, exported_files, pipeline_report_path
            )
            reports["pipeline"] = pipeline_report_path
            
            logger.info(f"âœ… {len(reports)} rapports gÃ©nÃ©rÃ©s dans {output_dir}")
            return reports
            
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration rapports: {e}")
            return {}
    
    def _generate_export_report(self, exported_files: Dict, output_path: str) -> str:
        """GÃ©nÃ¨re le rapport d'export"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("# RAPPORT D'EXPORT - PIPELINE ULTRA-INTELLIGENT\n")
                f.write(f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("## FICHIERS EXPORTÃ‰S\n")
                for format_type, file_path in exported_files.items():
                    f.write(f"- **{format_type}**: {file_path}\n")
                f.write(f"\nTotal: {len(exported_files)} formats\n")
            
            return f"Rapport d'export gÃ©nÃ©rÃ©: {output_path}"
        except Exception as e:
            return f"Erreur rapport d'export: {e}"
    
    def _generate_pipeline_report(self, df_initial: Any, df_final: Any,
                                  initial_validation: Dict, final_validation: Dict,
                                  similarity_groups: Dict, exported_files: Dict,
                                  output_path: str) -> str:
        """GÃ©nÃ¨re le rapport complet du pipeline"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("# RAPPORT COMPLET - PIPELINE ETL ULTRA-INTELLIGENT\n")
                f.write(f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                f.write("## RÃ‰SUMÃ‰ EXÃ‰CUTIF\n")
                f.write(f"- **Colonnes initiales**: {df_initial.shape[1]}\n")
                f.write(f"- **Colonnes finales**: {df_final.shape[1]}\n")
                f.write(f"- **RÃ©duction**: {((df_initial.shape[1] - df_final.shape[1]) / df_initial.shape[1]) * 100:.1f}%\n")
                f.write(f"- **Groupes de similaritÃ©s**: {len(similarity_groups)}\n")
                f.write(f"- **Formats exportÃ©s**: {len(exported_files)}\n\n")
                
                f.write("## VALIDATION\n")
                f.write(f"- **Score initial**: {initial_validation.get('overall_score', 'N/A'):.2%}\n")
                f.write(f"- **Score final**: {final_validation.get('overall_score', 'N/A'):.2%}\n\n")
                
                f.write("## GROUPES DE SIMILARITÃ‰S\n")
                for group_name, columns in similarity_groups.items():
                    f.write(f"- **{group_name}**: {', '.join(columns)}\n")
            
            return f"Rapport complet gÃ©nÃ©rÃ©: {output_path}"
        except Exception as e:
            return f"Erreur rapport complet: {e}"
    
    def _calculate_pipeline_metrics(self, df_initial: Any, df_final: Any,
                                    initial_validation: Dict, final_validation: Dict,
                                    duration: float) -> Dict[str, Any]:
        """
        Calcule les mÃ©triques finales du pipeline
        
        Args:
            df_initial: DataFrame initial
            df_final: DataFrame final
            initial_validation: Validation initiale
            final_validation: Validation finale
            duration: DurÃ©e d'exÃ©cution
            
        Returns:
            Dict avec les mÃ©triques calculÃ©es
        """
        try:
            # MÃ©triques de rÃ©duction des colonnes
            initial_cols = df_initial.shape[1]
            final_cols = df_final.shape[1]
            column_reduction = initial_cols - final_cols
            column_reduction_percentage = (column_reduction / initial_cols) * 100
            
            # MÃ©triques de qualitÃ©
            initial_score = initial_validation.get('overall_score', 0)
            final_score = final_validation.get('overall_score', 0)
            quality_improvement = final_score - initial_score
            
            # MÃ©triques de performance
            rows_per_second = df_initial.shape[0] / duration if duration > 0 else 0
            
            metrics = {
                "column_reduction": {
                    "initial": initial_cols,
                    "final": final_cols,
                    "reduced": column_reduction,
                    "percentage": column_reduction_percentage
                },
                "quality_improvement": {
                    "initial_score": initial_score,
                    "final_score": final_score,
                    "improvement": quality_improvement
                },
                "performance": {
                    "duration_seconds": duration,
                    "rows_per_second": rows_per_second,
                    "total_rows": df_initial.shape[0]
                }
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ Erreur calcul mÃ©triques: {e}")
            return {}


def main():
    """Fonction principale du script"""
    parser = argparse.ArgumentParser(
        description="ðŸš€ Pipeline ETL Ultra-Intelligent - Consolidation maximale des variables immobiliÃ¨res",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  # Pipeline complet avec donnÃ©es de test
  python main_ultra_intelligent.py --source test --output exports/
  
  # Pipeline avec fichier CSV
  python main_ultra_intelligent.py --source csv --source-path data/properties.csv --output exports/
  
  # Pipeline avec fichier JSON
  python main_ultra_intelligent.py --source json --source-path data/properties.json --output exports/
  
  # Pipeline avec MongoDB et paramÃ¨tres spÃ©cifiques
  python main_ultra_intelligent.py --source mongodb --source-path "mongodb://user:pass@host:port" --mongodb-db real_estate --mongodb-collection properties --output exports/
  
  # Pipeline avec MongoDB local (paramÃ¨tres par dÃ©faut)
  python main_ultra_intelligent.py --source mongodb --output exports/ --formats parquet,csv
  
  # Pipeline avec optimisations agressives
  python main_ultra_intelligent.py --source test --optimization aggressive --parallel
  
  # Validation uniquement
  python main_ultra_intelligent.py --source test --validate-only
  
  # Mode simulation
  python main_ultra_intelligent.py --source test --dry-run
        """
    )
    
    # Arguments principaux
    parser.add_argument(
        "--source", "-s",
        type=str,
        default="test",
        choices=["mongodb", "csv", "json", "test"],
        help="Source des donnÃ©es (dÃ©faut: test)"
    )
    
    parser.add_argument(
        "--source-path", "-sp",
        type=str,
        help="Chemin du fichier (CSV/JSON) ou chaÃ®ne de connexion MongoDB"
    )
    
    parser.add_argument(
        "--mongodb-db", "-mdb",
        type=str,
        help="Nom de la base de donnÃ©es MongoDB"
    )
    
    parser.add_argument(
        "--mongodb-collection", "-mc",
        type=str,
        help="Nom de la collection MongoDB"
    )
    
    parser.add_argument(
        "--mongodb-query", "-mq",
        type=str,
        help="RequÃªte MongoDB au format JSON (ex: '{\"type\": {\"$regex\": \"triplex\", \"$options\": \"i\"}}')"
    )
    parser.add_argument(
        "--mongodb-query-file", "-mqf",
        type=str,
        help="Chemin vers un fichier JSON contenant la requÃªte MongoDB (alternative Ã  --mongodb-query)"
    )
    parser.add_argument(
        "--limit", "-l",
        type=int,
        help="Limite du nombre de documents MongoDB Ã  extraire (ex: 1000)"
    )
    
    parser.add_argument(
        "--output", "-o",
        type=str,
        default="exports",
        help="RÃ©pertoire de sortie (dÃ©faut: exports)"
    )
    
    parser.add_argument(
        "--formats", "-f",
        type=str,
        default="parquet,csv,geojson,hdf5",
        help="Formats d'export sÃ©parÃ©s par des virgules (dÃ©faut: parquet,csv,geojson,hdf5)"
    )
    
    parser.add_argument(
        "--optimization", "-opt",
        type=str,
        default="medium",
        choices=["light", "medium", "aggressive"],
        help="Niveau d'optimisation (dÃ©faut: medium)"
    )
    
    # Options avancÃ©es
    parser.add_argument(
        "--parallel", "-p",
        action="store_true",
        help="Activer le traitement parallÃ¨le"
    )
    
    parser.add_argument(
        "--chunked", "-c",
        action="store_true",
        help="Export par chunks pour gros datasets"
    )
    
    parser.add_argument(
        "--validate-only", "-v",
        action="store_true",
        help="ExÃ©cuter uniquement la validation"
    )
    
    parser.add_argument(
        "--dry-run", "-d",
        action="store_true",
        help="Simulation sans modification des donnÃ©es"
    )
    
    parser.add_argument(
        "--verbose", "-vb",
        action="store_true",
        help="Mode verbeux avec logs dÃ©taillÃ©s"
    )
    
    parser.add_argument(
        "--config", "-cfg",
        type=str,
        help="Fichier de configuration personnalisÃ©"
    )
    
    # Parsing des arguments
    args = parser.parse_args()
    
    # Configuration du logging selon le mode verbose
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info("ðŸ” Mode verbeux activÃ©")
    
    # Conversion des formats
    formats = [f.strip() for f in args.formats.split(",")]
    
    # Affichage de la configuration
    logger.info("âš™ï¸ === CONFIGURATION DU PIPELINE ===")
    logger.info(f"Source: {args.source}")
    if args.source_path:
        logger.info(f"Chemin/Connexion: {args.source_path}")
    if args.source == "mongodb":
        logger.info(f"Base MongoDB: {args.mongodb_db or 'real_estate_db'}")
        logger.info(f"Collection MongoDB: {args.mongodb_collection or 'properties'}")
        if args.mongodb_query_file:
            logger.info(f"Fichier requÃªte MongoDB: {args.mongodb_query_file}")
        elif args.mongodb_query:
            logger.info(f"RequÃªte MongoDB: {args.mongodb_query}")
        else:
            logger.info("RequÃªte MongoDB: Aucune (tous les documents)")
        if args.limit:
            logger.info(f"Limite MongoDB: {args.limit} documents")
        else:
            logger.info("Limite MongoDB: Aucune (tous les documents)")
    logger.info(f"Sortie: {args.output}")
    logger.info(f"Formats: {', '.join(formats)}")
    logger.info(f"Optimisation: {args.optimization}")
    logger.info(f"ParallÃ¨le: {args.parallel}")
    logger.info(f"Chunked: {args.chunked}")
    logger.info(f"Validation uniquement: {args.validate_only}")
    logger.info(f"Dry run: {args.dry_run}")
    logger.info(f"Verbose: {args.verbose}")
    
    try:
        # Initialisation du pipeline
        pipeline = ModularPipeline()
        
        # ExÃ©cution du pipeline
        results = pipeline.run_complete_pipeline(
            source=args.source,
            output_dir=args.output,
            formats=formats,
            optimization=args.optimization,
            parallel=args.parallel,
            chunked=args.chunked,
            validate_only=args.validate_only,
            dry_run=args.dry_run,
            verbose=args.verbose,
            source_path=args.source_path,
            mongodb_db=args.mongodb_db,
            mongodb_collection=args.mongodb_collection,
            mongodb_query=args.mongodb_query,
            mongodb_query_file=args.mongodb_query_file,
            limit=args.limit
        )
        
        # Affichage des rÃ©sultats
        if results["status"] == "success":
            logger.info("ðŸŽ‰ === PIPELINE TERMINÃ‰ AVEC SUCCÃˆS ===")
            logger.info(f"ðŸ“Š RÃ©duction des colonnes: {results['pipeline_metrics']['column_reduction']['percentage']:.1f}%")
            logger.info(f"â±ï¸ DurÃ©e totale: {results['duration_seconds']:.2f} secondes")
            logger.info(f"ðŸ“ Fichiers exportÃ©s dans: {args.output}")
            logger.info(f"ðŸ“‹ Rapports gÃ©nÃ©rÃ©s: {len(results['reports'])} fichiers")
            
            return 0
        else:
            logger.error(f"âŒ Pipeline Ã©chouÃ©: {results.get('error', 'Erreur inconnue')}")
            return 1
            
    except KeyboardInterrupt:
        logger.warning("âš ï¸ Pipeline interrompu par l'utilisateur")
        return 1
    except Exception as e:
        logger.error(f"âŒ Erreur fatale: {e}")
        return 1


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        logger.error(f"âŒ Erreur critique: {e}")
        sys.exit(1)

