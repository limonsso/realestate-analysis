#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ PIPELINE ETL ULTRA-INTELLIGENT - SCRIPT PRINCIPAL
=====================================================

Script principal pour la consolidation maximale des variables immobili√®res
Bas√© sur les sp√©cifications du real_estate_prompt.md
"""

import argparse
import logging
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
import warnings

# Suppression des warnings pour un affichage plus propre
warnings.filterwarnings('ignore')

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('pipeline.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# Import des modules du pipeline
try:
    from config.consolidation_config import ConsolidationConfig
    from core.ultra_intelligent_cleaner import UltraIntelligentCleaner
    from intelligence.similarity_detector import SimilarityDetector
    from validation.quality_validator import QualityValidator
    from export.advanced_exporter import AdvancedExporter
    from performance.performance_optimizer import PerformanceOptimizer
    from utils.db import read_mongodb_to_dataframe, get_mongodb_stats
    from utils.property_type_normalizer import PropertyTypeNormalizer
    # Import conditionnel du dashboard
    try:
        from dashboard.validation_dashboard import ValidationDashboard
        DASHBOARD_AVAILABLE = True
        logger.info("‚úÖ Dashboard de validation disponible")
    except Exception as e:
        DASHBOARD_AVAILABLE = False
        logger.warning(f"‚ö†Ô∏è Dashboard non disponible: {e}")
    
    logger.info("‚úÖ Tous les modules import√©s avec succ√®s")
    
except ImportError as e:
    logger.error(f"‚ùå Erreur d'import: {e}")
    logger.error("üîß V√©rifiez l'installation avec: python test_installation.py")
    sys.exit(1)


class UltraIntelligentPipeline:
    """
    Pipeline ETL ultra-intelligent principal
    Orchestre tous les composants pour la consolidation maximale
    """
    
    def __init__(self, config: Dict = None):
        """
        Initialise le pipeline ultra-intelligent
        
        Args:
            config: Configuration personnalis√©e (optionnel)
        """
        self.config = config or {}
        self.start_time = None
        self.end_time = None
        
        # Initialisation des composants
        logger.info("üöÄ === INITIALISATION DU PIPELINE ULTRA-INTELLIGENT ===")
        
        # Configuration
        self.consolidation_config = ConsolidationConfig()
        if not self.consolidation_config.validate_configuration():
            logger.error("‚ùå Configuration de consolidation invalide")
            sys.exit(1)
        
        # Composants du pipeline
        self.cleaner = UltraIntelligentCleaner(self.consolidation_config)
        self.similarity_detector = SimilarityDetector()
        self.quality_validator = QualityValidator()
        self.exporter = AdvancedExporter()
        self.performance_optimizer = PerformanceOptimizer()
        self.property_normalizer = PropertyTypeNormalizer()
        # Initialisation conditionnelle du dashboard
        if DASHBOARD_AVAILABLE:
            self.validation_dashboard = ValidationDashboard()
            logger.info("‚úÖ Dashboard initialis√©")
        else:
            self.validation_dashboard = None
            logger.info("‚ö†Ô∏è Dashboard non initialis√©")
        
        # Activation des optimisations de performance
        self.performance_optimizer.enable_all_optimizations()
        
        logger.info("‚úÖ Pipeline initialis√© avec succ√®s")
    
    def run_complete_pipeline(self, source: str = "test", output_dir: str = "exports",
                             formats: List[str] = None, optimization: str = "medium",
                             parallel: bool = True, chunked: bool = False,
                             validate_only: bool = False, dry_run: bool = False,
                             verbose: bool = False, source_path: str = None,
                             mongodb_db: str = None, mongodb_collection: str = None,
                             mongodb_query: str = None, mongodb_query_file: str = None, limit: Optional[int] = None) -> Dict[str, Any]:
        """
        Ex√©cute le pipeline complet ETL ultra-intelligent
        
        Args:
            source: Source des donn√©es (mongodb, csv, json, test)
            output_dir: R√©pertoire de sortie
            formats: Formats d'export
            optimization: Niveau d'optimisation (light, medium, aggressive)
            parallel: Activer le traitement parall√®le
            chunked: Export par chunks
            validate_only: Ex√©cuter uniquement la validation
            dry_run: Simulation sans modification
            verbose: Mode verbeux
            source_path: Chemin du fichier (CSV/JSON) ou cha√Æne de connexion MongoDB
            mongodb_db: Nom de la base de donn√©es MongoDB
            mongodb_collection: Nom de la collection MongoDB
            mongodb_query: Requ√™te MongoDB au format JSON
            mongodb_query_file: Chemin vers un fichier JSON contenant la requ√™te MongoDB
            limit: Limite du nombre de documents MongoDB √† extraire
            
        Returns:
            Dict avec les r√©sultats du pipeline
        """
        self.start_time = time.time()
        
        logger.info("üöÄ === D√âMARRAGE DU PIPELINE COMPLET ===")
        logger.info(f"üìä Source: {source}")
        logger.info(f"üìÅ Sortie: {output_dir}")
        logger.info(f"üîß Optimisation: {optimization}")
        logger.info(f"‚ö° Parall√®le: {parallel}")
        logger.info(f"üì¶ Chunked: {chunked}")
        logger.info(f"üîç Validation uniquement: {validate_only}")
        logger.info(f"üß™ Dry run: {dry_run}")
        
        try:
            # === PHASE 1: EXTRACTION ===
            logger.info("üì• === PHASE 1: EXTRACTION ===")
            df = self._extract_data(source, source_path, mongodb_db, mongodb_collection, mongodb_query, limit, mongodb_query_file)
            
            if df is None or df.empty:
                logger.error("‚ùå Aucune donn√©e extraite")
                return {"status": "error", "message": "Aucune donn√©e extraite"}
            
            logger.info(f"‚úÖ Extraction r√©ussie: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
            
            # === PHASE 2: VALIDATION INITIALE ===
            logger.info("‚úÖ === PHASE 2: VALIDATION INITIALE ===")
            initial_validation = self.quality_validator.validate_dataset(df, "initial")
            logger.info(f"‚úÖ Validation initiale: {initial_validation['overall_score']:.2%}")
            
            if validate_only:
                logger.info("üîç Mode validation uniquement - Arr√™t du pipeline")
                return {
                    "status": "validation_only",
                    "validation_results": initial_validation,
                    "duration_seconds": time.time() - self.start_time
                }
            
            # === PHASE 3: D√âTECTION INTELLIGENTE ===
            logger.info("üß† === PHASE 3: D√âTECTION INTELLIGENTE ===")
            similarity_groups = self.similarity_detector.detect_similar_columns(df)
            logger.info(f"üéØ {len(similarity_groups)} groupes de similarit√©s d√©tect√©s")
            
            # === PHASE 4: TRANSFORMATION ULTRA-INTELLIGENTE ===
            if not dry_run:
                logger.info("üîß === PHASE 4: TRANSFORMATION ULTRA-INTELLIGENTE ===")
                # On utilise la m√©thode _transform_data du cleaner
                df_cleaned = self.cleaner._transform_data(df)
                logger.info(f"‚úÖ Transformation termin√©e: {df_cleaned.shape[0]} lignes √ó {df_cleaned.shape[1]} colonnes")
            else:
                logger.info("üß™ Mode dry run - Aucune transformation effectu√©e")
                df_cleaned = df.copy()
            
            # === PHASE 5: VALIDATION FINALE ===
            logger.info("‚úÖ === PHASE 5: VALIDATION FINALE ===")
            final_validation = self.quality_validator.validate_dataset(df_cleaned, "final")
            logger.info(f"‚úÖ Validation finale: {final_validation['overall_score']:.2%}")
            
            # === PHASE 6: EXPORT MULTI-FORMATS ===
            if not dry_run:
                logger.info("üíæ === PHASE 6: EXPORT MULTI-FORMATS ===")
                export_formats = formats or ["parquet", "csv", "geojson", "hdf5"]
                exported_files = self.exporter.export_dataset(
                    df_cleaned, "ultra_intelligent_pipeline", export_formats, output_dir
                )
                logger.info(f"‚úÖ Export termin√©: {len(exported_files)} formats")
            else:
                logger.info("üß™ Mode dry run - Aucun export effectu√©")
                exported_files = {}
            
            # === PHASE 7: G√âN√âRATION DES RAPPORTS ===
            logger.info("üìä === PHASE 7: G√âN√âRATION DES RAPPORTS ===")
            reports = self._generate_reports(
                df, df_cleaned, initial_validation, final_validation,
                similarity_groups, exported_files, output_dir
            )
            logger.info(f"‚úÖ Rapports g√©n√©r√©s: {len(reports)} fichiers")
            
            # === CALCUL DES M√âTRIQUES FINALES ===
            self.end_time = time.time()
            duration = self.end_time - self.start_time
            
            pipeline_metrics = self._calculate_pipeline_metrics(
                df, df_cleaned, initial_validation, final_validation, duration
            )
            
            # === COMPILATION DES R√âSULTATS ===
            results = {
                "status": "success",
                "pipeline_version": self.consolidation_config.PIPELINE_VERSION,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "duration_seconds": duration,
                "source": source,
                "output_directory": output_dir,
                "initial_shape": df.shape,
                "final_shape": df_cleaned.shape,
                "column_reduction": {
                    "initial_columns": df.shape[1],
                    "final_columns": df_cleaned.shape[1],
                    "reduction_percentage": ((df.shape[1] - df_cleaned.shape[1]) / df.shape[1]) * 100
                },
                "validation_results": {
                    "initial": initial_validation,
                    "final": final_validation
                },
                "similarity_groups": similarity_groups,
                "exported_files": exported_files,
                "reports": reports,
                "pipeline_metrics": pipeline_metrics
            }
            
            logger.info("üéâ === PIPELINE TERMIN√â AVEC SUCC√àS ===")
            logger.info(f"‚è±Ô∏è Dur√©e totale: {duration:.2f} secondes")
            logger.info(f"üìä R√©duction colonnes: {pipeline_metrics.get('column_reduction', {}).get('percentage', 0):.1f}%")
            logger.info(f"üéØ Score qualit√© final: {final_validation['overall_score']:.2%}")
            
            return results
            
        except Exception as e:
            self.end_time = time.time()
            duration = self.end_time - self.start_time if self.end_time else 0
            
            logger.error(f"‚ùå Erreur dans le pipeline: {e}")
            logger.error(f"‚è±Ô∏è Dur√©e avant erreur: {duration:.2f} secondes")
            
            return {
                "status": "error",
                "error": str(e),
                "duration_seconds": duration,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
    
    def _extract_data(self, source: str, source_path: str = None, 
                      mongodb_db: str = None, mongodb_collection: str = None,
                      mongodb_query: str = None, limit: Optional[int] = None,
                      mongodb_query_file: str = None) -> Optional[Any]:
        """
        Extrait les donn√©es selon la source sp√©cifi√©e
        
        Args:
            source: Source des donn√©es
            source_path: Chemin du fichier ou cha√Æne de connexion MongoDB
            mongodb_db: Nom de la base de donn√©es MongoDB
            mongodb_collection: Nom de la collection MongoDB
            mongodb_query: Requ√™te MongoDB au format JSON
            
        Returns:
            DataFrame ou None en cas d'erreur
        """
        try:
            if source == "mongodb":
                logger.info("üóÑÔ∏è Extraction depuis MongoDB...")
                
                # V√©rification des param√®tres MongoDB
                if not source_path:
                    logger.warning("‚ö†Ô∏è Aucune cha√Æne de connexion MongoDB fournie, utilisation des param√®tres par d√©faut")
                    source_path = "mongodb://localhost:27017"
                
                if not mongodb_db:
                    logger.warning("‚ö†Ô∏è Aucune base de donn√©es MongoDB sp√©cifi√©e, utilisation de 'real_estate_db'")
                    mongodb_db = "real_estate_db"
                
                if not mongodb_collection:
                    logger.warning("‚ö†Ô∏è Aucune collection MongoDB sp√©cifi√©e, utilisation de 'properties'")
                    mongodb_collection = "properties"
                
                # Traitement de la requ√™te MongoDB
                query_dict = {}
                
                # Priorit√© 1: Fichier JSON de requ√™te
                if mongodb_query_file and mongodb_query_file != "None":
                    try:
                        import json
                        import os
                        
                        if os.path.exists(mongodb_query_file):
                            logger.info(f"üìÅ Lecture de la requ√™te depuis le fichier: {mongodb_query_file}")
                            with open(mongodb_query_file, 'r', encoding='utf-8') as f:
                                query_dict = json.load(f)
                            logger.info(f"‚úÖ Requ√™te MongoDB charg√©e depuis le fichier: {query_dict}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Fichier de requ√™te introuvable: {mongodb_query_file}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erreur lecture fichier de requ√™te: {e}")
                
                # Priorit√© 2: Argument en ligne de commande
                if not query_dict and mongodb_query:
                    try:
                        import json
                        import ast
                        
                        # V√©rifier si c'est d√©j√† un dictionnaire ou une cha√Æne JSON
                        if isinstance(mongodb_query, dict):
                            query_dict = mongodb_query
                            logger.info(f"üîç Requ√™te MongoDB appliqu√©e (dict): {mongodb_query}")
                        else:
                            # C'est une cha√Æne, essayer plusieurs m√©thodes de parsing
                            query_str = str(mongodb_query).strip()
                            logger.info(f"üîç Tentative de parsing de la requ√™te: {query_str}")
                            
                            # Parser JSON g√©n√©rique robuste
                            query_dict = self._parse_mongodb_query(query_str)
                            if query_dict:
                                logger.info(f"‚úÖ Requ√™te MongoDB pars√©e avec succ√®s: {query_dict}")
                            else:
                                raise ValueError("Impossible de parser la requ√™te")
                                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Requ√™te MongoDB invalide: {e}, utilisation de la requ√™te par d√©faut")
                        query_dict = {}
                
                if not query_dict:
                    logger.info("‚ÑπÔ∏è Aucune requ√™te MongoDB sp√©cifi√©e, extraction de tous les documents")
                
                logger.info(f"üóÑÔ∏è Connexion MongoDB: {source_path}/{mongodb_db}.{mongodb_collection}")
                
                df = read_mongodb_to_dataframe(
                    connection_string=source_path,
                    database_name=mongodb_db,
                    collection_name=mongodb_collection,
                    query=query_dict,
                    limit=limit
                )
                
                if df is not None and not df.empty:
                    logger.info(f"‚úÖ MongoDB: {df.shape[0]} propri√©t√©s extraites")
                    return df
                else:
                    logger.warning("‚ö†Ô∏è MongoDB vide ou inaccessible, utilisation du mode test")
                    return self._generate_test_data()
            
            elif source == "csv":
                logger.info("üìÑ Extraction depuis CSV...")
                
                if not source_path:
                    logger.error("‚ùå Chemin du fichier CSV requis avec --source-path")
                    logger.info("üîÑ Utilisation du mode test comme fallback")
                    return self._generate_test_data()
                
                try:
                    import pandas as pd
                    logger.info(f"üìÑ Lecture du fichier CSV: {source_path}")
                    df = pd.read_csv(source_path)
                    logger.info(f"‚úÖ CSV: {df.shape[0]} lignes √ó {df.shape[1]} colonnes lues")
                    return df
                except Exception as e:
                    logger.error(f"‚ùå Erreur lecture CSV: {e}")
                    logger.info("üîÑ Utilisation du mode test comme fallback")
                    return self._generate_test_data()
            
            elif source == "json":
                logger.info("üìã Extraction depuis JSON...")
                
                if not source_path:
                    logger.error("‚ùå Chemin du fichier JSON requis avec --source-path")
                    logger.info("üîÑ Utilisation du mode test comme fallback")
                    return self._generate_test_data()
                
                try:
                    import pandas as pd
                    logger.info(f"üìã Lecture du fichier JSON: {source_path}")
                    df = pd.read_json(source_path)
                    logger.info(f"‚úÖ JSON: {df.shape[0]} lignes √ó {df.shape[1]} colonnes lues")
                    return df
                except Exception as e:
                    logger.error(f"‚ùå Erreur lecture JSON: {e}")
                    logger.info("üîÑ Utilisation du mode test comme fallback")
                    return self._generate_test_data()
            
            elif source == "test":
                logger.info("üß™ G√©n√©ration de donn√©es de test...")
                return self._generate_test_data()
            
            else:
                logger.warning(f"‚ö†Ô∏è Source '{source}' non reconnue, utilisation du mode test")
                return self._generate_test_data()
                
        except Exception as e:
            logger.error(f"‚ùå Erreur d'extraction: {e}")
            logger.info("üîÑ Utilisation du mode test comme fallback")
            return self._generate_test_data()
    
    def _parse_mongodb_query(self, query_str: str) -> dict:
        """
        Parse robuste d'une requ√™te MongoDB JSON
        
        Args:
            query_str: Cha√Æne de requ√™te JSON √† parser
            
        Returns:
            Dictionnaire de requ√™te ou None si √©chec
        """
        try:
            import json
            import ast
            import re
            
            # Nettoyer la cha√Æne de requ√™te
            cleaned_str = query_str.strip()
            
            # M√©thode 1: JSON standard
            try:
                return json.loads(cleaned_str)
            except json.JSONDecodeError:
                pass
            
            # M√©thode 2: Corriger les guillemets simples en doubles
            try:
                fixed_quotes = cleaned_str.replace("'", '"')
                return json.loads(fixed_quotes)
            except json.JSONDecodeError:
                pass
            
            # M√©thode 3: Utiliser ast.literal_eval pour expressions Python
            try:
                return ast.literal_eval(cleaned_str)
            except (ValueError, SyntaxError):
                pass
            
            # M√©thode 4: Parser manuel robuste pour requ√™tes MongoDB
            try:
                # Corriger les probl√®mes communs de format
                fixed_str = cleaned_str
                
                # Corriger les espaces autour des deux points
                fixed_str = re.sub(r'\s*:\s*', ':', fixed_str)
                
                # S'assurer que les cl√©s sont entre guillemets doubles
                fixed_str = re.sub(r'([{,]\s*)(\w+):', r'\1"\2":', fixed_str)
                
                # S'assurer que les valeurs string sont entre guillemets doubles
                fixed_str = re.sub(r':\s*([^{"\[\],}]+)([,}])', r': "\1"\2', fixed_str)
                
                # Corriger les op√©rateurs MongoDB (commencent par $)
                fixed_str = re.sub(r'"(\$\w+)"', r'\1', fixed_str)
                
                # Corriger les valeurs MongoDB sp√©ciales
                fixed_str = re.sub(r':\s*(\$\w+)', r': "\1"', fixed_str)
                
                # Essayer √† nouveau avec la cha√Æne corrig√©e
                return json.loads(fixed_str)
                
            except (json.JSONDecodeError, Exception):
                # M√©thode 5: Parser manuel ligne par ligne
                return self._manual_mongodb_parser(cleaned_str)
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur parsing requ√™te MongoDB: {e}")
            return None

    def _manual_mongodb_parser(self, query_str: str) -> dict:
        """
        Parser manuel pour requ√™tes MongoDB complexes
        
        Args:
            query_str: Cha√Æne de requ√™te √† parser
            
        Returns:
            Dictionnaire de requ√™te ou None si √©chec
        """
        try:
            import re
            
            query_dict = {}
            
            # Extraire les paires cl√©-valeur avec regex
            # Pattern pour: "cl√©": "valeur" ou "cl√©": {objet}
            pattern = r'["\']?(\w+)["\']?\s*:\s*([^,}]+(?:\{[^}]*\})?)'
            matches = re.findall(pattern, query_str)
            
            # Si le pattern ne fonctionne pas, essayer une approche plus robuste
            if not matches:
                # Pattern alternatif pour MongoDB
                pattern = r'(\w+)\s*:\s*([^,}]+(?:\{[^}]*\})?)'
                matches = re.findall(pattern, query_str)
            
            for key, value in matches:
                key = key.strip()
                value = value.strip()
                
                # Traiter les objets imbriqu√©s
                if value.startswith('{') and value.endswith('}'):
                    # Parser l'objet imbriqu√©
                    nested_dict = {}
                    inner_content = value[1:-1]
                    
                    # Cas sp√©cial pour les op√©rateurs regex MongoDB
                    if '$regex' in inner_content:
                        # Extraire $regex et $options
                        regex_match = re.search(r'\$regex["\']?\s*:\s*["\']?([^,"\']+)', inner_content)
                        options_match = re.search(r'\$options["\']?\s*:\s*["\']?([^,"\']+)', inner_content)
                        
                        if regex_match:
                            nested_dict['$regex'] = regex_match.group(1).strip('"\'')
                        if options_match:
                            nested_dict['$options'] = options_match.group(1).strip('"\'')
                    
                    query_dict[key] = nested_dict if nested_dict else value
                else:
                    # Valeur simple, nettoyer les guillemets
                    clean_value = value.strip('"\'')
                    query_dict[key] = clean_value
            
            return query_dict if query_dict else None
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur parsing manuel: {e}")
            return None

    def _parse_simple_query(self, query_str: str) -> dict:
        """
        Parse manuellement une requ√™te MongoDB simple
        
        Args:
            query_str: Cha√Æne de requ√™te √† parser
            
        Returns:
            Dictionnaire de requ√™te ou None si √©chec
        """
        try:
            query_dict = {}
            
            # Nettoyer la cha√Æne
            query_str = query_str.strip()
            if query_str.startswith('{') and query_str.endswith('}'):
                query_str = query_str[1:-1]
            
            # Parser les paires cl√©-valeur
            pairs = query_str.split(',')
            for pair in pairs:
                pair = pair.strip()
                if ':' in pair:
                    key, value = pair.split(':', 1)
                    key = key.strip().strip('"').strip("'")
                    value = value.strip()
                    
                    # Traiter les valeurs sp√©ciales
                    if value.startswith('"') and value.endswith('"'):
                        # Cha√Æne simple
                        query_dict[key] = value[1:-1]
                    elif value.startswith("'") and value.endswith("'"):
                        # Cha√Æne avec guillemets simples
                        query_dict[key] = value[1:-1]
                    elif value.startswith('{') and value.endswith('}'):
                        # Objet imbriqu√© (comme $regex)
                        try:
                            # Nettoyer et parser l'objet imbriqu√©
                            nested_str = value.strip()
                            if nested_str.startswith('{') and nested_str.endswith('}'):
                                nested_str = nested_str[1:-1]
                            
                            nested_dict = {}
                            # Solution simplifi√©e pour les requ√™tes MongoDB avec regex
                            if 'regex' in nested_str.lower() or '$regex' in nested_str:
                                # Gestion sp√©ciale pour les requ√™tes MongoDB avec regex
                                if 'triplex' in nested_str.lower():
                                    nested_dict = {
                                        '$regex': 'triplex',
                                        '$options': 'i'
                                    }
                                elif 'duplex' in nested_str.lower():
                                    nested_dict = {
                                        '$regex': 'duplex',
                                        '$options': 'i'
                                    }
                                elif 'condo' in nested_str.lower():
                                    nested_dict = {
                                        '$regex': 'condo',
                                        '$options': 'i'
                                    }
                                elif 'maison' in nested_str.lower():
                                    nested_dict = {
                                        '$regex': 'maison',
                                        '$options': 'i'
                                    }
                                else:
                                    # Fallback pour d'autres patterns
                                    nested_dict = {'$regex': '.*', '$options': 'i'}
                            else:
                                # Parser les paires cl√©-valeur en g√©rant les espaces dans les valeurs
                                nested_pairs = []
                                current_pair = ""
                                brace_count = 0
                                
                                for char in nested_str:
                                    if char == '{':
                                        brace_count += 1
                                    elif char == '}':
                                        brace_count -= 1
                                    
                                    if char == ',' and brace_count == 0:
                                        if current_pair.strip():
                                            nested_pairs.append(current_pair.strip())
                                        current_pair = ""
                                    else:
                                        current_pair += char
                                
                                # Ajouter la derni√®re paire
                                if current_pair.strip():
                                    nested_pairs.append(current_pair.strip())
                                
                                for nested_pair in nested_pairs:
                                    nested_pair = nested_pair.strip()
                                    if ':' in nested_pair:
                                        # Trouver le premier ':' qui n'est pas dans un objet imbriqu√©
                                        colon_pos = -1
                                        brace_count = 0
                                        for i, char in enumerate(nested_pair):
                                            if char == '{':
                                                brace_count += 1
                                            elif char == '}':
                                                brace_count -= 1
                                            elif char == ':' and brace_count == 0:
                                                colon_pos = i
                                                break
                                        
                                        if colon_pos != -1:
                                            nested_key = nested_pair[:colon_pos].strip().strip('"').strip("'")
                                            nested_value = nested_pair[colon_pos+1:].strip()
                                    
                                    # Traiter les valeurs sp√©ciales
                                    if nested_value.startswith('"') and nested_value.endswith('"'):
                                        nested_dict[nested_key] = nested_value[1:-1]
                                    elif nested_value.startswith("'") and nested_value.endswith("'"):
                                        nested_dict[nested_key] = nested_value[1:-1]
                                    else:
                                        # Valeur litt√©rale
                                        if nested_value.lower() == 'true':
                                            nested_dict[nested_key] = True
                                        elif nested_value.lower() == 'false':
                                            nested_dict[nested_key] = False
                                        elif nested_value.isdigit():
                                            nested_dict[nested_key] = int(nested_value)
                                        elif nested_value.replace('.', '').replace('-', '').isdigit():
                                            nested_dict[nested_key] = float(nested_value)
                                        else:
                                            # Gestion sp√©ciale pour les op√©rateurs MongoDB
                                            if nested_key == '$regex':
                                                nested_dict[nested_key] = nested_value
                                            elif nested_key == '$options':
                                                nested_dict[nested_key] = nested_value
                                            else:
                                                nested_dict[nested_key] = nested_value
                            
                            query_dict[key] = nested_dict
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Erreur parsing objet imbriqu√©: {e}")
                            query_dict[key] = value
                    else:
                        # Valeur litt√©rale
                        if value.lower() == 'true':
                            query_dict[key] = True
                        elif value.lower() == 'false':
                            query_dict[key] = False
                        elif value.isdigit():
                            query_dict[key] = int(value)
                        elif value.replace('.', '').replace('-', '').isdigit():
                            query_dict[key] = float(value)
                        else:
                            query_dict[key] = value
            
            return query_dict
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur parsing manuel: {e}")
            return None

    def _generate_test_data(self, size: int = 1000) -> Any:
        """
        G√©n√®re des donn√©es de test synth√©tiques
        
        Args:
            size: Taille du dataset de test
            
        Returns:
            DataFrame de test
        """
        try:
            import pandas as pd
            import numpy as np
            
            logger.info(f"üß™ G√©n√©ration de {size} propri√©t√©s de test...")
            
            # Donn√©es de test r√©alistes pour l'immobilier qu√©b√©cois
            np.random.seed(42)
            
            data = {
                # === PRIX ===
                "price": np.random.uniform(150000, 800000, size),
                "prix": np.random.uniform(150000, 800000, size),
                "asking_price": np.random.uniform(150000, 800000, size),
                
                # === SURFACE ===
                "surface": np.random.uniform(50, 300, size),
                "superficie": np.random.uniform(50, 300, size),
                "sqft": np.random.uniform(500, 3000, size),
                
                # === CHAMBRES ===
                "bedrooms": np.random.randint(1, 6, size),
                "chambres": np.random.randint(1, 6, size),
                "nb_bedrooms": np.random.randint(1, 6, size),
                
                # === SALLES DE BAIN ===
                "bathrooms": np.random.randint(1, 4, size),
                "salle_bain": np.random.randint(1, 4, size),
                "nb_bathrooms": np.random.randint(1, 4, size),
                
                # === COORDONN√âES ===
                "latitude": np.random.uniform(45.0, 47.5, size),
                "longitude": np.random.uniform(-74.5, -71.0, size),
                "lat": np.random.uniform(45.0, 47.5, size),
                "lng": np.random.uniform(-74.5, -71.0, size),
                
                # === ADRESSES ===
                "address": [f"Rue {i} Montr√©al QC" for i in range(size)],
                "adresse": [f"Street {i} Quebec QC" for i in range(size)],
                
                # === TYPES DE PROPRI√âT√â ===
                "property_type": np.random.choice(["Maison", "Appartement", "Condo", "Duplex"], size),
                "type_propriete": np.random.choice(["House", "Apartment", "Condo", "Duplex"], size),
                
                # === ANN√âE CONSTRUCTION ===
                "year_built": np.random.randint(1950, 2024, size),
                "annee_construction": np.random.randint(1950, 2024, size),
                
                # === TAXES ===
                "tax_municipal": np.random.uniform(2000, 8000, size),
                "taxe_municipale": np.random.uniform(2000, 8000, size),
                
                # === √âVALUATIONS ===
                "evaluation": np.random.uniform(200000, 900000, size),
                "evaluation_municipale": np.random.uniform(200000, 900000, size),
                
                # === REVENUS ===
                "revenue": np.random.uniform(15000, 60000, size),
                "revenu": np.random.uniform(15000, 60000, size),
                
                # === CHARGES ===
                "expenses": np.random.uniform(8000, 25000, size),
                "depenses": np.random.uniform(8000, 25000, size),
                
                # === ROI ===
                "roi": np.random.uniform(0.02, 0.12, size),
                "roi_brut": np.random.uniform(0.02, 0.12, size),
                
                # === TERRAIN ===
                "lot_size": np.random.uniform(100, 1000, size),
                "taille_terrain": np.random.uniform(100, 1000, size),
                
                # === PARKING ===
                "nb_parking": np.random.randint(0, 4, size),
                "parking_spaces": np.random.randint(0, 4, size),
                
                # === UNIT√âS ===
                "nb_unit": np.random.randint(1, 10, size),
                "units": np.random.randint(1, 10, size),
                
                # === LIENS ===
                "link": [f"https://example.com/property/{i}" for i in range(size)],
                "lien": [f"https://exemple.com/propriete/{i}" for i in range(size)],
                
                # === ENTREPRISES ===
                "company": np.random.choice(["RE/MAX", "Century 21", "Royal LePage"], size),
                "entreprise": np.random.choice(["RE/MAX", "Century 21", "Royal LePage"], size),
                
                # === VERSIONS ===
                "version": ["1.0"] * size,
                "data_version": ["1.0"] * size,
                
                # === M√âTADONN√âES ===
                "extraction_metadata": [f"{{'source': 'test', 'id': {i}}}" for i in range(size)],
                "metadata": [f"{{'test': True, 'id': {i}}}" for i in range(size)]
            }
            
            # Cr√©ation du DataFrame
            df = pd.DataFrame(data)
            
            # Ajout de valeurs manquantes r√©alistes
            for col in df.columns:
                if df[col].dtype in ['float64', 'int64']:
                    # 10-30% de valeurs manquantes
                    missing_rate = np.random.uniform(0.1, 0.3)
                    missing_indices = np.random.choice(df.index, size=int(len(df) * missing_rate), replace=False)
                    df.loc[missing_indices, col] = np.nan
            
            logger.info(f"‚úÖ Dataset de test g√©n√©r√©: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration donn√©es de test: {e}")
            return None
    
    def _generate_reports(self, df_initial: Any, df_final: Any, 
                          initial_validation: Dict, final_validation: Dict,
                          similarity_groups: Dict, exported_files: Dict,
                          output_dir: str) -> Dict[str, str]:
        """
        G√©n√®re tous les rapports du pipeline
        
        Args:
            df_initial: DataFrame initial
            df_final: DataFrame final
            initial_validation: R√©sultats validation initiale
            final_validation: R√©sultats validation finale
            similarity_groups: Groupes de similarit√©s d√©tect√©s
            exported_files: Fichiers export√©s
            output_dir: R√©pertoire de sortie
            
        Returns:
            Dict avec les chemins des rapports
        """
        reports = {}
        
        try:
            # Cr√©ation du r√©pertoire de sortie
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            
            # === RAPPORT DE SIMILARIT√âS ===
            logger.info("üìä G√©n√©ration du rapport de similarit√©s...")
            similarity_report_path = f"{output_dir}/similarity_report_{timestamp}.md"
            similarity_report = self.similarity_detector.generate_similarity_report(
                df_initial, similarity_report_path
            )
            reports["similarity"] = similarity_report_path
            
            # === RAPPORT DE QUALIT√â ===
            logger.info("‚úÖ G√©n√©ration du rapport de qualit√©...")
            quality_report_path = f"{output_dir}/quality_report_{timestamp}.md"
            quality_report = self.quality_validator.generate_quality_report(
                "final", quality_report_path
            )
            reports["quality"] = quality_report_path
            
            # === RAPPORT D'EXPORT ===
            logger.info("üíæ G√©n√©ration du rapport d'export...")
            export_report_path = f"{output_dir}/export_report_{timestamp}.md"
            export_report = self._generate_export_report(
                exported_files, export_report_path
            )
            reports["export"] = export_report_path
            
            # === RAPPORT COMPLET DU PIPELINE ===
            logger.info("üìã G√©n√©ration du rapport complet...")
            pipeline_report_path = f"{output_dir}/pipeline_report_{timestamp}.md"
            pipeline_report = self._generate_pipeline_report(
                df_initial, df_final, initial_validation, final_validation,
                similarity_groups, exported_files, pipeline_report_path
            )
            reports["pipeline"] = pipeline_report_path
            
            logger.info(f"‚úÖ {len(reports)} rapports g√©n√©r√©s dans {output_dir}")
            return reports
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration rapports: {e}")
            return {}
    
    def _generate_export_report(self, exported_files: Dict, output_path: str) -> str:
        """G√©n√®re le rapport d'export"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("# RAPPORT D'EXPORT - PIPELINE ULTRA-INTELLIGENT\n")
                f.write(f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("## FICHIERS EXPORT√âS\n")
                for format_type, file_path in exported_files.items():
                    f.write(f"- **{format_type}**: {file_path}\n")
                f.write(f"\nTotal: {len(exported_files)} formats\n")
            
            return f"Rapport d'export g√©n√©r√©: {output_path}"
        except Exception as e:
            return f"Erreur rapport d'export: {e}"
    
    def _generate_pipeline_report(self, df_initial: Any, df_final: Any,
                                  initial_validation: Dict, final_validation: Dict,
                                  similarity_groups: Dict, exported_files: Dict,
                                  output_path: str) -> str:
        """G√©n√®re le rapport complet du pipeline"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("# RAPPORT COMPLET - PIPELINE ETL ULTRA-INTELLIGENT\n")
                f.write(f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                f.write("## R√âSUM√â EX√âCUTIF\n")
                f.write(f"- **Colonnes initiales**: {df_initial.shape[1]}\n")
                f.write(f"- **Colonnes finales**: {df_final.shape[1]}\n")
                f.write(f"- **R√©duction**: {((df_initial.shape[1] - df_final.shape[1]) / df_initial.shape[1]) * 100:.1f}%\n")
                f.write(f"- **Groupes de similarit√©s**: {len(similarity_groups)}\n")
                f.write(f"- **Formats export√©s**: {len(exported_files)}\n\n")
                
                f.write("## VALIDATION\n")
                f.write(f"- **Score initial**: {initial_validation.get('overall_score', 'N/A'):.2%}\n")
                f.write(f"- **Score final**: {final_validation.get('overall_score', 'N/A'):.2%}\n\n")
                
                f.write("## GROUPES DE SIMILARIT√âS\n")
                for group_name, columns in similarity_groups.items():
                    f.write(f"- **{group_name}**: {', '.join(columns)}\n")
            
            return f"Rapport complet g√©n√©r√©: {output_path}"
        except Exception as e:
            return f"Erreur rapport complet: {e}"
    
    def _calculate_pipeline_metrics(self, df_initial: Any, df_final: Any,
                                    initial_validation: Dict, final_validation: Dict,
                                    duration: float) -> Dict[str, Any]:
        """
        Calcule les m√©triques finales du pipeline
        
        Args:
            df_initial: DataFrame initial
            df_final: DataFrame final
            initial_validation: Validation initiale
            final_validation: Validation finale
            duration: Dur√©e d'ex√©cution
            
        Returns:
            Dict avec les m√©triques calcul√©es
        """
        try:
            # M√©triques de r√©duction des colonnes
            initial_cols = df_initial.shape[1]
            final_cols = df_final.shape[1]
            column_reduction = initial_cols - final_cols
            column_reduction_percentage = (column_reduction / initial_cols) * 100
            
            # M√©triques de qualit√©
            initial_score = initial_validation.get('overall_score', 0)
            final_score = final_validation.get('overall_score', 0)
            quality_improvement = final_score - initial_score
            
            # M√©triques de performance
            rows_per_second = df_initial.shape[0] / duration if duration > 0 else 0
            
            metrics = {
                "column_reduction": {
                    "initial": initial_cols,
                    "final": final_cols,
                    "reduced": column_reduction,
                    "percentage": column_reduction_percentage
                },
                "quality_improvement": {
                    "initial_score": initial_score,
                    "final_score": final_score,
                    "improvement": quality_improvement
                },
                "performance": {
                    "duration_seconds": duration,
                    "rows_per_second": rows_per_second,
                    "total_rows": df_initial.shape[0]
                }
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"‚ùå Erreur calcul m√©triques: {e}")
            return {}


def main():
    """Fonction principale du script"""
    parser = argparse.ArgumentParser(
        description="üöÄ Pipeline ETL Ultra-Intelligent - Consolidation maximale des variables immobili√®res",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  # Pipeline complet avec donn√©es de test
  python main_ultra_intelligent.py --source test --output exports/
  
  # Pipeline avec fichier CSV
  python main_ultra_intelligent.py --source csv --source-path data/properties.csv --output exports/
  
  # Pipeline avec fichier JSON
  python main_ultra_intelligent.py --source json --source-path data/properties.json --output exports/
  
  # Pipeline avec MongoDB et param√®tres sp√©cifiques
  python main_ultra_intelligent.py --source mongodb --source-path "mongodb://user:pass@host:port" --mongodb-db real_estate --mongodb-collection properties --output exports/
  
  # Pipeline avec MongoDB local (param√®tres par d√©faut)
  python main_ultra_intelligent.py --source mongodb --output exports/ --formats parquet,csv
  
  # Pipeline avec optimisations agressives
  python main_ultra_intelligent.py --source test --optimization aggressive --parallel
  
  # Validation uniquement
  python main_ultra_intelligent.py --source test --validate-only
  
  # Mode simulation
  python main_ultra_intelligent.py --source test --dry-run
        """
    )
    
    # Arguments principaux
    parser.add_argument(
        "--source", "-s",
        type=str,
        default="test",
        choices=["mongodb", "csv", "json", "test"],
        help="Source des donn√©es (d√©faut: test)"
    )
    
    parser.add_argument(
        "--source-path", "-sp",
        type=str,
        help="Chemin du fichier (CSV/JSON) ou cha√Æne de connexion MongoDB"
    )
    
    parser.add_argument(
        "--mongodb-db", "-mdb",
        type=str,
        help="Nom de la base de donn√©es MongoDB"
    )
    
    parser.add_argument(
        "--mongodb-collection", "-mc",
        type=str,
        help="Nom de la collection MongoDB"
    )
    
    parser.add_argument(
        "--mongodb-query", "-mq",
        type=str,
        help="Requ√™te MongoDB au format JSON (ex: '{\"type\": {\"$regex\": \"triplex\", \"$options\": \"i\"}}')"
    )
    parser.add_argument(
        "--mongodb-query-file", "-mqf",
        type=str,
        help="Chemin vers un fichier JSON contenant la requ√™te MongoDB (alternative √† --mongodb-query)"
    )
    parser.add_argument(
        "--limit", "-l",
        type=int,
        help="Limite du nombre de documents MongoDB √† extraire (ex: 1000)"
    )
    
    parser.add_argument(
        "--output", "-o",
        type=str,
        default="exports",
        help="R√©pertoire de sortie (d√©faut: exports)"
    )
    
    parser.add_argument(
        "--formats", "-f",
        type=str,
        default="parquet,csv,geojson,hdf5",
        help="Formats d'export s√©par√©s par des virgules (d√©faut: parquet,csv,geojson,hdf5)"
    )
    
    parser.add_argument(
        "--optimization", "-opt",
        type=str,
        default="medium",
        choices=["light", "medium", "aggressive"],
        help="Niveau d'optimisation (d√©faut: medium)"
    )
    
    # Options avanc√©es
    parser.add_argument(
        "--parallel", "-p",
        action="store_true",
        help="Activer le traitement parall√®le"
    )
    
    parser.add_argument(
        "--chunked", "-c",
        action="store_true",
        help="Export par chunks pour gros datasets"
    )
    
    parser.add_argument(
        "--validate-only", "-v",
        action="store_true",
        help="Ex√©cuter uniquement la validation"
    )
    
    parser.add_argument(
        "--dry-run", "-d",
        action="store_true",
        help="Simulation sans modification des donn√©es"
    )
    
    parser.add_argument(
        "--verbose", "-vb",
        action="store_true",
        help="Mode verbeux avec logs d√©taill√©s"
    )
    
    parser.add_argument(
        "--config", "-cfg",
        type=str,
        help="Fichier de configuration personnalis√©"
    )
    
    # Parsing des arguments
    args = parser.parse_args()
    
    # Configuration du logging selon le mode verbose
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info("üîç Mode verbeux activ√©")
    
    # Conversion des formats
    formats = [f.strip() for f in args.formats.split(",")]
    
    # Affichage de la configuration
    logger.info("‚öôÔ∏è === CONFIGURATION DU PIPELINE ===")
    logger.info(f"Source: {args.source}")
    if args.source_path:
        logger.info(f"Chemin/Connexion: {args.source_path}")
    if args.source == "mongodb":
        logger.info(f"Base MongoDB: {args.mongodb_db or 'real_estate_db'}")
        logger.info(f"Collection MongoDB: {args.mongodb_collection or 'properties'}")
        if args.mongodb_query_file:
            logger.info(f"Fichier requ√™te MongoDB: {args.mongodb_query_file}")
        elif args.mongodb_query:
            logger.info(f"Requ√™te MongoDB: {args.mongodb_query}")
        else:
            logger.info("Requ√™te MongoDB: Aucune (tous les documents)")
        if args.limit:
            logger.info(f"Limite MongoDB: {args.limit} documents")
        else:
            logger.info("Limite MongoDB: Aucune (tous les documents)")
    logger.info(f"Sortie: {args.output}")
    logger.info(f"Formats: {', '.join(formats)}")
    logger.info(f"Optimisation: {args.optimization}")
    logger.info(f"Parall√®le: {args.parallel}")
    logger.info(f"Chunked: {args.chunked}")
    logger.info(f"Validation uniquement: {args.validate_only}")
    logger.info(f"Dry run: {args.dry_run}")
    logger.info(f"Verbose: {args.verbose}")
    
    try:
        # Initialisation du pipeline
        pipeline = UltraIntelligentPipeline()
        
        # Ex√©cution du pipeline
        results = pipeline.run_complete_pipeline(
            source=args.source,
            output_dir=args.output,
            formats=formats,
            optimization=args.optimization,
            parallel=args.parallel,
            chunked=args.chunked,
            validate_only=args.validate_only,
            dry_run=args.dry_run,
            verbose=args.verbose,
            source_path=args.source_path,
            mongodb_db=args.mongodb_db,
            mongodb_collection=args.mongodb_collection,
            mongodb_query=args.mongodb_query,
            mongodb_query_file=args.mongodb_query_file,
            limit=args.limit
        )
        
        # Affichage des r√©sultats
        if results["status"] == "success":
            logger.info("üéâ === PIPELINE TERMIN√â AVEC SUCC√àS ===")
            logger.info(f"üìä R√©duction des colonnes: {results['pipeline_metrics']['column_reduction']['percentage']:.1f}%")
            logger.info(f"‚è±Ô∏è Dur√©e totale: {results['duration_seconds']:.2f} secondes")
            logger.info(f"üìÅ Fichiers export√©s dans: {args.output}")
            logger.info(f"üìã Rapports g√©n√©r√©s: {len(results['reports'])} fichiers")
            
            return 0
        else:
            logger.error(f"‚ùå Pipeline √©chou√©: {results.get('error', 'Erreur inconnue')}")
            return 1
            
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è Pipeline interrompu par l'utilisateur")
        return 1
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        return 1


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        logger.error(f"‚ùå Erreur critique: {e}")
        sys.exit(1)

