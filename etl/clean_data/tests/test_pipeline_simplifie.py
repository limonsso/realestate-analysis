#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§ª TEST SIMPLIFIÃ‰ DU PIPELINE - VALIDATION ESSENTIELLE
======================================================

Test simplifiÃ© du pipeline qui Ã©vite les dÃ©pendances optionnelles
Focus sur les fonctionnalitÃ©s de base et la consolidation
"""

import sys
import os
import logging
import pandas as pd
import numpy as np
import time
from pathlib import Path
from datetime import datetime

# Ajout du rÃ©pertoire parent au PYTHONPATH pour les imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PipelineSimplifiedTest:
    """Test simplifiÃ© du pipeline sans dÃ©pendances optionnelles"""
    
    def __init__(self):
        self.start_time = time.time()
        self.test_results = {}
        self.test_data = None
        
    def run_simplified_test(self):
        """ExÃ©cute les tests simplifiÃ©s du pipeline"""
        logger.info("ğŸš€ === DÃ‰MARRAGE DU TEST SIMPLIFIÃ‰ DU PIPELINE ===")
        logger.info(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Tests sÃ©quentiels
        tests = [
            ("Configuration de Base", self.test_basic_configuration),
            ("CrÃ©ation Dataset Test", self.test_data_creation),
            ("Consolidation Intelligente", self.test_smart_consolidation),
            ("Validation Basique", self.test_basic_validation),
            ("Export Simple", self.test_simple_export)
        ]
        
        for test_name, test_func in tests:
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ§ª TEST: {test_name}")
            logger.info(f"{'='*80}")
            
            try:
                success = test_func()
                self.test_results[test_name] = {
                    "status": "âœ… SUCCÃˆS" if success else "âŒ Ã‰CHEC",
                    "success": success
                }
                logger.info(f"ğŸ“Š {test_name}: {'âœ… SUCCÃˆS' if success else 'âŒ Ã‰CHEC'}")
            except Exception as e:
                logger.error(f"ğŸ’¥ Erreur critique dans {test_name}: {e}")
                self.test_results[test_name] = {
                    "status": "ğŸ’¥ ERREUR CRITIQUE",
                    "success": False,
                    "error": str(e)
                }
        
        # Rapport final
        self.generate_final_report()
        
    def test_basic_configuration(self):
        """Test de la configuration de base"""
        logger.info("âš™ï¸ === TEST CONFIGURATION DE BASE ===")
        
        try:
            # Test des imports de base
            logger.info("ğŸ“¦ Test des imports de base...")
            from config.consolidation_config import ConsolidationConfig
            from config.custom_fields_config import custom_config
            logger.info("âœ… Imports de configuration rÃ©ussis")
            
            # Test de la configuration de base
            logger.info("âš™ï¸ Test configuration standard...")
            base_config = ConsolidationConfig()
            groups_count = len(base_config.CONSOLIDATION_GROUPS)
            logger.info(f"ğŸ“Š Configuration standard: {groups_count} groupes")
            
            if groups_count < 25:
                logger.error(f"âŒ Nombre de groupes insuffisant: {groups_count} < 25")
                return False
            
            # Test de la configuration personnalisÃ©e
            logger.info("ğŸ”§ Test configuration personnalisÃ©e...")
            summary = custom_config.get_67_fields_config_summary()
            logger.info(f"ğŸ“Š Configuration personnalisÃ©e: {summary['total_consolidation_groups']} groupes")
            logger.info(f"ğŸ“Š RÃ©duction estimÃ©e: {summary['estimated_reduction']}")
            
            # Validation de la configuration
            if base_config.validate_configuration():
                logger.info("âœ… Configuration validÃ©e avec succÃ¨s")
                return True
            else:
                logger.error("âŒ Validation de configuration Ã©chouÃ©e")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur configuration: {e}")
            return False
    
    def test_data_creation(self):
        """Test de crÃ©ation du dataset de test"""
        logger.info("ğŸ“¥ === TEST CRÃ‰ATION DATASET DE TEST ===")
        
        try:
            logger.info("ğŸ—ï¸ GÃ©nÃ©ration de donnÃ©es de test...")
            self.test_data = self.create_comprehensive_test_data()
            logger.info(f"ğŸ“Š Dataset crÃ©Ã©: {self.test_data.shape[0]} lignes Ã— {self.test_data.shape[1]} colonnes")
            
            # Validation du dataset
            if self.test_data.shape[1] >= 50:
                logger.info(f"âœ… Dataset valide: {self.test_data.shape[1]} colonnes")
                
                # Analyse rapide
                null_percentage = (self.test_data.isnull().sum().sum() / (self.test_data.shape[0] * self.test_data.shape[1])) * 100
                logger.info(f"ğŸ“Š Valeurs manquantes: {null_percentage:.1f}%")
                
                # Types de colonnes
                numeric_cols = len(self.test_data.select_dtypes(include=[np.number]).columns)
                text_cols = len(self.test_data.select_dtypes(include=['object']).columns)
                logger.info(f"ğŸ“Š Colonnes numÃ©riques: {numeric_cols}")
                logger.info(f"ğŸ“Š Colonnes textuelles: {text_cols}")
                
                return True
            else:
                logger.error(f"âŒ Dataset insuffisant: {self.test_data.shape[1]} colonnes")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur crÃ©ation dataset: {e}")
            return False
    
    def test_smart_consolidation(self):
        """Test de la consolidation intelligente"""
        logger.info("ğŸ”— === TEST CONSOLIDATION INTELLIGENTE ===")
        
        try:
            if self.test_data is None:
                logger.error("âŒ Pas de donnÃ©es de test disponibles")
                return False
            
            # Test de consolidation avec config personnalisÃ©e
            logger.info("âš™ï¸ Configuration pour consolidation...")
            from config.custom_fields_config import custom_config
            from core.ultra_intelligent_cleaner import UltraIntelligentCleaner
            
            cleaner = UltraIntelligentCleaner(custom_config)
            logger.info("âœ… Nettoyeur configurÃ©")
            
            # Consolidation
            logger.info("ğŸ”„ DÃ©marrage de la consolidation...")
            initial_columns = self.test_data.shape[1]
            
            df_consolidated = cleaner._consolidate_variables(self.test_data)
            final_columns = df_consolidated.shape[1]
            reduction_percentage = ((initial_columns - final_columns) / initial_columns) * 100
            
            logger.info(f"ğŸ“Š Consolidation terminÃ©e:")
            logger.info(f"   ğŸ“Š Colonnes initiales: {initial_columns}")
            logger.info(f"   ğŸ“Š Colonnes finales: {final_columns}")
            logger.info(f"   ğŸ“‰ RÃ©duction: {reduction_percentage:.1f}%")
            
            # Analyse de la qualitÃ© de consolidation
            if df_consolidated is not None and final_columns > 0:
                logger.info("ğŸ” Analyse de la qualitÃ© de consolidation...")
                
                # VÃ©rifier que les donnÃ©es ont Ã©tÃ© conservÃ©es
                initial_rows = self.test_data.shape[0]
                final_rows = df_consolidated.shape[0]
                logger.info(f"ğŸ“Š Lignes conservÃ©es: {final_rows}/{initial_rows}")
                
                # VÃ©rifier la complÃ©tude aprÃ¨s consolidation
                null_percentage_after = (df_consolidated.isnull().sum().sum() / (df_consolidated.shape[0] * df_consolidated.shape[1])) * 100
                logger.info(f"ğŸ“Š Valeurs manquantes aprÃ¨s: {null_percentage_after:.1f}%")
                
                # Sauvegarder pour les tests suivants
                self.consolidated_data = df_consolidated
                
                logger.info("âœ… Consolidation rÃ©ussie")
                return True
            else:
                logger.error("âŒ Consolidation Ã©chouÃ©e")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur consolidation: {e}")
            return False
    
    def test_basic_validation(self):
        """Test de validation basique"""
        logger.info("âœ… === TEST VALIDATION BASIQUE ===")
        
        try:
            if not hasattr(self, 'consolidated_data'):
                df = self.test_data
                logger.info("âš ï¸ Utilisation des donnÃ©es non consolidÃ©es")
            else:
                df = self.consolidated_data
                logger.info("âœ… Utilisation des donnÃ©es consolidÃ©es")
            
            # Validation manuelle basique
            logger.info("ğŸ” Validation manuelle des donnÃ©es...")
            
            # 1. Validation de la forme
            logger.info(f"ğŸ“Š Forme du dataset: {df.shape}")
            if df.shape[0] == 0:
                logger.error("âŒ Dataset vide")
                return False
            
            # 2. Validation de la complÃ©tude
            total_cells = df.shape[0] * df.shape[1]
            null_cells = df.isnull().sum().sum()
            completeness = (1 - (null_cells / total_cells)) * 100
            logger.info(f"ğŸ“Š ComplÃ©tude: {completeness:.1f}%")
            
            # 3. Validation des types
            numeric_cols = len(df.select_dtypes(include=[np.number]).columns)
            text_cols = len(df.select_dtypes(include=['object']).columns)
            date_cols = len(df.select_dtypes(include=['datetime64']).columns)
            logger.info(f"ğŸ“Š Types: {numeric_cols} num, {text_cols} text, {date_cols} date")
            
            # 4. Validation des valeurs aberrantes
            outliers_found = 0
            for col in df.select_dtypes(include=[np.number]).columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))][col]
                outliers_found += len(outliers)
            
            outlier_percentage = (outliers_found / total_cells) * 100
            logger.info(f"ğŸ“Š Valeurs aberrantes: {outlier_percentage:.2f}%")
            
            # Score de qualitÃ© basique
            quality_score = (completeness * 0.5) + ((100 - outlier_percentage) * 0.3) + (20 if numeric_cols > 0 else 0)
            logger.info(f"ğŸ“Š Score de qualitÃ© basique: {quality_score:.1f}%")
            
            if quality_score > 50:
                logger.info("âœ… Validation basique rÃ©ussie")
                return True
            else:
                logger.warning("âš ï¸ QualitÃ© basique insuffisante")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur validation basique: {e}")
            return False
    
    def test_simple_export(self):
        """Test d'export simple"""
        logger.info("ğŸ’¾ === TEST EXPORT SIMPLE ===")
        
        try:
            if not hasattr(self, 'consolidated_data'):
                df = self.test_data.head(50)  # RÃ©duire pour les tests
                logger.info("âš ï¸ Export des donnÃ©es non consolidÃ©es")
            else:
                df = self.consolidated_data.head(50)
                logger.info("âœ… Export des donnÃ©es consolidÃ©es")
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Test export CSV (toujours disponible)
            logger.info("ğŸ“„ Test export CSV...")
            csv_path = f"exports/tests/test_simplifie_{timestamp}.csv"
            
            # CrÃ©er le dossier si nÃ©cessaire
            Path("exports/tests").mkdir(parents=True, exist_ok=True)
            
            # Export
            df.to_csv(csv_path, index=False, encoding='utf-8')
            
            # VÃ©rification
            if Path(csv_path).exists():
                file_size = Path(csv_path).stat().st_size
                logger.info(f"âœ… Export CSV rÃ©ussi: {csv_path} ({file_size} bytes)")
                
                # Test de lecture
                df_test = pd.read_csv(csv_path)
                if df_test.shape == df.shape:
                    logger.info("âœ… VÃ©rification lecture CSV rÃ©ussie")
                    return True
                else:
                    logger.error("âŒ DonnÃ©es corrompues lors de l'export")
                    return False
            else:
                logger.error("âŒ Fichier CSV non crÃ©Ã©")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur export simple: {e}")
            return False
    
    def create_comprehensive_test_data(self):
        """CrÃ©e un dataset de test complet"""
        n_rows = 100
        
        # Dataset avec les colonnes essentielles
        test_data = pd.DataFrame({
            # Prix et Ã©valuations
            'price': np.random.uniform(200000, 2000000, n_rows),
            'prix_evaluation': np.random.uniform(180000, 1800000, n_rows),
            'price_assessment': np.random.uniform(180000, 1800000, n_rows),
            'evaluation_total': np.random.uniform(180000, 1800000, n_rows),
            'evaluation_terrain': np.random.uniform(80000, 800000, n_rows),
            'evaluation_batiment': np.random.uniform(100000, 1000000, n_rows),
            'municipal_evaluation_building': np.random.uniform(100000, 1000000, n_rows),
            'municipal_evaluation_land': np.random.uniform(80000, 800000, n_rows),
            'municipal_evaluation_total': np.random.uniform(180000, 1800000, n_rows),
            
            # Surface et caractÃ©ristiques
            'surface': np.random.uniform(50, 500, n_rows),
            'living_area': np.random.uniform(50, 500, n_rows),
            'superficie': np.random.uniform(50, 500, n_rows),
            'lot_size': np.random.uniform(100, 1000, n_rows),
            'bedrooms': np.random.randint(1, 6, n_rows),
            'nbr_chanbres': np.random.randint(1, 6, n_rows),
            'nb_bedroom': np.random.randint(1, 6, n_rows),
            'bathrooms': np.random.randint(1, 4, n_rows),
            'nbr_sal_deau': np.random.randint(1, 3, n_rows),
            'nbr_sal_bain': np.random.randint(1, 4, n_rows),
            'nb_bathroom': np.random.randint(1, 4, n_rows),
            'water_rooms': np.random.randint(1, 3, n_rows),
            
            # CoordonnÃ©es
            'latitude': np.random.uniform(45.4, 45.7, n_rows),
            'longitude': np.random.uniform(-73.8, -73.4, n_rows),
            'geolocation': [f'45.5,-73.6#{i}' for i in range(n_rows)],
            
            # Adresses
            'address': [f'123 Rue Principale #{i}' for i in range(n_rows)],
            'full_address': [f'123 Rue Principale #{i}, MontrÃ©al, QC' for i in range(n_rows)],
            'city': (['MontrÃ©al', 'QuÃ©bec', 'Laval'] * (n_rows // 3 + 1))[:n_rows],
            'postal_code': (['H1A 1A1', 'H2B 2B2', 'H3C 3C3'] * (n_rows // 3 + 1))[:n_rows],
            
            # Type propriÃ©tÃ©
            'type': (['Maison', 'Appartement', 'Duplex', 'Triplex'] * (n_rows // 4 + 1))[:n_rows],
            'building_style': (['Moderne', 'Traditionnel', 'Contemporain'] * (n_rows // 3 + 1))[:n_rows],
            'style': (['Moderne', 'Traditionnel', 'Contemporain'] * (n_rows // 3 + 1))[:n_rows],
            
            # AnnÃ©e construction
            'year_built': np.random.randint(1950, 2025, n_rows),
            'construction_year': np.random.randint(1950, 2025, n_rows),
            'annee': np.random.randint(1950, 2025, n_rows),
            
            # Taxes
            'municipal_taxes': np.random.uniform(2000, 20000, n_rows),
            'municipal_tax': np.random.uniform(2000, 20000, n_rows),
            'taxes': np.random.uniform(3000, 30000, n_rows),
            'school_taxes': np.random.uniform(1000, 10000, n_rows),
            'school_tax': np.random.uniform(1000, 10000, n_rows),
            
            # Revenus
            'revenu': np.random.uniform(15000, 150000, n_rows),
            'revenus_annuels_bruts': np.random.uniform(15000, 150000, n_rows),
            'plex-revenu': np.random.uniform(15000, 150000, n_rows),
            'plex_revenu': np.random.uniform(15000, 150000, n_rows),
            'potential_gross_revenue': np.random.uniform(15000, 150000, n_rows),
            
            # DÃ©penses
            'expense': np.random.uniform(5000, 50000, n_rows),
            'depenses': np.random.uniform(5000, 50000, n_rows),
            'expense_period': ['Annuel'] * n_rows,
            
            # Parking et unitÃ©s
            'nb_parking': np.random.randint(0, 4, n_rows),
            'parking': np.random.randint(0, 4, n_rows),
            'nb_garage': np.random.randint(0, 3, n_rows),
            'unites': np.random.randint(1, 5, n_rows),
            'residential_units': np.random.randint(1, 5, n_rows),
            'commercial_units': np.random.randint(0, 3, n_rows),
            
            # Autres
            'image': [f'https://exemple.com/image_{i}.jpg' for i in range(n_rows)],
            'images': [f'https://exemple.com/images_{i}.jpg' for i in range(n_rows)],
            'img_src': [f'https://exemple.com/image_{i}.jpg' for i in range(n_rows)],
            'revenu_period': ['Annuel'] * n_rows,
            'basement': (['Oui', 'Non', 'Partiel'] * (n_rows // 3 + 1))[:n_rows],
            
            # Champs prÃ©servÃ©s
            'main_unit_details': [f'DÃ©tails unitÃ© #{i}' for i in range(n_rows)],
            'vendue': (['Non', 'Oui', 'En cours'] * (n_rows // 3 + 1))[:n_rows],
            'description': [f'Belle propriÃ©tÃ© #{i}' for i in range(n_rows)],
            '_id': range(n_rows),
            'updated_at': pd.date_range('2024-01-01', periods=n_rows, freq='D'),
            'region': ['QC'] * n_rows,
            'extraction_metadata': [f'Metadata #{i}' for i in range(n_rows)],
            
            # MÃ©tadonnÃ©es Ã  supprimer
            'link': [f'https://exemple.com/propriete/{i}' for i in range(n_rows)],
            'company': (['Royal LePage', 'Century 21', 'RE/MAX'] * (n_rows // 3 + 1))[:n_rows],
            'version': ['1.0'] * n_rows
        })
        
        # Ajout de valeurs manquantes pour tester la consolidation
        for col in test_data.columns:
            if test_data[col].dtype in ['object', 'float64']:
                # 10% de valeurs manquantes alÃ©atoires
                mask = np.random.choice([True, False], size=n_rows, p=[0.1, 0.9])
                test_data.loc[mask, col] = np.nan
        
        return test_data
    
    def generate_final_report(self):
        """GÃ©nÃ¨re le rapport final du test simplifiÃ©"""
        end_time = time.time()
        duration = end_time - self.start_time
        
        logger.info(f"\n{'='*100}")
        logger.info("ğŸ“Š RAPPORT FINAL DU TEST SIMPLIFIÃ‰ DU PIPELINE")
        logger.info(f"{'='*100}")
        
        logger.info(f"ğŸ•’ DurÃ©e totale: {duration:.2f} secondes")
        logger.info(f"ğŸ“… Date de fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # RÃ©sumÃ© des tests
        logger.info(f"\nğŸ“‹ RÃ‰SUMÃ‰ DES TESTS:")
        success_count = sum(1 for result in self.test_results.values() if result["success"])
        total_count = len(self.test_results)
        
        for test_name, result in self.test_results.items():
            logger.info(f"   {test_name:35} : {result['status']}")
        
        logger.info(f"\nğŸ¯ RÃ‰SULTAT GLOBAL: {success_count}/{total_count} tests rÃ©ussis")
        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
        logger.info(f"ğŸ“Š Taux de rÃ©ussite: {success_rate:.1f}%")
        
        # Conclusion
        if success_count == total_count:
            logger.info("\nğŸ‰ TOUS LES TESTS SONT PASSÃ‰S!")
            logger.info("âœ… Les fonctionnalitÃ©s essentielles du pipeline sont 100% fonctionnelles")
            logger.info("ğŸš€ Structure organisÃ©e validÃ©e et prÃªte pour production!")
        elif success_rate >= 80:
            logger.info("\nâœ¨ TESTS MAJORITAIREMENT RÃ‰USSIS!")
            logger.info(f"âœ… {success_rate:.1f}% des fonctionnalitÃ©s essentielles opÃ©rationnelles")
            logger.info("âš ï¸ Quelques optimisations mineures recommandÃ©es")
        else:
            logger.warning(f"\nâš ï¸ TESTS PARTIELLEMENT RÃ‰USSIS ({success_rate:.1f}%)")
            logger.warning("ğŸ”§ Corrections nÃ©cessaires avant production")
        
        # Sauvegarde du rapport
        self.save_test_report(success_count == total_count, success_rate)
        
        logger.info(f"\n{'='*100}")
        
        return success_count == total_count
    
    def save_test_report(self, all_passed, success_rate):
        """Sauvegarde le rapport de test dans un fichier"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = f"logs/test_pipeline_simplifie_{timestamp}.md"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("# ğŸ§ª RAPPORT DE TEST SIMPLIFIÃ‰ DU PIPELINE\n\n")
                f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"**DurÃ©e**: {time.time() - self.start_time:.2f} secondes\n\n")
                
                f.write("## ğŸ“‹ RÃ©sultats des Tests Essentiels\n\n")
                for test_name, result in self.test_results.items():
                    status_icon = "âœ…" if result["success"] else "âŒ"
                    f.write(f"- {status_icon} **{test_name}**: {result['status']}\n")
                
                success_count = sum(1 for result in self.test_results.values() if result["success"])
                total_count = len(self.test_results)
                
                f.write(f"\n## ğŸ¯ RÃ©sultat Global\n\n")
                f.write(f"- **Tests rÃ©ussis**: {success_count}/{total_count}\n")
                f.write(f"- **Taux de rÃ©ussite**: {success_rate:.1f}%\n")
                
                if all_passed:
                    f.write("\n## ğŸ‰ Conclusion\n\n")
                    f.write("Les **fonctionnalitÃ©s essentielles** du pipeline sont **100% fonctionnelles** !\n")
                    f.write("La **structure rÃ©organisÃ©e** est **validÃ©e** et **prÃªte pour production** !\n")
                elif success_rate >= 80:
                    f.write("\n## âœ¨ Conclusion\n\n")
                    f.write(f"**{success_rate:.1f}%** des fonctionnalitÃ©s essentielles sont opÃ©rationnelles.\n")
                    f.write("Le pipeline est **majoritairement fonctionnel** avec quelques optimisations recommandÃ©es.\n")
                else:
                    f.write("\n## âš ï¸ Conclusion\n\n")
                    f.write("Des corrections sont nÃ©cessaires avant la mise en production.\n")
            
            logger.info(f"ğŸ“„ Rapport sauvegardÃ©: {report_file}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Impossible de sauvegarder le rapport: {e}")

def main():
    """Fonction principale"""
    test_suite = PipelineSimplifiedTest()
    success = test_suite.run_simplified_test()
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())
