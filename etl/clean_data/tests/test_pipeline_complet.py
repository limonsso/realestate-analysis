#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§ª TEST COMPLET DU PIPELINE ULTRA-INTELLIGENT
=============================================

Test exhaustif de toutes les fonctionnalitÃ©s du pipeline:
- Configuration et imports
- Extraction des donnÃ©es
- Consolidation maximale
- FonctionnalitÃ©s avancÃ©es
- Export multi-formats
- GÃ©nÃ©ration de rapports
"""

import sys
import os
import logging
import pandas as pd
import numpy as np
import time
from pathlib import Path
from datetime import datetime

# Ajout du rÃ©pertoire parent au PYTHONPATH pour les imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PipelineCompleteTest:
    """Classe de test complet du pipeline ultra-intelligent"""
    
    def __init__(self):
        self.start_time = time.time()
        self.test_results = {}
        self.test_data = None
        self.pipeline = None
        
    def run_complete_test(self):
        """ExÃ©cute tous les tests du pipeline"""
        logger.info("ğŸš€ === DÃ‰MARRAGE DU TEST COMPLET DU PIPELINE ===")
        logger.info(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Tests sÃ©quentiels
        tests = [
            ("Configuration et Imports", self.test_configuration_imports),
            ("Extraction des DonnÃ©es", self.test_data_extraction),
            ("Consolidation Maximale", self.test_consolidation_maximale),
            ("FonctionnalitÃ©s AvancÃ©es", self.test_fonctionnalites_avancees),
            ("Export Multi-Formats", self.test_export_multi_formats),
            ("Performance et Optimisations", self.test_performance),
            ("Validation et QualitÃ©", self.test_validation_qualite)
        ]
        
        for test_name, test_func in tests:
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ§ª TEST: {test_name}")
            logger.info(f"{'='*80}")
            
            try:
                success = test_func()
                self.test_results[test_name] = {
                    "status": "âœ… SUCCÃˆS" if success else "âŒ Ã‰CHEC",
                    "success": success
                }
                logger.info(f"ğŸ“Š {test_name}: {'âœ… SUCCÃˆS' if success else 'âŒ Ã‰CHEC'}")
            except Exception as e:
                logger.error(f"ğŸ’¥ Erreur critique dans {test_name}: {e}")
                self.test_results[test_name] = {
                    "status": "ğŸ’¥ ERREUR CRITIQUE",
                    "success": False,
                    "error": str(e)
                }
        
        # Rapport final
        self.generate_final_report()
        
    def test_configuration_imports(self):
        """Test de la configuration et des imports"""
        logger.info("âš™ï¸ === TEST CONFIGURATION ET IMPORTS ===")
        
        try:
            # Test des imports principaux
            logger.info("ğŸ“¦ Test des imports...")
            from config.consolidation_config import ConsolidationConfig
            from config.custom_fields_config import custom_config
            from core.ultra_intelligent_cleaner import UltraIntelligentCleaner
            from main_ultra_intelligent import UltraIntelligentPipeline
            logger.info("âœ… Tous les imports rÃ©ussis")
            
            # Test de la configuration de base
            logger.info("âš™ï¸ Test configuration de base...")
            base_config = ConsolidationConfig()
            groups_count = len(base_config.CONSOLIDATION_GROUPS)
            logger.info(f"ğŸ“Š Configuration de base: {groups_count} groupes")
            
            if groups_count < 25:
                logger.error(f"âŒ Nombre de groupes insuffisant: {groups_count} < 25")
                return False
            
            # Test de la configuration personnalisÃ©e
            logger.info("ğŸ”§ Test configuration personnalisÃ©e...")
            summary = custom_config.get_67_fields_config_summary()
            logger.info(f"ğŸ“Š Configuration personnalisÃ©e: {summary['total_consolidation_groups']} groupes")
            logger.info(f"ğŸ“Š RÃ©duction estimÃ©e: {summary['estimated_reduction']}")
            
            # Validation de la configuration
            if base_config.validate_configuration():
                logger.info("âœ… Configuration validÃ©e avec succÃ¨s")
                return True
            else:
                logger.error("âŒ Validation de configuration Ã©chouÃ©e")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur configuration/imports: {e}")
            return False
    
    def test_data_extraction(self):
        """Test de l'extraction des donnÃ©es"""
        logger.info("ğŸ“¥ === TEST EXTRACTION DES DONNÃ‰ES ===")
        
        try:
            # Initialisation du pipeline
            logger.info("ğŸš€ Initialisation du pipeline...")
            from main_ultra_intelligent import UltraIntelligentPipeline
            
            self.pipeline = UltraIntelligentPipeline()
            logger.info("âœ… Pipeline initialisÃ©")
            
            # Test avec donnÃ©es synthÃ©tiques
            logger.info("ğŸ—ï¸ GÃ©nÃ©ration de donnÃ©es de test...")
            self.test_data = self.create_comprehensive_test_data()
            logger.info(f"ğŸ“Š Dataset crÃ©Ã©: {self.test_data.shape[0]} lignes Ã— {self.test_data.shape[1]} colonnes")
            
            # Validation du dataset
            if self.test_data.shape[1] >= 50:
                logger.info(f"âœ… Dataset valide: {self.test_data.shape[1]} colonnes")
                return True
            else:
                logger.error(f"âŒ Dataset insuffisant: {self.test_data.shape[1]} colonnes")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur extraction donnÃ©es: {e}")
            return False
    
    def test_consolidation_maximale(self):
        """Test de la consolidation maximale"""
        logger.info("ğŸ”— === TEST CONSOLIDATION MAXIMALE ===")
        
        try:
            if self.test_data is None:
                logger.error("âŒ Pas de donnÃ©es de test disponibles")
                return False
            
            # Configuration avec la config personnalisÃ©e
            logger.info("âš™ï¸ Configuration pour consolidation...")
            from config.custom_fields_config import custom_config
            from core.ultra_intelligent_cleaner import UltraIntelligentCleaner
            
            cleaner = UltraIntelligentCleaner(custom_config)
            logger.info("âœ… Nettoyeur configurÃ©")
            
            # Consolidation
            logger.info("ğŸ”„ DÃ©marrage de la consolidation...")
            initial_columns = self.test_data.shape[1]
            
            df_consolidated = cleaner._consolidate_variables(self.test_data)
            final_columns = df_consolidated.shape[1]
            reduction_percentage = ((initial_columns - final_columns) / initial_columns) * 100
            
            logger.info(f"ğŸ“Š Consolidation terminÃ©e:")
            logger.info(f"   ğŸ“Š Colonnes initiales: {initial_columns}")
            logger.info(f"   ğŸ“Š Colonnes finales: {final_columns}")
            logger.info(f"   ğŸ“‰ RÃ©duction: {reduction_percentage:.1f}%")
            
            # Validation des rÃ©sultats
            if reduction_percentage > 0 and final_columns > 0:
                logger.info("âœ… Consolidation rÃ©ussie")
                self.consolidated_data = df_consolidated
                return True
            else:
                logger.error("âŒ Consolidation Ã©chouÃ©e")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur consolidation: {e}")
            return False
    
    def test_fonctionnalites_avancees(self):
        """Test des fonctionnalitÃ©s avancÃ©es"""
        logger.info("ğŸš€ === TEST FONCTIONNALITÃ‰S AVANCÃ‰ES ===")
        
        try:
            if not hasattr(self, 'consolidated_data'):
                logger.warning("âš ï¸ Pas de donnÃ©es consolidÃ©es, utilisation des donnÃ©es de test")
                df = self.test_data
            else:
                df = self.consolidated_data
            
            # Test du clustering spatial
            logger.info("ğŸŒ Test clustering spatial...")
            from intelligence.similarity_detector import SimilarityDetector
            detector = SimilarityDetector()
            
            # Ajouter des coordonnÃ©es si nÃ©cessaire
            if 'latitude' not in df.columns:
                df['latitude'] = np.random.uniform(45.4, 45.7, len(df))
                df['longitude'] = np.random.uniform(-73.8, -73.4, len(df))
            
            spatial_results = detector.spatial_clustering(df)
            if spatial_results.get("success"):
                logger.info(f"âœ… Clustering spatial: {spatial_results['n_clusters']} zones crÃ©Ã©es")
            else:
                logger.warning(f"âš ï¸ Clustering spatial Ã©chouÃ©: {spatial_results.get('error', 'Erreur inconnue')}")
            
            # Test de la catÃ©gorisation automatique
            logger.info("ğŸ·ï¸ Test catÃ©gorisation automatique...")
            from core.ultra_intelligent_cleaner import UltraIntelligentCleaner
            from config.custom_fields_config import custom_config
            
            cleaner = UltraIntelligentCleaner(custom_config)
            
            # Ajouter des donnÃ©es financiÃ¨res si nÃ©cessaire
            if 'revenue_final' not in df.columns:
                df['revenue_final'] = np.random.uniform(15000, 150000, len(df))
            if 'price_final' not in df.columns:
                df['price_final'] = np.random.uniform(200000, 2000000, len(df))
            
            df_categorized = cleaner.categorize_investment_opportunities(df)
            
            # VÃ©rifier les colonnes de catÃ©gorisation
            categorization_columns = ['segment_roi', 'classe_prix', 'type_opportunite', 'score_qualite']
            found_columns = [col for col in categorization_columns if col in df_categorized.columns]
            
            logger.info(f"âœ… CatÃ©gorisation: {len(found_columns)}/{len(categorization_columns)} colonnes crÃ©Ã©es")
            
            # Test du dashboard
            logger.info("ğŸ“Š Test dashboard de validation...")
            try:
                from dashboard.validation_dashboard import ValidationDashboard
                dashboard = ValidationDashboard()
                
                # CrÃ©ation d'un dashboard basique
                quality_metrics = {
                    "completeness_score": 85.5,
                    "consistency_score": 90.2,
                    "validity_score": 88.7
                }
                
                dashboard_result = dashboard.create_quality_overview_dashboard(df, quality_metrics)
                logger.info("âœ… Dashboard crÃ©Ã© avec succÃ¨s")
            except ImportError as e:
                logger.warning(f"âš ï¸ Dashboard non disponible (dÃ©pendances manquantes): {e}")
            except Exception as e:
                logger.warning(f"âš ï¸ Erreur dashboard: {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur fonctionnalitÃ©s avancÃ©es: {e}")
            return False
    
    def test_export_multi_formats(self):
        """Test de l'export multi-formats"""
        logger.info("ğŸ’¾ === TEST EXPORT MULTI-FORMATS ===")
        
        try:
            if not hasattr(self, 'consolidated_data'):
                logger.warning("âš ï¸ Pas de donnÃ©es consolidÃ©es, utilisation des donnÃ©es de test")
                df = self.test_data.head(100)  # RÃ©duire pour les tests
            else:
                df = self.consolidated_data.head(100)
            
            from export.advanced_exporter import AdvancedExporter
            
            exporter = AdvancedExporter()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Test des formats principaux
            formats_to_test = ["csv", "parquet", "json"]
            export_results = {}
            
            for format_name in formats_to_test:
                try:
                    logger.info(f"ğŸ“„ Test export {format_name.upper()}...")
                    
                    output_config = {
                        "base_filename": f"test_pipeline_complet_{timestamp}",
                        "output_directory": "exports/tests/",
                        "formats": [format_name],
                        "include_metadata": True
                    }
                    
                    # CrÃ©er le dossier si nÃ©cessaire
                    output_dir = Path(output_config["output_directory"])
                    output_dir.mkdir(parents=True, exist_ok=True)
                    
                    result = exporter.export_dataframe(df, output_config)
                    
                    if result.get("success"):
                        export_results[format_name] = "âœ… SUCCÃˆS"
                        logger.info(f"âœ… Export {format_name.upper()} rÃ©ussi")
                    else:
                        export_results[format_name] = "âŒ Ã‰CHEC"
                        logger.warning(f"âš ï¸ Export {format_name.upper()} Ã©chouÃ©")
                        
                except Exception as e:
                    export_results[format_name] = f"âŒ ERREUR: {str(e)[:50]}"
                    logger.warning(f"âš ï¸ Erreur export {format_name.upper()}: {e}")
            
            # RÃ©sumÃ© des exports
            successful_exports = sum(1 for result in export_results.values() if "SUCCÃˆS" in result)
            total_exports = len(export_results)
            
            logger.info(f"ğŸ“Š Exports rÃ©ussis: {successful_exports}/{total_exports}")
            
            for format_name, result in export_results.items():
                logger.info(f"   {format_name.upper()}: {result}")
            
            return successful_exports > 0
            
        except Exception as e:
            logger.error(f"âŒ Erreur export multi-formats: {e}")
            return False
    
    def test_performance(self):
        """Test des optimisations de performance"""
        logger.info("âš¡ === TEST PERFORMANCE ET OPTIMISATIONS ===")
        
        try:
            from performance.performance_optimizer import PerformanceOptimizer
            
            optimizer = PerformanceOptimizer()
            logger.info("âœ… Optimiseur de performance initialisÃ©")
            
            # Test d'activation des optimisations
            logger.info("ğŸ”§ Test activation des optimisations...")
            optimization_results = optimizer.enable_all_optimizations()
            
            enabled_optimizations = sum(1 for enabled in optimization_results.values() if enabled)
            total_optimizations = len(optimization_results)
            
            logger.info(f"ğŸ“Š Optimisations activÃ©es: {enabled_optimizations}/{total_optimizations}")
            
            for opt_name, enabled in optimization_results.items():
                status = "âœ… ACTIVÃ‰E" if enabled else "âŒ DÃ‰SACTIVÃ‰E"
                logger.info(f"   {opt_name}: {status}")
            
            return enabled_optimizations > 0
            
        except Exception as e:
            logger.error(f"âŒ Erreur performance: {e}")
            return False
    
    def test_validation_qualite(self):
        """Test de la validation et contrÃ´le qualitÃ©"""
        logger.info("âœ… === TEST VALIDATION ET QUALITÃ‰ ===")
        
        try:
            if not hasattr(self, 'consolidated_data'):
                df = self.test_data
            else:
                df = self.consolidated_data
            
            from validation.quality_validator import QualityValidator
            
            validator = QualityValidator()
            logger.info("âœ… Validateur de qualitÃ© initialisÃ©")
            
            # Test de validation
            logger.info("ğŸ” Test validation des donnÃ©es...")
            validation_results = validator.validate_dataset(df, "test_dataset")
            
            if validation_results.get("overall_score", 0) > 0:
                score = validation_results["overall_score"]
                logger.info(f"âœ… Score de qualitÃ© global: {score:.1f}%")
                
                # DÃ©tail par catÃ©gorie
                for category, details in validation_results.get("category_scores", {}).items():
                    if isinstance(details, dict) and "score" in details:
                        logger.info(f"   {category}: {details['score']:.1f}%")
                
                return score > 50  # Seuil minimum de qualitÃ©
            else:
                logger.warning("âš ï¸ Pas de score de qualitÃ© disponible")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur validation qualitÃ©: {e}")
            return False
    
    def create_comprehensive_test_data(self):
        """CrÃ©e un dataset de test complet avec toutes les colonnes nÃ©cessaires"""
        n_rows = 200
        
        # Dataset complet avec 78 colonnes comme spÃ©cifiÃ©
        test_data = pd.DataFrame({
            # Prix et Ã©valuations
            'price': np.random.uniform(200000, 2000000, n_rows),
            'prix_evaluation': np.random.uniform(180000, 1800000, n_rows),
            'price_assessment': np.random.uniform(180000, 1800000, n_rows),
            'evaluation_total': np.random.uniform(180000, 1800000, n_rows),
            'evaluation_terrain': np.random.uniform(80000, 800000, n_rows),
            'evaluation_batiment': np.random.uniform(100000, 1000000, n_rows),
            'municipal_evaluation_building': np.random.uniform(100000, 1000000, n_rows),
            'municipal_evaluation_land': np.random.uniform(80000, 800000, n_rows),
            'municipal_evaluation_total': np.random.uniform(180000, 1800000, n_rows),
            'evaluation_year': np.random.randint(2018, 2025, n_rows),
            'municipal_evaluation_year': np.random.randint(2018, 2025, n_rows),
            
            # Surface et caractÃ©ristiques
            'surface': np.random.uniform(50, 500, n_rows),
            'living_area': np.random.uniform(50, 500, n_rows),
            'superficie': np.random.uniform(50, 500, n_rows),
            'lot_size': np.random.uniform(100, 1000, n_rows),
            'bedrooms': np.random.randint(1, 6, n_rows),
            'nbr_chanbres': np.random.randint(1, 6, n_rows),
            'nb_bedroom': np.random.randint(1, 6, n_rows),
            'rooms': np.random.randint(3, 10, n_rows),
            'bathrooms': np.random.randint(1, 4, n_rows),
            'nbr_sal_deau': np.random.randint(1, 3, n_rows),
            'nbr_sal_bain': np.random.randint(1, 4, n_rows),
            'nb_bathroom': np.random.randint(1, 4, n_rows),
            'water_rooms': np.random.randint(1, 3, n_rows),
            'nb_water_room': np.random.randint(1, 3, n_rows),
            
            # CoordonnÃ©es
            'latitude': np.random.uniform(45.4, 45.7, n_rows),
            'longitude': np.random.uniform(-73.8, -73.4, n_rows),
            'geolocation': [f'45.5,-73.6#{i}' for i in range(n_rows)],
            'geo': [f'45.5,-73.6#{i}' for i in range(n_rows)],
            
            # Adresses
            'address': [f'123 Rue Principale #{i}' for i in range(n_rows)],
            'full_address': [f'123 Rue Principale #{i}, MontrÃ©al, QC' for i in range(n_rows)],
            'location': [f'MontrÃ©al, QC #{i}' for i in range(n_rows)],
            'city': ['MontrÃ©al', 'QuÃ©bec', 'Laval'] * (n_rows // 3 + 1),
            'postal_code': ['H1A 1A1', 'H2B 2B2', 'H3C 3C3'] * (n_rows // 3 + 1),
            
            # Type propriÃ©tÃ©
            'type': ['Maison', 'Appartement', 'Duplex', 'Triplex'] * (n_rows // 4 + 1),
            'building_style': ['Moderne', 'Traditionnel', 'Contemporain'] * (n_rows // 3 + 1),
            'style': ['Moderne', 'Traditionnel', 'Contemporain'] * (n_rows // 3 + 1),
            
            # AnnÃ©e construction
            'year_built': np.random.randint(1950, 2025, n_rows),
            'construction_year': np.random.randint(1950, 2025, n_rows),
            'annee': np.random.randint(1950, 2025, n_rows),
            
            # Taxes
            'municipal_taxes': np.random.uniform(2000, 20000, n_rows),
            'municipal_tax': np.random.uniform(2000, 20000, n_rows),
            'taxes': np.random.uniform(3000, 30000, n_rows),
            'school_taxes': np.random.uniform(1000, 10000, n_rows),
            'school_tax': np.random.uniform(1000, 10000, n_rows),
            
            # Revenus
            'revenu': np.random.uniform(15000, 150000, n_rows),
            'revenus_annuels_bruts': np.random.uniform(15000, 150000, n_rows),
            'plex-revenu': np.random.uniform(15000, 150000, n_rows),
            'plex_revenu': np.random.uniform(15000, 150000, n_rows),
            'potential_gross_revenue': np.random.uniform(15000, 150000, n_rows),
            
            # DÃ©penses
            'expense': np.random.uniform(5000, 50000, n_rows),
            'depenses': np.random.uniform(5000, 50000, n_rows),
            'expense_period': ['Annuel'] * n_rows,
            
            # Parking et unitÃ©s
            'nb_parking': np.random.randint(0, 4, n_rows),
            'parking': np.random.randint(0, 4, n_rows),
            'nb_garage': np.random.randint(0, 3, n_rows),
            'unites': np.random.randint(1, 5, n_rows),
            'residential_units': np.random.randint(1, 5, n_rows),
            'commercial_units': np.random.randint(0, 3, n_rows),
            
            # Images et mÃ©tadonnÃ©es
            'image': [f'https://exemple.com/image_{i}.jpg' for i in range(n_rows)],
            'images': [f'https://exemple.com/images_{i}.jpg' for i in range(n_rows)],
            'img_src': [f'https://exemple.com/image_{i}.jpg' for i in range(n_rows)],
            'revenu_period': ['Annuel'] * n_rows,
            'basement': ['Oui', 'Non', 'Partiel'] * (n_rows // 3 + 1),
            
            # Champs prÃ©servÃ©s
            'main_unit_details': [f'DÃ©tails unitÃ© #{i}' for i in range(n_rows)],
            'vendue': ['Non', 'Oui', 'En cours'] * (n_rows // 3 + 1),
            'description': [f'Belle propriÃ©tÃ© #{i}' for i in range(n_rows)],
            '_id': range(n_rows),
            'updated_at': pd.date_range('2024-01-01', periods=n_rows, freq='D'),
            'add_date': pd.date_range('2024-01-01', periods=n_rows, freq='D'),
            'created_at': pd.date_range('2024-01-01', periods=n_rows, freq='D'),
            'update_at': pd.date_range('2024-01-01', periods=n_rows, freq='D'),
            'region': ['QC'] * n_rows,
            'extraction_metadata': [f'Metadata extraction #{i}' for i in range(n_rows)],
            
            # MÃ©tadonnÃ©es Ã  supprimer
            'link': [f'https://exemple.com/propriete/{i}' for i in range(n_rows)],
            'company': ['Royal LePage', 'Century 21', 'RE/MAX'] * (n_rows // 3 + 1),
            'version': ['1.0'] * n_rows
        })
        
        # Ajout de valeurs manquantes pour tester la consolidation
        for col in test_data.columns:
            if test_data[col].dtype in ['object', 'float64']:
                # 15% de valeurs manquantes alÃ©atoires
                mask = np.random.choice([True, False], size=n_rows, p=[0.15, 0.85])
                test_data.loc[mask, col] = np.nan
        
        return test_data
    
    def generate_final_report(self):
        """GÃ©nÃ¨re le rapport final du test complet"""
        end_time = time.time()
        duration = end_time - self.start_time
        
        logger.info(f"\n{'='*100}")
        logger.info("ğŸ“Š RAPPORT FINAL DU TEST COMPLET DU PIPELINE")
        logger.info(f"{'='*100}")
        
        logger.info(f"ğŸ•’ DurÃ©e totale: {duration:.2f} secondes")
        logger.info(f"ğŸ“… Date de fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # RÃ©sumÃ© des tests
        logger.info(f"\nğŸ“‹ RÃ‰SUMÃ‰ DES TESTS:")
        success_count = sum(1 for result in self.test_results.values() if result["success"])
        total_count = len(self.test_results)
        
        for test_name, result in self.test_results.items():
            logger.info(f"   {test_name:35} : {result['status']}")
        
        logger.info(f"\nğŸ¯ RÃ‰SULTAT GLOBAL: {success_count}/{total_count} tests rÃ©ussis")
        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
        logger.info(f"ğŸ“Š Taux de rÃ©ussite: {success_rate:.1f}%")
        
        # Conclusion
        if success_count == total_count:
            logger.info("\nğŸ‰ TOUS LES TESTS SONT PASSÃ‰S!")
            logger.info("âœ… Le pipeline ultra-intelligent est 100% fonctionnel")
            logger.info("ğŸš€ Production-ready avec la nouvelle structure organisÃ©e!")
        elif success_rate >= 80:
            logger.info("\nâœ¨ TESTS MAJORITAIREMENT RÃ‰USSIS!")
            logger.info(f"âœ… {success_rate:.1f}% de fonctionnalitÃ©s opÃ©rationnelles")
            logger.info("âš ï¸ Quelques optimisations mineures recommandÃ©es")
        else:
            logger.warning(f"\nâš ï¸ TESTS PARTIELLEMENT RÃ‰USSIS ({success_rate:.1f}%)")
            logger.warning("ğŸ”§ Corrections nÃ©cessaires avant production")
        
        # Sauvegarde du rapport
        self.save_test_report()
        
        logger.info(f"\n{'='*100}")
        
        return success_count == total_count
    
    def save_test_report(self):
        """Sauvegarde le rapport de test dans un fichier"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = f"logs/test_pipeline_complet_{timestamp}.md"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("# ğŸ§ª RAPPORT DE TEST COMPLET DU PIPELINE\n\n")
                f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"**DurÃ©e**: {time.time() - self.start_time:.2f} secondes\n\n")
                
                f.write("## ğŸ“‹ RÃ©sultats des Tests\n\n")
                for test_name, result in self.test_results.items():
                    status_icon = "âœ…" if result["success"] else "âŒ"
                    f.write(f"- {status_icon} **{test_name}**: {result['status']}\n")
                
                success_count = sum(1 for result in self.test_results.values() if result["success"])
                total_count = len(self.test_results)
                success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
                
                f.write(f"\n## ğŸ¯ RÃ©sultat Global\n\n")
                f.write(f"- **Tests rÃ©ussis**: {success_count}/{total_count}\n")
                f.write(f"- **Taux de rÃ©ussite**: {success_rate:.1f}%\n")
                
                if success_count == total_count:
                    f.write("\n## ğŸ‰ Conclusion\n\n")
                    f.write("Le pipeline ultra-intelligent est **100% fonctionnel** et **production-ready**!\n")
            
            logger.info(f"ğŸ“„ Rapport sauvegardÃ©: {report_file}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Impossible de sauvegarder le rapport: {e}")

def main():
    """Fonction principale"""
    test_suite = PipelineCompleteTest()
    success = test_suite.run_complete_test()
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())
